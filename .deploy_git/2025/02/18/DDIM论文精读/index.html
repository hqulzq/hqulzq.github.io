<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"hqulzq.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="table {   border-collapse: collapse;   border: none; }  th, td {   border: none; }   DDIM论文精读全文翻译Abstract去噪扩散概率模型（DDPMs）在无需对抗训练的情况下实现了高质量的图像生成，但为了生成一个样本，它们需要模拟马尔可夫链的多个步骤。为了加速采样过程，我们提出了去噪扩散隐式模型（DDIMs）">
<meta property="og:type" content="article">
<meta property="og:title" content="DDIM论文精读">
<meta property="og:url" content="https://hqulzq.github.io/2025/02/18/DDIM%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/index.html">
<meta property="og:site_name" content="Lzq&#39;s blog">
<meta property="og:description" content="table {   border-collapse: collapse;   border: none; }  th, td {   border: none; }   DDIM论文精读全文翻译Abstract去噪扩散概率模型（DDPMs）在无需对抗训练的情况下实现了高质量的图像生成，但为了生成一个样本，它们需要模拟马尔可夫链的多个步骤。为了加速采样过程，我们提出了去噪扩散隐式模型（DDIMs）">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://hqulzq.github.io/2025/02/18/DDIM%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/f_1.png">
<meta property="og:image" content="https://hqulzq.github.io/2025/02/18/DDIM%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/f_2.png">
<meta property="og:image" content="https://hqulzq.github.io/2025/02/18/DDIM%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/t_1.png">
<meta property="og:image" content="https://hqulzq.github.io/2025/02/18/DDIM%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/f_3.png">
<meta property="og:image" content="https://hqulzq.github.io/2025/02/18/DDIM%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/f_4.png">
<meta property="og:image" content="https://hqulzq.github.io/2025/02/18/DDIM%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/f_5.png">
<meta property="og:image" content="https://hqulzq.github.io/2025/02/18/DDIM%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/f_6.png">
<meta property="og:image" content="https://hqulzq.github.io/2025/02/18/DDIM%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/t_2.png">
<meta property="og:image" content="https://hqulzq.github.io/2025/02/18/DDIM%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/t_3.png">
<meta property="og:image" content="https://hqulzq.github.io/2025/02/18/DDIM%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/f_7.png">
<meta property="og:image" content="https://hqulzq.github.io/2025/02/18/DDIM%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/f_8.png">
<meta property="og:image" content="https://hqulzq.github.io/2025/02/18/DDIM%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/f_9.png">
<meta property="og:image" content="https://hqulzq.github.io/2025/02/18/DDIM%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/f_10.png">
<meta property="og:image" content="https://hqulzq.github.io/2025/02/18/DDIM%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/f_11.png">
<meta property="og:image" content="https://hqulzq.github.io/2025/02/18/DDIM%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/f_12.png">
<meta property="og:image" content="https://hqulzq.github.io/2025/02/18/DDIM%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/f_13.png">
<meta property="article:published_time" content="2025-02-18T13:14:08.000Z">
<meta property="article:modified_time" content="2025-03-02T14:25:34.543Z">
<meta property="article:author" content="Zongqing Li">
<meta property="article:tag" content="diffusion">
<meta property="article:tag" content="论文">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://hqulzq.github.io/2025/02/18/DDIM%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/f_1.png">

<link rel="canonical" href="https://hqulzq.github.io/2025/02/18/DDIM%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>DDIM论文精读 | Lzq's blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Lzq's blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">31</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">12</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">42</span></a>

  </li>
        <li class="menu-item menu-item-book">

    <a href="/book" rel="section"><i class="fas fa-book fa-fw"></i>Book</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://hqulzq.github.io/2025/02/18/DDIM%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://github.com/hqulzq/hqulzq.github.io/blob/main/images/userimg.jpg?raw=true">
      <meta itemprop="name" content="Zongqing Li">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lzq's blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          DDIM论文精读
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-02-18 21:14:08" itemprop="dateCreated datePublished" datetime="2025-02-18T21:14:08+08:00">2025-02-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-03-02 22:25:34" itemprop="dateModified" datetime="2025-03-02T22:25:34+08:00">2025-03-02</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <style>
table {
  border-collapse: collapse;
  border: none;
}

th,
td {
  border: none;
}
</style>

<h1 id="DDIM论文精读"><a href="#DDIM论文精读" class="headerlink" title="DDIM论文精读"></a>DDIM论文精读</h1><h2 id="全文翻译"><a href="#全文翻译" class="headerlink" title="全文翻译"></a>全文翻译</h2><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>去噪扩散概率模型（DDPMs）在无需对抗训练的情况下实现了高质量的图像生成，但为了生成一个样本，它们需要模拟马尔可夫链的多个步骤。为了加速采样过程，我们提出了去噪扩散隐式模型（DDIMs），这是一类更高效的迭代隐式概率模型，其训练过程与DDPMs相同。在DDPMs中，生成过程被定义为特定马尔可夫扩散过程的逆过程。我们通过一类非马尔可夫扩散过程对DDPMs进行了扩展，这些非马尔可夫扩散过程能够产生与DDPMs相同的训练目标。这些非马尔可夫过程可以对应于确定性的生成过程，从而产生能够更快生成高质量样本的隐式模型。实验证明，与DDPMs相比，DDIMs生成高质量样本的速度在实际时间上快10到50倍，并且可以在计算量和样本质量之间进行权衡，能够直接在潜在空间中进行语义上有意义的图像插值，还能以极低的误差重建观测数据。<br><span id="more"></span></p>
<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>深度生成模型已展现出在诸多领域生成高质量样本的能力（Karras等人，2020；van den Oord等人，2016a）。在图像生成方面，生成对抗网络（GANs，Goodfellow等人，2014）目前生成的样本质量要高于基于似然的方法，如变分自编码器（Kingma和Welling，2013）、自回归模型（van den Oord等人，2016b）和归一化流（Rezende和Mohamed，2015；Dinh等人，2016 ）。然而，GANs为了稳定训练，在优化方法和架构选择上有非常特殊的要求（Arjovsky等人，2017；Gulrajani等人，2017；Karras等人，2018；Brock等人，2018），并且可能无法覆盖数据分布的所有模式（Zhao等人，2018）。</p>
<p>最近关于迭代生成模型（Bengio等人，2014）的研究成果，例如去噪扩散概率模型（DDPM，Ho等人，2020）和噪声条件得分网络（NCSN，Song和Ermon，2019），已经证明了无需进行对抗训练就能生成与GANs质量相当的样本。为实现这一点，许多去噪自编码模型被训练用于对被不同程度高斯噪声破坏的样本进行去噪。然后，通过一个马尔可夫链生成样本，该马尔可夫链从白噪声开始，逐步将其去噪转化为图像。这种生成马尔可夫链过程要么基于朗之万动力学（Song和Ermon，2019），要么通过反转一个将图像逐步转化为噪声的正向扩散过程得到（Sohl-Dickstein等人，2015）。</p>
<p>这些模型的一个关键缺点是，它们需要多次迭代才能生成高质量样本。对于DDPMs来说，这是因为其生成过程（从噪声到数据）是对正向扩散过程（从数据到噪声）的近似逆向，而正向扩散过程可能包含数千步；生成单个样本需要迭代所有步骤，这与GANs相比要慢得多，GANs只需让数据通过网络一次即可。例如，在英伟达2080 Ti GPU上，从DDPM中采样50,000张32×32大小的图像大约需要20小时，而从GANs中采样同样数量的图像则不到一分钟。对于更大尺寸的图像，这个问题更加突出，在同一GPU上采样50,000张256×256大小的图像可能需要近1000小时。</p>
<p>为了缩小DDPMs与GANs之间的效率差距，我们提出了去噪扩散隐式模型（DDIMs）。DDIMs属于隐式概率模型（Mohamed和Lakshminarayanan，2016），并且与DDPMs密切相关，它们使用相同的目标函数进行训练。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"><img src="f_1.png" alt="f_1"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><em>Figure 1: Graphical models for diffusion (left) and non-Markovian (right) inference models</em></td>
</tr>
</tbody>
</table>
</div>
<p>在第3节中，我们将DDPMs使用的马尔可夫正向扩散过程扩展到非马尔可夫扩散过程，对于这些非马尔可夫过程，我们仍然能够设计合适的逆向生成马尔可夫链。我们证明了由此产生的变分训练目标具有一个共同的替代目标，而这个替代目标恰好是用于训练DDPM的目标。因此，我们只需选择不同的非马尔可夫扩散过程（第4.1节）以及相应的逆向生成马尔可夫链，就可以使用同一个神经网络从众多生成模型中自由选择。特别地，我们能够使用那些能产生 “短” 生成马尔可夫链的非马尔可夫扩散过程（第4.2节），这些短链可以通过少量步骤模拟，这能在对样本质量影响较小的情况下大幅提高采样效率。在第5节中，我们展示了DDIMs相较于DDPMs在多个方面的优势。第一，当我们使用提出的方法将采样速度提高10到100倍时，DDIMs生成的样本质量优于DDPMs。第二，DDIMs的样本具有 “一致性” 特性，而DDPMs不具备这一特性：如果从相同的初始潜在变量出发，使用不同长度的马尔可夫链生成多个样本，这些样本会具有相似的高层次特征。第三，由于DDIMs的 “一致性”，我们可以通过操纵初始潜在变量在DDIMs中进行语义上有意义的图像插值，而DDPMs由于其随机生成过程，只能在图像空间附近进行插值。</p>
<h3 id="BACKGROUND"><a href="#BACKGROUND" class="headerlink" title="BACKGROUND"></a>BACKGROUND</h3><p>给定来自数据分布$q(x_{0})$的样本，我们希望学习一个模型分布$p_{\theta}(x_{0})$，它能逼近$q(x_{0})$且易于从中采样。去噪扩散概率模型（DDPMs，Sohl-Dickstein等人，2015；Ho等人，2020 ）是一种潜在变量模型，其形式为：</p>
<script type="math/tex; mode=display">p_{\theta}(x_{0}) = \int p_{\theta}(x_{0:T})dx_{1:T}\quad(1)</script><p>其中$p_{\theta}(x_{0:T}) := p_{\theta}(x_{T})\prod_{t = 1}^{T}p_{\theta}^{(t)}(x_{t - 1}|x_{t})$，$x_{1}, \ldots, x_{T}$是与$x_{0}$处于相同样本空间（记为$x$ ）的潜在变量。通过最大化变分下界来学习参数$\theta$，以使模型分布拟合数据分布$q(x_{0})$：</p>
<script type="math/tex; mode=display">\max_{\theta}\mathbb{E}_{q(x_{0})}[\log p_{\theta}(x_{0})] \leq \max_{\theta}\mathbb{E}_{q(x_{0}, x_{1}, \ldots, x_{T})}[\log p_{\theta}(x_{0:T}) - \log q(x_{1:T}|x_{0})] \quad(2)</script><p>其中$q(x_{1:T}|x_{0})$是关于潜在变量的某个推断分布。与典型的潜在变量模型（如变分自编码器，Rezende等人，2014 ）不同，DDPMs使用固定（而非可训练）的推断过程$q(x_{1:T}|x_{0})$进行学习，并且潜在变量的维度相对较高。例如，Ho等人（2020）考虑了以下由递减序列$\alpha_{1:T} \in (0, 1]^{T}$参数化的具有高斯转移的马尔可夫链：</p>
<script type="math/tex; mode=display">q(x_{1:T}|x_{0}) := \prod_{t = 1}^{T}q(x_{t}|x_{t - 1}) \quad(3)</script><p>其中</p>
<script type="math/tex; mode=display">q(x_{t}|x_{t - 1}) := \mathcal{N}(\sqrt{\frac{\alpha_{t}}{\alpha_{t - 1}}}x_{t - 1}, (1 - \frac{\alpha_{t}}{\alpha_{t - 1}})I)</script><p>这里协方差矩阵的对角项确保为正。由于采样过程的自回归性质（从$x_{0}$到$x_{T}$ ），这被称为正向过程。我们将潜在变量模型$p_{\theta}(x_{0:T})$（一个从$x_{T}$到$x_{0}$采样的马尔可夫链）称为生成过程，因为它逼近了难以处理的逆向过程$q(x_{t - 1}|x_{t})$。直观地说，正向过程逐渐向观测值$x_{0}$添加噪声，而生成过程则逐渐对带噪观测值进行去噪（图1，左）。</p>
<p>正向过程的一个特殊性质是：</p>
<script type="math/tex; mode=display">q(x_{t}|x_{0}) := \int q(x_{1:t}|x_{0})dx_{1:(t - 1)} = \mathcal{N}(x_{t}; \sqrt{\alpha_{t}}x_{0}, (1 - \alpha_{t})I)</script><p>所以我们可以将$x_{t}$表示为$x_{0}$和噪声变量$\epsilon$的线性组合：</p>
<script type="math/tex; mode=display">x_{t} = \sqrt{\alpha_{t}}x_{0} + \sqrt{1 - \alpha_{t}}\epsilon \quad(4)</script><p>其中$\epsilon \sim \mathcal{N}(0, I)$。</p>
<p>当我们将$\alpha_{T}$设置得足够接近0时，对于所有的$x_{0}$，$q(x_{T}|x_{0})$都收敛到标准高斯分布，因此自然地设置$p_{\theta}(x_{T}) := \mathcal{N}(0, I)$ 。如果所有的条件分布都被建模为具有可训练均值函数和固定方差的高斯分布，那么公式（2）中的目标函数可以简化为：</p>
<script type="math/tex; mode=display">L_{\gamma}(\epsilon_{\theta}) := \sum_{t = 1}^{T}\gamma_{t}\mathbb{E}_{x_{0} \sim q(x_{0}), \epsilon_{t} \sim \mathcal{N}(0, I)}[\|\epsilon_{\theta}^{(t)}(\sqrt{\alpha_{t}}x_{0} + \sqrt{1 - \alpha_{t}}\epsilon_{t}) - \epsilon_{t}\|_{2}^{2}] \quad(5)</script><p>其中$\epsilon_{\theta} = \{\epsilon_{\theta}^{(t)}\}_{t = 1}^{T}$是一组$T$个函数，每个$\epsilon_{\theta}^{(t)}: X \to X$（由$t$索引）是一个具有可训练参数$\theta^{(t)}$的函数，并且$\gamma = [\gamma_{1}, \ldots, \gamma_{T}]$是目标函数中的一个正系数向量，它取决于$\alpha_{1:T}$。在Ho等人（2020）的研究中，为了最大化训练模型的生成性能，优化的是$\gamma = 1$时的目标函数；这也是基于得分匹配（Hyvarinen，2005；Vincent，2011 ）的噪声条件得分网络（Song和Ermon，2019）中使用的相同目标函数。从训练好的模型中采样$x_{0}$时，首先从先验分布$p_{\theta}(x_{T})$中采样$x_{T}$，然后通过生成过程迭代地采样$x_{t - 1}$。</p>
<p>正向过程的长度$T$是DDPMs中的一个重要超参数。从变分的角度来看，较大的$T$值能使逆向过程接近高斯分布（Sohl-Dickstein等人，2015），这样用高斯条件分布建模的生成过程就能成为一个很好的近似；这促使人们选择较大的$T$值，例如Ho等人（2020）中选择$T = 1000$。然而，由于要获得样本$x_{0}$，所有$T$次迭代都必须顺序执行，而不能并行执行，因此从DDPMs中采样比从其他深度生成模型中采样要慢得多，这使得它们在计算资源有限和延迟要求严格的任务中不太实用。</p>
<h3 id="VARIATIONAL-INFERENCE-FOR-NON-MARKOVIAN-FORWARD-PROCESSES"><a href="#VARIATIONAL-INFERENCE-FOR-NON-MARKOVIAN-FORWARD-PROCESSES" class="headerlink" title="VARIATIONAL INFERENCE FOR NON-MARKOVIAN FORWARD PROCESSES"></a>VARIATIONAL INFERENCE FOR NON-MARKOVIAN FORWARD PROCESSES</h3><p>由于生成模型是对推断过程的逆向近似，为了减少生成模型所需的迭代次数，我们需要重新思考推断过程。我们的关键发现是，以$L_{\gamma}$形式呈现的DDPM目标函数仅依赖于边缘分布$q(x_{t}|x_{0})$，而并非直接依赖于联合分布$q(x_{1:T}|x_{0})$。鉴于存在许多具有相同边缘分布的推断分布（联合分布），我们探索了非马尔可夫的替代推断过程，这将引出新的生成过程（图1，右）。如下文所示，这些非马尔可夫推断过程会产生与DDPM相同的替代目标函数。在附录A中，我们证明了非马尔可夫视角不仅适用于高斯分布的情况。</p>
<h4 id="NON-MARKOVIAN-FORWARD-PROCESSES"><a href="#NON-MARKOVIAN-FORWARD-PROCESSES" class="headerlink" title="NON-MARKOVIAN FORWARD PROCESSES"></a>NON-MARKOVIAN FORWARD PROCESSES</h4><p>我们考虑一族由实向量$\sigma \in \mathbb{R}_{\geq0}^{T}$索引的推断分布$\mathcal{Q}$：</p>
<script type="math/tex; mode=display">q_{\sigma}(x_{1:T}|x_{0}) := q_{\sigma}(x_{T}|x_{0})\prod_{t = 2}^{T}q_{\sigma}(x_{t - 1}|x_{t}, x_{0}) \quad(6)</script><p>其中$q_{\sigma}(x_{T}|x_{0}) = \mathcal{N}(\sqrt{\alpha_{T}}x_{0}, (1 - \alpha_{T})I)$，并且对于所有$t &gt; 1$，有</p>
<script type="math/tex; mode=display">q_{\sigma}(x_{t - 1}|x_{t}, x_{0}) = \mathcal{N}(\sqrt{\alpha_{t - 1}}x_{0} + \sqrt{1 - \alpha_{t - 1} - \sigma_{t}^{2}} \cdot \frac{x_{t} - \sqrt{\alpha_{t}}x_{0}}{\sqrt{1 - \alpha_{t}}}, \sigma_{t}^{2}I) \quad(7)</script><p>选择这样的均值函数是为了确保对于所有的$t$，都有$q_{\sigma}(x_{t}|x_{0}) = \mathcal{N}(\sqrt{\alpha_{t}}x_{0}, (1 - \alpha_{t})I)$（见附录B中的引理1），这样就定义了一个符合预期、与“边缘分布”匹配的联合推断分布。正向过程可以根据贝叶斯规则推导得出：</p>
<script type="math/tex; mode=display">q_{\sigma}(x_{t}|x_{t - 1}, x_{0}) = \frac{q_{\sigma}(x_{t - 1}|x_{t}, x_{0})q_{\sigma}(x_{t}|x_{0})}{q_{\sigma}(x_{t - 1}|x_{0})} \quad(8)</script><p>它也是高斯分布（尽管在本文后续内容中我们不会用到这个事实）。与公式（3）中的扩散过程不同，这里的正向过程不再是马尔可夫过程，因为每个$x_{t}$可能同时依赖于$x_{t - 1}$和$x_{0}$。$\sigma$的大小控制着正向过程的随机性；当$\sigma \to 0$时，我们会达到一个极端情况，即只要观察到某个$t$对应的$x_{0}$和$x_{t}$，那么$x_{t - 1}$就会被确定下来。</p>
<h4 id="GENERATIVE-PROCESS-AND-UNIFIED-VARIATIONAL-INFERENCE-OBJECTIVE"><a href="#GENERATIVE-PROCESS-AND-UNIFIED-VARIATIONAL-INFERENCE-OBJECTIVE" class="headerlink" title="GENERATIVE PROCESS AND UNIFIED VARIATIONAL INFERENCE OBJECTIVE"></a>GENERATIVE PROCESS AND UNIFIED VARIATIONAL INFERENCE OBJECTIVE</h4><p>接下来，我们定义一个可训练的生成过程$p_{\theta}(x_{0:T})$，其中每个$p_{\theta}^{(t)}(x_{t - 1}|x_{t})$都利用了$q_{\sigma}(x_{t - 1}|x_{t}, x_{0})$的信息。直观地说，给定一个带噪观测值$x_{t}$，我们首先预测相应的$x_{0}$，然后利用这个预测值，通过已定义的逆向条件分布$q_{\sigma}(x_{t - 1}|x_{t}, x_{0})$得到样本$x_{t - 1}$。</p>
<p>对于某些$x_{0} \sim q(x_{0})$和$\epsilon_{t} \sim \mathcal{N}(0, I)$，可以使用公式（4）得到$x_{t}$。然后模型$\epsilon_{\theta}^{(t)}(x_{t})$尝试在不知道$x_{0}$的情况下，从$x_{t}$预测$\epsilon_{t}$。通过重写公式（4），可以预测去噪后的观测值，也就是给定$x_{t}$时对$x_{0}$的预测：</p>
<script type="math/tex; mode=display">f_{\theta}^{(t)}(x_{t}) := \frac{x_{t} - \sqrt{1 - \alpha_{t}} \cdot \epsilon_{\theta}^{(t)}(x_{t})}{\sqrt{\alpha_{t}}}  \quad(9)</script><p>然后，我们定义生成过程，其先验为固定的$p_{\theta}(x_{T}) = \mathcal{N}(0, I)$，并且</p>
<script type="math/tex; mode=display">p_{\theta}^{(t)}(x_{t - 1}|x_{t}) = \begin{cases} \mathcal{N}(f_{\theta}^{(1)}(x_{1}), \sigma_{1}^{2}I) & \text{如果 } t = 1 \\ q_{\sigma}(x_{t - 1}|x_{t}, f_{\theta}^{(t)}(x_{t})) & \text{否则} \end{cases}  \quad(10)</script><p>其中$q_{\sigma}(x_{t - 1}|x_{t}, f_{\theta}^{(t)}(x_{t}))$的定义与公式（7）相同，只是将$x_{0}$替换为$f_{\theta}^{(t)}(x_{t})$ 。在$t = 1$的情况下，我们添加了一些高斯噪声（协方差为$\sigma_{1}^{2}I$），以确保生成过程在任何地方都有定义。</p>
<p>我们通过以下变分推断目标（它是关于$\epsilon_{\theta}$的一个泛函）来优化$\theta$：</p>
<script type="math/tex; mode=display">\begin{align*}J_{\sigma}(\epsilon_{\theta}) &:= \mathbb{E}_{x_{0:T} \sim q_{\sigma}(x_{0:T})}[\log q_{\sigma}(x_{1:T}|x_{0}) - \log p_{\theta}(x_{0:T})] \\&= \mathbb{E}_{x_{0:T} \sim q_{\sigma}(x_{0:T})}[\log q_{\sigma}(x_{T}|x_{0}) + \sum_{t = 2}^{T}\log q_{\sigma}(x_{t - 1}|x_{t}, x_{0}) - \sum_{t = 1}^{T}\log p_{\theta}^{(t)}(x_{t - 1}|x_{t}) - \log p_{\theta}(x_{T})]\end{align*}  \quad(11)</script><p>这里我们根据公式（6）对$q_{\sigma}(x_{1:T}|x_{0})$进行因式分解，根据公式（1）对$p_{\theta}(x_{0:T})$进行因式分解。从$J_{\sigma}$的定义来看，似乎对于每一个$\sigma$的选择都需要训练一个不同的模型，因为它对应着不同的变分目标（以及不同的生成过程）。然而，正如我们下面将展示的，对于某些权重$\gamma$，$J_{\sigma}$与$L_{\gamma}$是等价的。</p>
<p>定理1：对于所有$\sigma &gt; 0$，存在$\gamma \in \mathbb{R}_{&gt;0}^{T}$和$C \in \mathbb{R}$，使得$J_{\sigma} = L_{\gamma} + C$。</p>
<p>变分目标$L_{\gamma}$有一个特殊性质：如果模型$\epsilon_{\theta}^{(t)}$的参数$\theta$在不同的$t$之间不共享，那么$\epsilon_{\theta}$的最优解将不依赖于权重$\gamma$（因为通过分别最大化求和中的每一项就可以达到全局最优）。$L_{\gamma}$的这个性质有两个重要意义。一方面，这证明了在DDPMs中使用$L_{1}$作为变分下界的替代目标函数是合理的；另一方面，由于定理1表明$J_{\sigma}$与某个$L_{\gamma}$等价，所以$J_{\sigma}$的最优解也与$L_{1}$的最优解相同。因此，如果在模型$\epsilon_{\theta}$中参数在不同的$t$之间不共享，那么Ho等人（2020）使用的$L_{1}$目标函数也可以作为变分目标$J_{\sigma}$的替代目标。</p>
<h3 id="SAMPLING-FROM-GENERALIZED-GENERATIVE-PROCESSES"><a href="#SAMPLING-FROM-GENERALIZED-GENERATIVE-PROCESSES" class="headerlink" title="SAMPLING FROM GENERALIZED GENERATIVE PROCESSES"></a>SAMPLING FROM GENERALIZED GENERATIVE PROCESSES</h3><p>以$L_{1}$为目标函数，我们不仅在学习Sohl-Dickstein等人（2015）和Ho等人（2020）中所考虑的马尔可夫推断过程的生成过程，还在学习由$\sigma$参数化的许多非马尔可夫正向过程的生成过程。因此，我们基本上可以将预训练的DDPM模型作为新目标的解决方案，然后通过改变$\sigma$来找到更符合我们需求的样本生成过程。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"><img src="f_2.png" alt="f_2"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><em>Figure 2: Graphical model for accelerated generation, where $\tau=[1,3]$</em></td>
</tr>
</tbody>
</table>
</div>
<h4 id="DENOISING-DIFFUSION-IMPLICIT-MODELS"><a href="#DENOISING-DIFFUSION-IMPLICIT-MODELS" class="headerlink" title="DENOISING DIFFUSION IMPLICIT MODELS"></a>DENOISING DIFFUSION IMPLICIT MODELS</h4><p>从公式（10）中的$p_{\theta}(x_{1:T})$出发，可以通过以下方式从样本$x_{t}$生成样本$x_{t - 1}$：</p>
<script type="math/tex; mode=display">x_{t - 1}=\sqrt{\alpha_{t - 1}}\underbrace{\left(\frac{x_{t}-\sqrt{1-\alpha_{t}}\epsilon_{\theta}^{(t)}(x_{t})}{\sqrt{\alpha_{t}}}\right)}_{“预测的x_{0}”}+\underbrace{\sqrt{1-\alpha_{t - 1}-\sigma_{t}^{2}}\cdot\epsilon_{\theta}^{(t)}(x_{t})}_{“指向x_{t}的方向”}+\underbrace{\sigma_{t}\epsilon_{t}}_{随机噪声} \quad(12)</script><p>其中$\epsilon_{t} \sim \mathcal{N}(0, I)$是与$x_{t}$相互独立的标准高斯噪声，并且我们定义$\alpha_{0}:=1$ 。不同的$\sigma$值会产生不同的生成过程，但都使用相同的模型$\epsilon_{\theta}$，因此无需重新训练模型。当对于所有的$t$，$\sigma_{t}=\sqrt{(1 - \alpha_{t - 1})/(1 - \alpha_{t})}\sqrt{1 - \alpha_{t}/\alpha_{t - 1}}$时，正向过程变为马尔可夫过程，生成过程则变为DDPM。</p>
<p>我们注意到另一种特殊情况，当对于所有的$t$，$\sigma_{t}=0$时（尽管定理1未涵盖这种情况，但我们总是可以通过让$\sigma_{t}$非常小来逼近它），正向过程在给定$x_{t - 1}$和$x_{0}$的情况下（除了$t = 1$ ）变为确定性过程；在生成过程中，随机噪声$\epsilon_{t}$前面的系数变为零。由此产生的模型成为一个隐式概率模型（Mohamed &amp; Lakshminarayanan, 2016），其中样本是通过固定的程序从潜在变量中生成的（从$x_{T}$到$x_{0}$ ）。我们将其命名为去噪扩散隐式模型（DDIM，发音为/d:Im/），因为它是一个使用DDPM目标函数训练的隐式概率模型（尽管正向过程不再是扩散过程）。</p>
<h4 id="ACCELERATED-GENERATION-PROCESSES"><a href="#ACCELERATED-GENERATION-PROCESSES" class="headerlink" title="ACCELERATED GENERATION PROCESSES"></a>ACCELERATED GENERATION PROCESSES</h4><p>在前面的章节中，生成过程被视为对逆向过程的近似；由于正向过程有$T$步，生成过程也必须进行$T$步采样。然而，由于去噪目标$L_{1}$只要$q_{\sigma}(x_{t}|x_{0})$固定，就不依赖于具体的正向过程，所以我们也可以考虑长度小于$T$的正向过程，这在无需训练不同模型的情况下加速了相应的生成过程。</p>
<p>我们考虑正向过程不是定义在所有潜在变量$x_{1:T}$上，而是定义在子集$\{x_{\tau_{1}}, \ldots, x_{\tau_{S}}\}$上，其中$\tau$是$[1, \ldots, T]$的一个长度为$S$的递增子序列。特别地，我们定义在$x_{\tau_{1}}, \ldots, x_{\tau_{S}}$上的顺序正向过程，使得$q(x_{\tau_{i}}|x_{0}) = \mathcal{N}(\sqrt{\alpha_{\tau_{i}}}x_{0}, (1 - \alpha_{\tau_{i}})I)$与“边缘分布”匹配（见图2的示例）。现在生成过程根据反转的$(\tau)$来采样潜在变量，我们将其称为（采样）轨迹。当采样轨迹的长度远小于$T$时，由于采样过程的迭代性质，我们可以显著提高计算效率。</p>
<p>使用与第3节类似的论证，我们可以证明使用以$L_{1}$为目标函数训练的模型是合理的，因此在训练过程中无需进行更改。我们表明，只需对公式（12）中的更新进行轻微修改，就可以得到新的、更快的生成过程，这适用于DDPM、DDIM以及公式（10）中考虑的所有生成过程。我们在附录C.1中给出了这些细节。</p>
<p>原则上，这意味着我们可以训练一个具有任意数量正向步骤的模型，但在生成过程中只从其中一些步骤进行采样。因此，训练好的模型可以考虑比Ho等人（2020）中更多的步骤，甚至可以考虑连续的时间变量$t$（Chen等人，2020）。我们将这方面的实证研究留作未来的工作。</p>
<h4 id="RELEVANCE-TO-NEURAL-ODES"><a href="#RELEVANCE-TO-NEURAL-ODES" class="headerlink" title="RELEVANCE TO NEURAL ODES"></a>RELEVANCE TO NEURAL ODES</h4><p>此外，我们可以根据公式（12）重写DDIM的迭代过程，它与求解常微分方程（ODEs）的欧拉积分的相似性就更加明显：</p>
<script type="math/tex; mode=display">\frac{x_{t-\Delta t}}{\sqrt{\alpha_{t-\Delta t}}}=\frac{x_{t}}{\sqrt{\alpha_{t}}}+\left(\sqrt{\frac{1-\alpha_{t-\Delta t}}{\alpha_{t-\Delta t}}}-\sqrt{\frac{1-\alpha_{t}}{\alpha_{t}}}\right)\epsilon_{\theta}^{(t)}(x_{t}) \quad(13)</script><p>为了推导相应的ODE，我们可以用$\sigma$对$(\sqrt{1 - \alpha}/\sqrt{\alpha})$进行重新参数化，用$\bar{x}$对$(x/\sqrt{\alpha})$进行重新参数化。在连续情况下，$\sigma$和$x$是$t$的函数，其中$\sigma: \mathbb{R}_{\geq0} \to \mathbb{R}_{\geq0}$是连续的、单调递增的，且$\sigma(0)=0$ 。公式（13）可以看作是对以下ODE的欧拉方法：</p>
<script type="math/tex; mode=display">d\overline{x}(t)=\epsilon_{\theta}^{(t)}\left(\frac{\overline{x}(t)}{\sqrt{\sigma^{2}+1}}\right)d\sigma(t) \quad(14)</script><p>其初始条件是$x(T) \sim \mathcal{N}(0, \sigma(T))$（对应于$\alpha \approx 0$的情况，此时$\sigma(T)$非常大）。这表明，通过足够多的离散化步骤，我们也可以反转生成过程（从$t = 0$到$T$ ），将$x_{0}$编码为$x_{T}$，并模拟公式（14）中ODE的逆向过程。这意味着与DDPM不同，我们可以使用DDIM获得观测值的编码（以$x_{T}$的形式），这对于其他需要模型潜在表示的下游应用可能是有用的。</p>
<p>在同期的一项工作中，Song等人（2020）提出了一种“概率流ODE”，旨在基于分数恢复随机微分方程（SDE）的边际密度，从中可以得到类似的采样调度。在这里，我们指出我们的ODE等同于他们工作中的一个特殊情况（对应于DDPM的连续时间模拟）。</p>
<p>命题1：具有最优模型$\epsilon_{\theta}^{(t)}$的公式（14）中的ODE，有一个与之等价的概率流ODE，它对应于Song等人（2020）中的“方差爆炸”SDE。</p>
<p>我们在附录B中给出了证明。虽然这些ODE是等价的，但采样过程并不相同。概率流ODE的欧拉方法会进行如下更新：</p>
<script type="math/tex; mode=display">\frac{x_{t-\Delta t}}{\sqrt{\alpha_{t-\Delta t}}}=\frac{x_{t}}{\sqrt{\alpha_{t}}}+\frac{1}{2}\left(\frac{1-\alpha_{t-\Delta t}}{\alpha_{t-\Delta t}}-\frac{1-\alpha_{t}}{\alpha_{t}}\right)\cdot\sqrt{\frac{\alpha_{t}}{1-\alpha_{t}}}\cdot\epsilon_{\theta}^{(t)}(x_{t}) \quad(15)</script><p>在采样步骤足够多的情况下，这些选择的差异不大；但在较少的采样步骤下，这些选择会产生不同的结果。我们是相对于$d\sigma(t)$进行欧拉步（它与“时间”$t$的缩放关系不太直接），而Song等人（2020）是相对于$dt$进行欧拉步。</p>
<h3 id="EXPERIMENTS"><a href="#EXPERIMENTS" class="headerlink" title="EXPERIMENTS"></a>EXPERIMENTS</h3><p>在本节中，我们展示了在考虑较少迭代次数的情况下，去噪扩散隐式模型（DDIM）在图像生成方面优于去噪扩散概率模型（DDPM），与原始的DDPM生成过程相比，速度提升了10到100倍。此外，与DDPM不同，一旦初始潜在变量$x_T$固定，DDIM无论生成轨迹如何，都能保留高层次的图像特征，因此它们能够直接在潜在空间中进行插值。DDIM还可用于对样本进行编码，从潜在代码中重建样本，而DDPM由于随机采样过程无法做到这一点。</p>
<p>对于每个数据集，我们使用相同的训练模型，其中$T = 1000$，目标函数为公式（5）中的$L_{\gamma}$，且$\gamma = 1$。正如我们在第3节中所讨论的，训练过程无需改变。我们唯一的改变在于从模型生成样本的方式，通过控制$\tau$（它控制获取样本的速度）和$\sigma$（它在确定性的DDIM和随机的DDPM之间进行插值）来实现。</p>
<p>我们考虑$[1, …, T]$的不同子序列$\tau$，以及由$\tau$中的元素索引的不同方差超参数$\sigma$。为了简化比较，我们考虑形式为：</p>
<script type="math/tex; mode=display">\sigma_{\tau_{i}}(\eta)=\eta \sqrt{\left(1-\alpha_{\tau_{i-1}}\right) /\left(1-\alpha_{\tau_{i}}\right)} \sqrt{1-\alpha_{\tau_{i}} / \alpha_{\tau_{i-1}}} \quad(16)</script><p>的$\sigma$，其中$\eta \in \mathbb{R}_{\geq0}$是我们可以直接控制的超参数。当$\eta = 1$时，这包括原始的DDPM生成过程；当$\eta = 0$时，则为DDIM。我们还考虑了随机噪声标准差大于$\sigma(1)$的DDPM，我们将其表示为$\hat{\sigma}$：$\hat{\sigma}_{\tau_{i}}=\sqrt{1-\alpha_{\tau_{i}} / \alpha_{\tau_{i-1}}}$。这仅在Ho等人（2020年）的实现中用于获取CIFAR10样本，而不用于其他数据集的样本。更多详细信息请参见附录D。<br><img src="t_1.png" alt=""><br><img src="f_3.png" alt=""></p>
<h4 id="SAMPLE-QUALITY-AND-EFFICIENCY"><a href="#SAMPLE-QUALITY-AND-EFFICIENCY" class="headerlink" title="SAMPLE QUALITY AND EFFICIENCY"></a>SAMPLE QUALITY AND EFFICIENCY</h4><p>在表1中，我们报告了在CIFAR10和CelebA上训练的模型生成样本的质量，通过弗雷歇初始距离（FID，Heusel等人，2017年）进行衡量，其中我们改变用于生成样本的时间步数（$dim(\tau)$）和过程的随机性（$\eta$）。正如预期的那样，随着$dim(\tau)$的增加，样本质量会提高，这体现了样本质量和计算成本之间的权衡。我们观察到，当$dim(\tau)$较小时，DDIM（$\eta = 0$）实现了最佳的样本质量，而DDPM（$\eta = 1$和$\hat{\sigma}$）在相同$dim(\tau)$下，通常比其随机性较低的对应模型样本质量更差，除了在$dim(\tau)=1000$且采用Ho等人（2020年）报告的$\hat{\sigma}$的情况下，此时DDIM的样本质量略逊一筹。然而，对于较小的$dim(\tau)$，$\hat{\sigma}$的样本质量会变得更差，这表明它不适合较短的轨迹。另一方面，DDIM更能始终如一地实现高质量样本。在图3中，我们展示了具有相同采样步数但不同$\eta$的CIFAR10和CelebA样本。对于DDPM，当采样轨迹为10步时，样本质量会迅速下降。对于$\hat{\sigma}$的情况，在短轨迹下生成的图像似乎有更多的噪声扰动；这就解释了为什么其FID分数比其他方法差得多，因为FID对这种扰动非常敏感（如Jolicoeur-Martineau等人，2020年所讨论的）。在图4中，我们展示了生成一个样本所需的时间与样本轨迹长度成线性关系。这表明DDIM有助于更高效地生成样本，因为可以用更少的步数生成样本。值得注意的是，DDIM能够在20到100步内生成与1000步模型质量相当的样本，与原始的DDPM相比，速度提升了10到50倍。尽管DDPM在100倍步数下也能实现合理的样本质量，但DDIM达到相同质量所需的步数要少得多；在CelebA数据集上，100步DDPM的FID分数与20步DDIM的FID分数相似。</p>
<h4 id="SAMPLE-CONSISTENCY-IN-DDIMS"><a href="#SAMPLE-CONSISTENCY-IN-DDIMS" class="headerlink" title="SAMPLE CONSISTENCY IN DDIMS"></a>SAMPLE CONSISTENCY IN DDIMS</h4><p>对于DDIM，生成过程是确定性的，$x_0$仅取决于初始状态$x_T$。在图5中，我们观察了在相同的初始$x_T$下，不同生成轨迹（即不同的$\tau$）下生成的图像。有趣的是，对于具有相同初始$x_T$的生成图像，无论生成轨迹如何，大多数高层次特征都是相似的。在许多情况下，仅用20步生成的样本在高层次特征上已经与用1000步生成的样本非常相似，只是在细节上有一些小差异。因此，$x_T$本身似乎就是图像的一个有信息的潜在编码；影响样本质量的微小细节编码在参数中，因为更长的采样轨迹会产生质量更好的样本，但不会显著影响高层次特征。更多样本见附录D.4。<br><img src="f_4.png" alt=""><br><img src="f_5.png" alt=""></p>
<h4 id="INTERPOLATION-IN-DETERMINISTIC-GENERATIVE-PROCESSES"><a href="#INTERPOLATION-IN-DETERMINISTIC-GENERATIVE-PROCESSES" class="headerlink" title="INTERPOLATION IN DETERMINISTIC GENERATIVE PROCESSES"></a>INTERPOLATION IN DETERMINISTIC GENERATIVE PROCESSES</h4><p><img src="f_6.png" alt=""><br>由于DDIM样本的高层次特征由$x_T$编码，我们想看看它是否会表现出与其他隐式概率模型（如GAN，Goodfellow等人，2014年）类似的语义插值效果。这与Ho等人（2020年）中的插值过程不同，因为在DDPM中，由于随机生成过程，相同的$x_T$会导致非常多样的$x_0$ 。在图6中，我们展示了在$x_T$中进行简单插值可以在两个样本之间实现语义上有意义的插值。更多细节和样本见附录D.5。这使得DDIM能够通过潜在变量直接在高层次上控制生成的图像，而DDPM无法做到这一点。<br><img src="t_2.png" alt=""></p>
<h4 id="RECONSTRUCTION-FROM-LATENT-SPACE"><a href="#RECONSTRUCTION-FROM-LATENT-SPACE" class="headerlink" title="RECONSTRUCTION FROM LATENT SPACE"></a>RECONSTRUCTION FROM LATENT SPACE</h4><p>由于DDIM是特定常微分方程（ODE）的欧拉积分，我们想看看它是否能够从$x_0$编码到$x_T$（公式（14）的反向），并从得到的$x_T$重建$x_0$（公式（14）的正向）。我们在CIFAR-10测试集上使用CIFAR-10模型进行编码和解码，编码和解码都使用$S$步；我们在表2中报告了每个维度的均方误差（缩放到[0, 1]）。我们的结果表明，对于较大的$S$值，DDIM具有较低的重建误差，并且具有与神经ODE和归一化流相似的性质。由于DDPM的随机性，它无法做到这一点。</p>
<h3 id="RELATED-WORK"><a href="#RELATED-WORK" class="headerlink" title="RELATED WORK"></a>RELATED WORK</h3><p>我们的工作基于一系列现有的将生成模型作为马尔可夫链转移算子进行学习的方法（Sohl-Dickstein等人，2015；Bengio等人，2014；Salimans等人，2014；Song等人，2017；Goyal等人，2017；Levy等人，2017）。其中，去噪扩散概率模型（DDPMs，Ho等人，2020）和噪声条件得分网络（NCSN，Song和Ermon，2019；2020）最近在生成样本质量上取得了与生成对抗网络（GANs，Brock等人，2018；Karras等人，2018）相媲美的成果。DDPMs通过优化对数似然的变分下界来学习，而NCSNs则通过对数据的非参数Parzen密度估计（Vincent，2011；Raphan和Simoncelli，2011）来优化得分匹配目标（Hyvärinen，2005）。</p>
<p>尽管动机不同，但DDPMs和NCSNs密切相关。它们都对多种噪声水平使用去噪自动编码器目标，并且都使用与朗之万动力学类似的过程来生成样本（Neal等人，2011）。由于朗之万动力学是梯度流的离散化（Jordan等人，1998），DDPM和NCSN都需要许多步骤才能获得高质量的样本。这与观察到的DDPM和现有的NCSN方法难以在少数几次迭代中生成高质量样本的现象相符。</p>
<p>另一方面，DDIM是一种隐式生成模型（Mohamed和Lakshminarayanan，2016），其中样本由潜在变量唯一确定。因此，DDIM具有一些类似于GANs（Goodfellow等人，2014）和可逆流（Dinh等人，2016）的特性，例如能够进行语义上有意义的插值。我们从纯粹的变分角度推导了DDIM，其中朗之万动力学的限制并不相关；这可以部分解释为什么在较少的迭代次数下，我们能够观察到DDIM的样本质量优于DDPM。DDIM的采样过程也让人联想到具有连续深度的神经网络（Chen等人，2018；Grathwohl等人，2018），因为无论具体的采样轨迹如何，它从相同潜在变量生成的样本都具有相似的高层次视觉特征。</p>
<h3 id="DISCUSSION"><a href="#DISCUSSION" class="headerlink" title="DISCUSSION"></a>DISCUSSION</h3><p>我们从纯粹的变分角度提出了DDIM，这是一种使用去噪自动编码/分数匹配目标进行训练的隐式生成模型。DDIM能够比现有的DDPM和NCSN更高效地生成高质量样本，并且具备从潜在空间进行有意义插值的能力。本文提出的非马尔可夫前向过程表明，除了高斯分布（在原始扩散框架中，高斯是唯一具有有限方差的稳定分布，无法实现其他分布）之外，还可以有连续的前向过程。我们在附录A中展示了一个离散情况下的多项式前向过程示例，研究其他组合结构的类似替代方案将是一个有趣的研究方向。</p>
<p>此外，由于DDIM的采样过程与神经ODE相似，研究减少ODE离散化误差的方法（如Adams - Bashforth等多步方法（Butcher和Goodwin，2008））是否有助于在更少的步骤中进一步提高样本质量，将是一个有趣的研究点（Queiruga等，2020）。研究DDIM是否展现出与现有隐式模型（Bau等，2019）类似的其他属性，也是很有意义的。</p>
<h3 id="ACKNOWLEDGEMENTS"><a href="#ACKNOWLEDGEMENTS" class="headerlink" title="ACKNOWLEDGEMENTS"></a>ACKNOWLEDGEMENTS</h3><p>作者们想要感谢杨松和赵胜家就相关想法进行的有益讨论，感谢Kuno Kim对论文初稿的审阅，以及感谢Sharvil Nanavati和Sophie Liu帮忙找出文中的错别字。本研究得到了美国国家科学基金会（NSF，项目编号：#1651565、#1522054、#1733686）、美国海军研究办公室（ONR，项目编号：N00014 - 19 - 1 - 2145）、美国空军科学研究办公室（AFOSR，项目编号：FA9550 - 19 - 1 - 0024）以及亚马逊云服务（Amazon AWS）的支持。</p>
<h3 id="A-NON-MARKOVIAN-FORWARD-PROCESSES-FOR-A-DISCRETE-CASE"><a href="#A-NON-MARKOVIAN-FORWARD-PROCESSES-FOR-A-DISCRETE-CASE" class="headerlink" title="A NON-MARKOVIAN FORWARD PROCESSES FOR A DISCRETE CASE"></a>A NON-MARKOVIAN FORWARD PROCESSES FOR A DISCRETE CASE</h3><p>在本节中，我们描述了离散数据的非马尔可夫前向过程以及相应的变分目标。由于本文的重点是加速与高斯扩散对应的逆向模型，我们将实证评估留作未来的工作。</p>
<p>对于一个分类观测值$x_0$，它是一个具有$K$个可能值的独热向量，我们定义前向过程如下。首先，我们将$q(x_t|x_0)$定义为以下分类分布：</p>
<script type="math/tex; mode=display">q(x_t|x_0) = Cat(\alpha_tx_0 + (1 - \alpha_t)\mathbf{1}_K) \quad(17)</script><p>其中，$\mathbf{1}_K \in \mathbb{R}^K$是一个所有元素都为$1/K$的向量，$\alpha_t$从$t = 0$时的$\alpha_0 = 1$逐渐减小到$t = T$时的$\alpha_T = 0$ 。然后，我们将$q(x_{t - 1}|x_t, x_0)$定义为以下混合分布：</p>
<script type="math/tex; mode=display">q(x_{t - 1}|x_t, x_0) =
\begin{cases}
Cat(x_t) & \text{概率为 } \sigma_t \\
Cat(x_0) & \text{概率为 } (\alpha_{t - 1} - \sigma_t\alpha_t) \\
Cat(\mathbf{1}_K) & \text{概率为 } (1 - \alpha_{t - 1}) - (1 - \alpha_t)\sigma_t
\end{cases} \quad(18)</script><p>或者等价地表示为：</p>
<script type="math/tex; mode=display">q(x_{t - 1}|x_t, x_0) = Cat(\sigma_tx_t + (\alpha_{t - 1} - \sigma_t\alpha_t)x_0 + ((1 - \alpha_{t - 1}) - (1 - \alpha_t)\sigma_t)\mathbf{1}_K) \quad(19)</script><p>这与我们对$q(x_t|x_0)$的定义是一致的。</p>
<p>类似地，我们可以将逆向过程$p_{\theta}(x_{t - 1}|x_t)$定义为：</p>
<script type="math/tex; mode=display">p_{\theta}(x_{t - 1}|x_t) = Cat(\sigma_tx_t + (\alpha_{t - 1} - \sigma_t\alpha_t)f_{\theta}^{(t)}(x_t) + ((1 - \alpha_{t - 1}) - (1 - \alpha_t)\sigma_t)\mathbf{1}_K) \quad(20)</script><p>其中，$f_{\theta}^{(t)}(x_t)$将$x_t$映射到一个$K$维向量。当$(1 - \alpha_{t - 1}) - (1 - \alpha_t)\sigma_t \to 0$时，采样过程的随机性会降低，即它将以高概率选择$x_t$或预测的$x_0$ 。</p>
<p>KL散度$D_{KL}(q(x_{t - 1}|x_t, x_0) || p_{\theta}(x_{t - 1}|x_t)) \quad(21)$是定义明确的，它就是两个分类分布之间的KL散度。因此，得到的变分目标函数也应该很容易优化。此外，由于KL散度是凸函数，我们有以下上界（当右侧趋近于零时，该上界是紧的）：</p>
<script type="math/tex; mode=display">D_{KL}(q(x_{t - 1}|x_t, x_0) || p_{\theta}(x_{t - 1}|x_t)) \leq (\alpha_{t - 1} - \sigma_t\alpha_t)D_{KL}(Cat(x_0) || Cat(f_{\theta}^{(t)}(x_t)))</script><p>右侧实际上就是一个多分类损失（相差一个常数项），所以我们可以得出类似的结论，即$\sigma_t$的变化不会影响目标函数（除了重新加权）。</p>
<h3 id="B-PROOFS"><a href="#B-PROOFS" class="headerlink" title="B PROOFS"></a>B PROOFS</h3><p><strong>引理1</strong>：对于公式（6）定义的$q_{\sigma}(x_{1:T}|x_0)$和公式（7）定义的$q_{\sigma}(x_{t - 1}|x_t, x_0)$，有：</p>
<script type="math/tex; mode=display">q_{\sigma}(x_t|x_0)=\mathcal{N}(\sqrt{\alpha_t}x_0, (1 - \alpha_t)I) \tag{22}</script><p><strong>证明</strong>：假设对于任意$t\leq T$，$q_{\sigma}(x_t|x_0)=\mathcal{N}(\sqrt{\alpha_t}x_0, (1 - \alpha_t)I)$成立。若：</p>
<script type="math/tex; mode=display">q_{\sigma}(x_{t - 1}|x_0)=\mathcal{N}(\sqrt{\alpha_{t - 1}}x_0, (1 - \alpha_{t - 1})I) \tag{23}</script><p>那么我们可以通过对$t$从$T$到$1$进行归纳论证来证明该命题，因为基础情况（$t = T$）已经成立。</p>
<p>已知：</p>
<script type="math/tex; mode=display">q_{\sigma}(x_t|x_0)=\mathcal{N}(\sqrt{\alpha_t}x_0, (1 - \alpha_t)I) \tag{24}</script><script type="math/tex; mode=display">q_{\sigma}(x_{t - 1}|x_t, x_0)=\mathcal{N}(\sqrt{\alpha_{t - 1}}x_0 + \sqrt{1 - \alpha_{t - 1} - \sigma_t^2} \cdot \frac{x_t - \sqrt{\alpha_t}x_0}{\sqrt{1 - \alpha_t}}, \sigma_t^2I) \tag{25}</script><p>且</p>
<script type="math/tex; mode=display">q_{\sigma}(x_{t - 1}|x_0):=\int_{x_t}q_{\sigma}(x_t|x_0)q_{\sigma}(x_{t - 1}|x_t, x_0)dx_t \tag{26}</script><p>首先，根据Bishop（2006）中的公式（2.115），$q_{\sigma}(x_{t - 1}|x_0)$是高斯分布，记为$\mathcal{N}(\mu_{t - 1}, \sum_{t - 1})$，其中：</p>
<script type="math/tex; mode=display">\begin{align*}
\mu_{t - 1}&=\sqrt{\alpha_{t - 1}}x_0 + \sqrt{1 - \alpha_{t - 1} - \sigma_t^2} \cdot \frac{\sqrt{\alpha_t}x_0 - \sqrt{\alpha_t}x_0}{\sqrt{1 - \alpha_t}}\\
&=\sqrt{\alpha_{t - 1}}x_0 \tag{27}
\end{align*}</script><p>并且</p>
<script type="math/tex; mode=display">\sum_{t - 1}=\sigma_t^2I + \frac{1 - \alpha_{t - 1} - \sigma_t^2}{1 - \alpha_t}(1 - \alpha_t)I=(1 - \alpha_{t - 1})I \tag{28}</script><p>因此，$q_{\sigma}(x_{t - 1}|x_0)=\mathcal{N}(\sqrt{\alpha_{t - 1}}x_0, (1 - \alpha_{t - 1})I)$，这使得我们可以应用归纳论证。证毕。</p>
<p><strong>定理1</strong>：对于所有$\sigma &gt; 0$，存在$\gamma \in \mathbb{R}_{&gt;0}^T$和$C \in \mathbb{R}$，使得$J_{\sigma}=L_{\gamma}+C$。<br><strong>证明</strong>：根据$J_{\sigma}$的定义：</p>
<script type="math/tex; mode=display">\begin{align*}
J_{\sigma}(\epsilon_{\theta})&:=\mathbb{E}_{x_{0:T}\sim q(x_{0:T})}[\log q_{\sigma}(x_T|x_0) + \sum_{t = 2}^{T}\log q_{\sigma}(x_{t - 1}|x_t, x_0) - \sum_{t = 1}^{T}\log p_{\theta}^{(t)}(x_{t - 1}|x_t)]\\
&\equiv\mathbb{E}_{x_{0:T}\sim q(x_{0:T})}[\sum_{t = 2}^{T}D_{KL}(q_{\sigma}(x_{t - 1}|x_t, x_0) || p_{\theta}^{(t)}(x_{t - 1}|x_t)) - \log p_{\theta}^{(1)}(x_0|x_1)] \tag{29}
\end{align*}</script><p>这里“$\equiv$”表示“在一个不依赖于$\epsilon_{\theta}$（但可能依赖于$q_{\sigma}$）的值的范围内相等”。</p>
<p>对于$t &gt; 1$：</p>
<script type="math/tex; mode=display">
\begin{align*}
&\mathbb{E}_{\boldsymbol{x}_{0},\boldsymbol{x}_{t}\sim q(\boldsymbol{x}_{0},\boldsymbol{x}_{t})}[D_{\mathrm{KL}}(q_{\sigma}(\boldsymbol{x}_{t - 1}|\boldsymbol{x}_{t}, \boldsymbol{x}_{0})||p_{\theta}^{(t)}(\boldsymbol{x}_{t - 1}|\boldsymbol{x}_{t}))]\\
=&\mathbb{E}_{\boldsymbol{x}_{0},\boldsymbol{x}_{t}\sim q(\boldsymbol{x}_{0},\boldsymbol{x}_{t})}[D_{\mathrm{KL}}(q_{\sigma}(\boldsymbol{x}_{t - 1}|\boldsymbol{x}_{t}, \boldsymbol{x}_{0})||q_{\sigma}(\boldsymbol{x}_{t - 1}|\boldsymbol{x}_{t}, f_{\theta}^{(t)}(\boldsymbol{x}_{t})))]\\
\equiv&\mathbb{E}_{\boldsymbol{x}_{0},\boldsymbol{x}_{t}\sim q(\boldsymbol{x}_{0},\boldsymbol{x}_{t})} \left[\frac{\|\boldsymbol{x}_{0} - f_{\theta}^{(t)}(\boldsymbol{x}_{t})\|_{2}^{2}}{2\sigma_{t}^{2}}\right] \tag{30}\\
=&\mathbb{E}_{\boldsymbol{x}_{0}\sim q(\boldsymbol{x}_{0}),\epsilon\sim\mathcal{N}(\boldsymbol{0},\boldsymbol{I}),\boldsymbol{x}_{t}=\sqrt{\alpha_{t}}\boldsymbol{x}_{0}+\sqrt{1 - \alpha_{t}}\epsilon} \left[\frac{\left\|\frac{(\boldsymbol{x}_{t}-\sqrt{1 - \alpha_{t}}\epsilon)}{\sqrt{\alpha_{t}}} - \frac{(\boldsymbol{x}_{t}-\sqrt{1 - \alpha_{t}}\epsilon_{\theta}^{(t)}(\boldsymbol{x}_{t}))}{\sqrt{\alpha_{t}}}\right\|_{2}^{2}}{2\sigma_{t}^{2}}\right] \tag{31}\\
=&\mathbb{E}_{\boldsymbol{x}_{0}\sim q(\boldsymbol{x}_{0}),\epsilon\sim\mathcal{N}(\boldsymbol{0},\boldsymbol{I}),\boldsymbol{x}_{t}=\sqrt{\alpha_{t}}\boldsymbol{x}_{0}+\sqrt{1 - \alpha_{t}}\epsilon} \left[\frac{\|\epsilon - \epsilon_{\theta}^{(t)}(\boldsymbol{x}_{t})\|_{2}^{2}}{2d\sigma_{t}^{2}\alpha_{t}}\right] \tag{32}
\end{align*}</script><p>其中$d$是$x_0$的维度。</p>
<p>对于$t = 1$：</p>
<script type="math/tex; mode=display">\begin{align*}
&\mathbb{E}_{x_0, x_1\sim q(x_0, x_1)}[-\log p_{\theta}^{(1)}(x_0|x_1)]\\
\equiv&\mathbb{E}_{x_0, x_1\sim q(x_0, x_1)}[\frac{\|x_0 - f_{\theta}^{(t)}(x_1)\|_2^2}{2\sigma_1^2}] \tag{33}\\
=&\mathbb{E}_{x_0\sim q(x_0), \epsilon\sim\mathcal{N}(0, I), x_1=\sqrt{\alpha_1}x_0 + \sqrt{1 - \alpha_t}\epsilon}[\frac{\|\epsilon - \epsilon_{\theta}^{(1)}(x_1)\|_2^2}{2d\sigma_1^2\alpha_1}] \tag{34}
\end{align*}</script><p>因此，当对于所有$t \in \{1, \ldots, T\}$，$\gamma_t = 1/(2d\sigma_t^2\alpha_t)$时，对于所有$\epsilon_{\theta}$，有：</p>
<script type="math/tex; mode=display">J_{\sigma}(\epsilon_{\theta})\equiv\sum_{t = 1}^{T}\frac{1}{2d\sigma_t^2\alpha_t}\mathbb{E}[\|\epsilon_{\theta}^{(t)}(x_t) - \epsilon_t\|_2^2]=L_{\gamma}(\epsilon_{\theta}) \tag{35}</script><p>根据“$\equiv$”的定义，有$J_{\sigma}=L_{\gamma}+C$。证毕。</p>
<p><strong>命题1</strong>：公式（14）中的常微分方程（ODE）在最优模型$\epsilon_{\theta}^{(t)}$下，具有与Song等人（2020）中“方差爆炸”随机微分方程（SDE）对应的等价概率流ODE。<br><strong>证明</strong>：在本证明中，我们将$t$视为一个连续、独立的“时间”变量，$x$和$\alpha$视为$t$的函数。首先，通过引入变量$\bar{x}$和$\sigma$，对DDIM和方差爆炸随机微分方程（VE - SDE）进行重新参数化：</p>
<script type="math/tex; mode=display">\bar{x}(t)=\bar{x}(0)+\sigma(t)\epsilon, \epsilon\sim\mathcal{N}(0, I) \tag{36}</script><p>对于$t \in [0, +\infty)$以及一个单调递增的连续函数$\sigma: \mathbb{R}_{\geq0} \to \mathbb{R}_{\geq0}$，且$\sigma(0) = 0$。然后，我们可以定义与DDIM情况相对应的$\alpha(t)$和$x(t)$为：$\bar{x}(t)=\frac{x(t)}{\sqrt{\alpha(t)}}\quad(37)$，$\sigma(t)=\sqrt{\frac{1 - \alpha(t)}{\alpha(t)}}\quad(38)$。</p>
<p>这也意味着：</p>
<script type="math/tex; mode=display">x(t)=\frac{\bar{x}(t)}{\sqrt{\sigma^2(t) + 1}} \tag{39}</script><script type="math/tex; mode=display">\alpha(t)=\frac{1}{1 + \sigma^2(t)} \tag{40}</script><p>这就在$(x, \alpha)$和$(\bar{x}, \sigma)$之间建立了一一对应关系。由公式（4）（注意$\alpha(0) = 1$）：</p>
<script type="math/tex; mode=display">\frac{x(t)}{\sqrt{\alpha(t)}}=\frac{x(0)}{\sqrt{\alpha(0)}}+\sqrt{\frac{1 - \alpha(t)}{\alpha(t)}}\epsilon, \epsilon\sim\mathcal{N}(0, I) \tag{41}</script><p>可以重新参数化为与VE - SDE一致的形式：</p>
<script type="math/tex; mode=display">\bar{x}(t)=\bar{x}(0)+\sigma(t)\epsilon \tag{42}</script><p>现在，我们推导DDIM和VE - SDE的ODE形式，并证明它们是等价的。</p>
<ul>
<li><strong>DDIM的ODE形式</strong>：这里重复公式（13）：<script type="math/tex; mode=display">\frac{x_{t - \Delta t}}{\sqrt{\alpha_{t - \Delta t}}}=\frac{x_t}{\sqrt{\alpha_t}}+(\sqrt{\frac{1 - \alpha_{t - \Delta t}}{\alpha_{t - \Delta t}}}-\sqrt{\frac{1 - \alpha_t}{\alpha_t}})\epsilon_{\theta}^{(t)}(x_t) \tag{13}</script>等价于：<script type="math/tex; mode=display">\bar{x}(t - \Delta t)=\bar{x}(t)+(\sigma(t - \Delta t)-\sigma(t))\cdot\epsilon_{\theta}^{(t)}(x(t)) \tag{43}</script>两边同时除以$(-\Delta t)$，并令$\Delta t \to 0$，得到：<script type="math/tex; mode=display">\frac{d\bar{x}(t)}{dt}=\frac{d\sigma(t)}{dt}\epsilon_{\theta}^{(t)}(\frac{\bar{x}(t)}{\sqrt{\sigma^2(t) + 1}}) \tag{45}</script>这正是公式（14）中的形式。我们注意到，对于最优模型，$\epsilon_{\theta}^{(t)}$是一个极小化器：<script type="math/tex; mode=display">\epsilon_{\theta}^{(t)}=\underset{f_t}{\arg\min}\mathbb{E}_{x(0)\sim q(x), \epsilon\sim\mathcal{N}(0, I)}[\|f_t(x(t)) - \epsilon\|_2^2] \tag{44}</script>其中$x(t)=\sqrt{\alpha(t)}x(t)+\sqrt{1 - \alpha(t)}\epsilon$。</li>
<li><strong>VE - SDE的ODE形式</strong>：定义$p_t(\bar{x})$为用方差为$\sigma^2(t)$的高斯噪声扰动后的数据分布。VE - SDE的概率流定义为（Song等人，2020）：<script type="math/tex; mode=display">d\bar{x}=-\frac{1}{2}g(t)^2\nabla_{\bar{x}}\log p_t(\bar{x})dt \tag{47}</script>其中$g(t)=\sqrt{\frac{d\sigma^2(t)}{dt}}$是扩散系数，$\nabla_{\bar{x}}\log p_t(\bar{x})$是$p_t$的得分。</li>
</ul>
<p>$\sigma(t)$扰动的得分函数$\nabla_{\bar{x}}\log p_t(\bar{x})$也是一个极小化器（根据去噪得分匹配，Vincent，2011）：</p>
<script type="math/tex; mode=display">\nabla_{\bar{x}}\log p_t=\underset{g_t}{\arg\min}\mathbb{E}_{x(0)\sim q(x), \epsilon\sim\mathcal{N}(0, I)}[\|g_t(\bar{x})+\epsilon / \sigma(t)\|_2^2] \tag{48}</script><p>其中$\bar{x}(t)=\bar{x}(t)+\sigma(t)\epsilon$。</p>
<p>由于$x(t)$和$\bar{x}(t)$之间存在等价关系，由公式（46）和公式（48）可得以下关系：</p>
<script type="math/tex; mode=display">\nabla_{\bar{x}}\log p_t(\bar{x})=-\frac{\epsilon_{\theta}^{(t)}(\frac{\bar{x}(t)}{\sqrt{\sigma^2(t) + 1}})}{\sigma(t)} \tag{49}</script><p>将公式（49）和$g(t)$的定义代入公式（47），得到：</p>
<script type="math/tex; mode=display">d\bar{x}(t)=\frac{1}{2}\frac{d\sigma^2(t)}{dt}\frac{\epsilon_{\theta}^{(t)}(\frac{\bar{x}(t)}{\sqrt{\sigma^2(t) + 1}})}{\sigma(t)}dt \tag{50}</script><p>整理后可得：</p>
<script type="math/tex; mode=display">\frac{d\bar{x}(t)}{dt}=\frac{d\sigma(t)}{dt}\epsilon_{\theta}^{(t)}(\frac{\bar{x}(t)}{\sqrt{\sigma^2(t) + 1}}) \tag{51}</script><p>在两种情况下，初始条件都是$\bar{x}(T)\sim\mathcal{N}(0, \sigma^2(T)I)$，所以得到的ODE是相同的。证毕。</p>
<h3 id="C-ADDITIONALDERIVATIONS"><a href="#C-ADDITIONALDERIVATIONS" class="headerlink" title="C ADDITIONALDERIVATIONS"></a>C ADDITIONALDERIVATIONS</h3><h4 id="C-1-加速采样过程"><a href="#C-1-加速采样过程" class="headerlink" title="C.1 加速采样过程"></a>C.1 加速采样过程</h4><p>在加速情况下，我们可以将推理过程分解为：</p>
<script type="math/tex; mode=display">q_{\sigma, \tau}\left(x_{1: T} | x_{0}\right)=q_{\sigma, \tau}\left(x_{\tau_{S}} | x_{0}\right) \prod_{i=1}^{S} q_{\sigma, \tau}\left(x_{\tau_{i-1}} | x_{\tau_{i}}, x_{0}\right) \prod_{t \in \overline{\tau}} q_{\sigma, \tau}\left(x_{t} | x_{0}\right) \tag{52}</script><p>其中，$\tau$是$[1,\ldots,T]$的一个长度为$S$的子序列，且$\tau_{S}=T$，令$\bar{\tau}:=\{1,\ldots,T\} \setminus \tau$为其补集。直观地说，$\{x_{\tau_{i}}\}_{i = 1}^{S}$和$x_{0}$的图形模型形成一个链，而$\{x_{t}\}_{t \in \bar{\tau}}$和$x_{0}$的图形模型形成一个星型图。我们定义：</p>
<script type="math/tex; mode=display">q_{\sigma, \tau}\left(x_{t} | x_{0}\right)=\mathcal{N}\left(\sqrt{\alpha_{t}} x_{0},\left(1-\alpha_{t}\right) I\right) \quad \forall t \in \overline{\tau} \cup\{T\} \tag{53}</script><script type="math/tex; mode=display">q_{\sigma, \tau}\left(x_{\tau_{i-1}} | x_{\tau_{i}}, x_{0}\right)=\mathcal{N}\left(\sqrt{\alpha_{\tau_{i-1}}} x_{0}+\sqrt{1-\alpha_{\tau_{i-1}}-\sigma_{\tau_{i}}^{2}} \cdot \frac{x_{\tau_{i}}-\sqrt{\alpha_{\tau_{i}}} x_{0}}{\sqrt{1-\alpha_{\tau_{i}}}}, \sigma_{\tau_{i}}^{2} I\right) \quad \forall i \in [S]</script><p>选择这些系数是为了使得：</p>
<script type="math/tex; mode=display">q_{\sigma, \tau}\left(x_{\tau_{i}} | x_{0}\right)=\mathcal{N}\left(\sqrt{\alpha_{\tau_{i}}} x_{0},\left(1-\alpha_{\tau_{i}}\right) I\right) \quad \forall i \in [S] \tag{54}</script><p>即，“边缘分布”相匹配。</p>
<p>相应的“生成过程”定义为：</p>
<script type="math/tex; mode=display">p_{\theta}\left(x_{0: T}\right):=\underbrace{p_{\theta}\left(x_{T}\right) \prod_{i=1}^{S} p_{\theta}^{\left(\tau_{i}\right)}\left(x_{\tau_{i-1}} | x_{\tau_{i}}\right)}_{\text{用于生成样本}} \times \underbrace{\prod_{t \in \overline{\tau}} p_{\theta}^{(t)}\left(x_{0} | x_{t}\right)}_{\text{用于变分目标}} \tag{55}</script><p>其中只有部分模型实际用于生成样本。条件分布为：</p>
<script type="math/tex; mode=display">p_{\theta}^{\left(\tau_{i}\right)}\left(x_{\tau_{i-1}} | x_{\tau_{i}}\right)=q_{\sigma, \tau}\left(x_{\tau_{i-1}} | x_{\tau_{i}}, f_{\theta}^{\left(\tau_{i}\right)}\left(x_{\tau_{i-1}}\right)\right) \quad \text{如果 } i \in [S], i>1 \tag{56}</script><script type="math/tex; mode=display">p_{\theta}^{(t)}\left(x_{0} | x_{t}\right)=\mathcal{N}\left(f_{\theta}^{(t)}\left(x_{t}\right), \sigma_{t}^{2} I\right) \quad \text{否则} \tag{57}</script><p>在这里，我们将$q_{\sigma, \tau}(x_{\tau_{i-1}} | x_{\tau_{i}}, x_{0})$作为推理过程的一部分（与我们在第3节中所做的类似）。得到的变分目标变为（为简洁起见，定义$x_{\tau_{L + 1}}=\varnothing$）：</p>
<script type="math/tex; mode=display">
\begin{align*}
J(\epsilon_{\theta})&=\mathbb{E}_{\boldsymbol{x}_{0:T}\sim q_{\sigma,\tau}(\boldsymbol{x}_{0:T})}\left[\log q_{\sigma,\tau}(\boldsymbol{x}_{1:T}|\boldsymbol{x}_{0})-\log p_{\theta}(\boldsymbol{x}_{0:T})\right] \tag{58}\\
&=\mathbb{E}_{\boldsymbol{x}_{0:T}\sim q_{\sigma,\tau}(\boldsymbol{x}_{0:T})}\left[\sum_{t\in\overline{\tau}}D_{\mathrm{KL}}(q_{\sigma,\tau}(\boldsymbol{x}_{t}|\boldsymbol{x}_{0})\|p_{\theta}^{(t)}(\boldsymbol{x}_{0}|\boldsymbol{x}_{t}))\right.\\
&\left.\quad+\sum_{i = 1}^{L}D_{\mathrm{KL}}(q_{\sigma,\tau}(\boldsymbol{x}_{\tau_{i - 1}}|\boldsymbol{x}_{\tau_{i}},\boldsymbol{x}_{0})\|p_{\theta}^{(\tau_{i})}(\boldsymbol{x}_{\tau_{i - 1}}|\boldsymbol{x}_{\tau_{i}}))\right] \tag{59}
\end{align*}</script><p>其中每个KL散度是在两个方差与$\theta$无关的高斯分布之间计算的。与定理1证明中类似的论证可以表明，变分目标$J$也可以转换为$L_{\gamma}$形式的目标。</p>
<h4 id="C-2-去噪扩散概率模型（DDPMs）去噪目标的推导"><a href="#C-2-去噪扩散概率模型（DDPMs）去噪目标的推导" class="headerlink" title="C.2 去噪扩散概率模型（DDPMs）去噪目标的推导"></a>C.2 去噪扩散概率模型（DDPMs）去噪目标的推导</h4><p>我们注意到，在Ho等人（2020）的研究中，首先引入了一个扩散超参数$\beta_{t}$ ，然后定义了相关变量$\alpha_{t}:=1 - \beta_{t}$和$\bar{\alpha}_{t}=\prod_{t = 1}^{T} \alpha_{t}$。在本文中，我们使用符号$\alpha_{t}$来表示Ho等人（2020）中的变量$\bar{\alpha}_{t}$，原因有三点。第一，这使得我们仅需选择一组超参数，减少了对派生变量可能产生的混淆引用。第二，它让我们更轻松地引入泛化和加速的情况，因为推理过程不再受扩散过程的限制。第三，在$\alpha_{1: T}$和$1,\ldots,T$之间存在同构关系，而$\beta_{t}$并不具备这种特性。在本节中，为了与Ho等人（2020）的推导保持一致，我们使用$\beta_{t}$和$\alpha_{t}$ ，其中：</p>
<script type="math/tex; mode=display">\alpha_{t}=\frac{\alpha_{t}}{\alpha_{t - 1}} \tag{60}</script><script type="math/tex; mode=display">\beta_{t}=1-\frac{\alpha_{t}}{\alpha_{t - 1}} \tag{61}</script><p>它们可以由$\alpha_{t}$（即$\bar{\alpha}_{t}$ ）唯一确定。</p>
<p>首先，从扩散正向过程可得：</p>
<script type="math/tex; mode=display">q\left(x_{t - 1} | x_{t}, x_{0}\right)=\mathcal{N}(\underbrace{\frac{\sqrt{\alpha_{t - 1}} \beta_{t}}{1 - \alpha_{t}} x_{0}+\frac{\sqrt{\alpha_{t}}\left(1 - \alpha_{t - 1}\right)}{1 - \alpha_{t}} x_{t}}_{\tilde{\mu}\left(x_{t}, x_{0}\right)}, \frac{1 - \alpha_{t - 1}}{1 - \alpha_{t}} \beta_{t} I)</script><p>Ho等人（2020）考虑了一种特定类型的$p_{\theta}^{(t)}(x_{t - 1} | x_{t})$：</p>
<script type="math/tex; mode=display">p_{\theta}^{(t)}\left(x_{t - 1} | x_{t}\right)=\mathcal{N}\left(\mu_{\theta}\left(x_{t}, t\right), \sigma_{t} I\right) \tag{62}</script><p>这导致了以下变分目标：</p>
<script type="math/tex; mode=display">
\begin{align*}
L&:=\mathbb{E}_{\boldsymbol{x}_{0:T}\sim q(\boldsymbol{x}_{0:T})}\left[q(\boldsymbol{x}_{T}|\boldsymbol{x}_{0})+\sum_{t = 2}^{T}\log q(\boldsymbol{x}_{t - 1}|\boldsymbol{x}_{t},\boldsymbol{x}_{0})-\sum_{t = 1}^{T}\log p_{\theta}^{(t)}(\boldsymbol{x}_{t - 1}|\boldsymbol{x}_{t})\right] \tag{63}\\
&\equiv\mathbb{E}_{\boldsymbol{x}_{0:T}\sim q(\boldsymbol{x}_{0:T})}\left[\sum_{t = 2}^{T}\underbrace{D_{\mathrm{KL}}(q(\boldsymbol{x}_{t - 1}|\boldsymbol{x}_{t},\boldsymbol{x}_{0})\|p_{\theta}^{(t)}(\boldsymbol{x}_{t - 1}|\boldsymbol{x}_{t}))}_{L_{t - 1}}-\log p_{\theta}^{(1)}(\boldsymbol{x}_{0}|\boldsymbol{x}_{1})\right]
\end{align*}</script><p>可以写成：</p>
<script type="math/tex; mode=display">L_{t - 1}=\mathbb{E}_{q}\left[\frac{1}{2 \sigma_{t}^{2}}\left\|\mu_{\theta}\left(x_{t}, t\right)-\tilde{\mu}\left(x_{t}, x_{0}\right)\right\|_{2}^{2}\right] \tag{64}</script><p>Ho等人（2020）选择的参数化形式为：</p>
<script type="math/tex; mode=display">\mu_{\theta}\left(x_{t}, t\right)=\frac{1}{\sqrt{\alpha_{t}}}\left(x_{t}-\frac{\beta_{t}}{\sqrt{1 - \alpha_{t}}} \epsilon_{\theta}\left(x_{t}, t\right)\right) \tag{65}</script><p>这可以简化为：</p>
<script type="math/tex; mode=display">L_{t - 1}=\mathbb{E}_{x_{0}, \epsilon}\left[\frac{\beta_{t}^{2}}{2 \sigma_{t}^{2}\left(1 - \alpha_{t}\right) \alpha_{t}}\left\|\epsilon-\epsilon_{\theta}\left(\sqrt{\alpha_{t}} x_{0}+\sqrt{1 - \alpha_{t}} \epsilon, t\right)\right\|_{2}^{2}\right] \tag{66}</script><h3 id="D-EXPERIMENTALDETAILS"><a href="#D-EXPERIMENTALDETAILS" class="headerlink" title="D EXPERIMENTALDETAILS"></a>D EXPERIMENTALDETAILS</h3><h4 id="D-1数据集和架构"><a href="#D-1数据集和架构" class="headerlink" title="D.1数据集和架构"></a>D.1数据集和架构</h4><p>我们考虑了4个具有不同分辨率的图像数据集：CIFAR10（32×32，无条件）、CelebA（64×64）、LSUN卧室（256×256）和LSUN教堂（256×256）。对于所有数据集，我们根据（Ho等人，2020年）中的启发式方法设置超参数α，以便直接比较结果。我们为每个数据集使用相同的模型，仅比较不同生成过程的性能。对于CIFAR10、卧室和教堂数据集，我们从原始的去噪扩散概率模型（DDPM）实现中获取预训练的检查点；对于CelebA数据集，我们使用去噪目标L1训练了自己的模型。</p>
<p>我们的$\epsilon_{\theta}^{(t)}(x_{t})$架构沿用了Ho等人（2020年）的设计，这是一种基于宽残差网络（Zagoruyko和Komodakis，2016年）的U型网络（Ronneberger等人，2015年）。对于CIFAR10、卧室和教堂数据集，我们使用Ho等人（2020年）的预训练模型；对于64×64的CelebA模型（因为没有提供预训练模型），我们训练了自己的模型。我们的CelebA模型有五个特征图分辨率，从64×64到4×4，并且我们使用原始的CelebA数据集（而非CelebA-HQ），并采用了StyleGAN（Karras等人，2018年）代码库中的预处理技术。</p>
<p><img src="t_3.png" alt=""></p>
<h4 id="D-2反向过程子序列选择"><a href="#D-2反向过程子序列选择" class="headerlink" title="D.2反向过程子序列选择"></a>D.2反向过程子序列选择</h4><p>当期望的$dim(\tau)&lt;T$时，我们考虑两种选择$\tau$的过程：</p>
<ul>
<li><strong>线性选择</strong>：我们选择时间步，使得对于某个c，$\tau_{i}=\lfloor ci\rfloor$。</li>
<li><strong>二次选择</strong>：我们选择时间步，使得对于某个c，$\tau_{i}=\left\lfloor ci^{2}\right\rfloor$。</li>
</ul>
<p>常数c的选择应使$\tau_{-1}$接近T。我们对CIFAR10数据集使用二次选择，对其余数据集使用线性选择。这些选择在各自的数据集中比其他选择实现了略好的FID。</p>
<h4 id="D-3每个采样步骤的封闭形式方程"><a href="#D-3每个采样步骤的封闭形式方程" class="headerlink" title="D.3每个采样步骤的封闭形式方程"></a>D.3每个采样步骤的封闭形式方程</h4><p>根据公式（12）中的一般采样方程，我们有以下更新方程：</p>
<script type="math/tex; mode=display">x_{\tau_{i-1}}(\eta)=\sqrt{\alpha_{\tau_{i-1}}}\left(\frac{x_{\tau_{i}}-\sqrt{1-\alpha_{\tau_{i}}} \epsilon_{\theta}^{\left(\tau_{i}\right)}\left(x_{\tau_{i}}\right)}{\sqrt{\alpha_{\tau_{i}}}}\right)+\sqrt{1-\alpha_{\tau_{i-1}}-\sigma_{\tau_{i}}(\eta)^{2}} \cdot \epsilon_{\theta}^{\left(\tau_{i}\right)}\left(x_{\tau_{i}}\right)+\sigma_{\tau_{i}}(\eta) \epsilon_{i}</script><p>其中，$\sigma_{\tau_{i}}(\eta)=\eta \sqrt{\frac{1-\alpha_{\tau_{i-1}}}{1-\alpha_{\tau_{i}}}} \sqrt{1-\frac{\alpha_{\tau_{i}}}{\alpha_{\tau_{i-1}}}}$。<br><img src="f_7.png" alt=""><br>对于$\hat{\sigma}$（具有更大方差的DDPM）的情况，更新方程变为：</p>
<script type="math/tex; mode=display">x_{\tau_{i-1}}=\sqrt{\alpha_{\tau_{i-1}}}\left(\frac{x_{\tau_{i}}-\sqrt{1-\alpha_{\tau_{i}}} \epsilon_{\theta}^{\left(\tau_{i}\right)}\left(x_{\tau_{i}}\right)}{\sqrt{\alpha_{\tau_{i}}}}\right)+\sqrt{1-\alpha_{\tau_{i-1}}-\sigma_{\tau_{i}}(1)^{2}} \cdot \epsilon_{\theta}^{\left(\tau_{i}\right)}\left(x_{\tau_{i}}\right)+\hat{\sigma}_{\tau_{i}} \epsilon</script><p>与$\eta = 1$的更新相比，它对$\epsilon$使用了不同的系数，但对非随机部分使用了相同的系数。这种更新比$\eta = 1$的更新更具随机性，这解释了为什么在$dim(\tau)$较小时它的性能更差。</p>
<h4 id="D-4样本和一致性"><a href="#D-4样本和一致性" class="headerlink" title="D.4样本和一致性"></a>D.4样本和一致性</h4><p>我们在图7（CIFAR10）、图8（CelebA）、图10（教堂）中展示了更多样本，并在图9（CelebA）中展示了DDIM的一致性结果。</p>
<h4 id="D-5插值"><a href="#D-5插值" class="headerlink" title="D.5插值"></a>D.5插值</h4><p>为了在一条线上生成插值，我们从标准高斯分布中随机采样两个初始$x_{T}$值，使用球面线性插值（Shoemake，1985年）对它们进行插值，然后使用DDIM获得$x_{0}$样本。</p>
<script type="math/tex; mode=display">x_{T}^{(\alpha)}=\frac{\sin ((1-\alpha) \theta)}{\sin (\theta)} x_{T}^{(0)}+\frac{\sin (\alpha \theta)}{\sin (\theta)} x_{T}^{(1)} \tag{67}</script><p>其中，$\theta=\arccos (\frac{(x_{T}^{(0)})^{\top} x_{T}^{(1)}}{\left|x_{T}^{(0)}\right|\left|x_{T}^{(1)}\right|})$。这些值用于生成DDIM样本。</p>
<p>为了在网格上生成插值，我们采样四个潜在变量并将它们分成两对；然后，我们对同一$\alpha$下的两对使用球面线性插值，并在两对之间对插值后的样本使用球面线性插值（在独立选择的插值系数下）。我们在图11（CelebA）、图12（卧室）和图13（教堂）中展示了更多的网格插值结果。</p>
<p><img src="f_8.png" alt=""><br><img src="f_9.png" alt=""><br><img src="f_10.png" alt=""><br><img src="f_11.png" alt=""><br><img src="f_12.png" alt=""><br><img src="f_13.png" alt=""></p>
<h2 id="疑问"><a href="#疑问" class="headerlink" title="疑问"></a>疑问</h2><h3 id="BACKGROUND-1"><a href="#BACKGROUND-1" class="headerlink" title="BACKGROUND"></a>BACKGROUND</h3><h4 id="p-theta-x-0-int-p-theta-x-0-T-dx-1-T"><a href="#p-theta-x-0-int-p-theta-x-0-T-dx-1-T" class="headerlink" title="$p_{\theta}(x_{0}) = \int p_{\theta}(x_{0:T})dx_{1:T}$"></a>$p_{\theta}(x_{0}) = \int p_{\theta}(x_{0:T})dx_{1:T}$</h4><p>在去噪扩散概率模型（DDPMs）的框架下，公式$p_{\theta}(x_{0}) = \int p_{\theta}(x_{0:T})dx_{1:T}$用于描述从模型分布中生成样本的过程，具体解释如下：</p>
<ul>
<li><strong>符号含义</strong><ul>
<li>$p_{\theta}(x_{0})$：表示模型最终生成的目标数据（如图片像素值向量）$x_{0}$的概率分布，其中$\theta$是模型的参数，用于控制分布的具体形式，通过训练调整$\theta$使$p_{\theta}(x_{0})$逼近真实数据分布。</li>
<li>$p_{\theta}(x_{0:T})$：代表联合概率分布，描述了从初始状态$x_{0}$到中间多个潜在状态$x_{1}, x_{2}, \cdots, x_{T}$的联合概率情况，它由先验概率$p_{\theta}(x_{T})$和一系列条件概率$p_{\theta}^{(t)}(x_{t - 1}|x_{t})$（$t = 1, 2, \cdots, T$）相乘得到，即$p_{\theta}(x_{0:T}) := p_{\theta}(x_{T})\prod_{t = 1}^{T}p_{\theta}^{(t)}(x_{t - 1}|x_{t})$ 。</li>
<li>$dx_{1:T}$：是对中间潜在变量$x_{1}$到$x_{T}$进行积分的微元，表示对所有可能的中间状态组合进行求和。</li>
</ul>
</li>
<li><strong>公式意义</strong>：该公式本质上是在求<code>边缘概率</code>。在生成模型中，$x_{0}$是最终想要生成的样本，但它的生成依赖于一系列中间潜在变量$x_{1}, \cdots, x_{T}$ 。$p_{\theta}(x_{0:T})$描述了包含中间变量的联合分布情况，通过对中间变量$x_{1}$到$x_{T}$在其整个取值空间上进行积分，就可以得到只关于目标变量$x_{0}$的概率分布$p_{\theta}(x_{0})$。这就好比在一个复杂的生成过程中，把中间步骤所有可能的变化情况都考虑进来，最终得到生成某个特定样本$x_{0}$的概率。在DDPMs训练时，通过调整$\theta$使$p_{\theta}(x_{0})$接近真实数据分布，从而让模型学会生成逼真的样本。</li>
</ul>
<h3 id="VARIATIONAL-INFERENCE-FOR-NON-MARKOVIAN-FORWARD-PROCESSES-1"><a href="#VARIATIONAL-INFERENCE-FOR-NON-MARKOVIAN-FORWARD-PROCESSES-1" class="headerlink" title="VARIATIONAL INFERENCE FOR NON-MARKOVIAN FORWARD PROCESSES"></a>VARIATIONAL INFERENCE FOR NON-MARKOVIAN FORWARD PROCESSES</h3><h4 id="q-sigma-x-1-T-x-0-q-sigma-x-T-x-0-prod-t-2-T-q-sigma-x-t-1-x-t-x-0-的推导"><a href="#q-sigma-x-1-T-x-0-q-sigma-x-T-x-0-prod-t-2-T-q-sigma-x-t-1-x-t-x-0-的推导" class="headerlink" title="$q_{\sigma}(x_{1:T}|x_{0}) := q_{\sigma}(x_{T}|x_{0})\prod_{t = 2}^{T}q_{\sigma}(x_{t - 1}|x_{t}, x_{0})$的推导"></a>$q_{\sigma}(x_{1:T}|x_{0}) := q_{\sigma}(x_{T}|x_{0})\prod_{t = 2}^{T}q_{\sigma}(x_{t - 1}|x_{t}, x_{0})$的推导</h4><ul>
<li>DDPM令$q(x_{t}|x_{t-1},…x_{0}) = q_(x_{t}|x_{t-1})$，构造了一个马尔可夫过程，使得$q\left(x_{1: T} | x_{0}\right):=\prod_{t=1}^{T} q\left(x_{t} | x_{t-1}\right), q\left(x_{t} | x_{t-1}\right):=\mathcal{N}\left(x_{t} ; \sqrt{1-\beta_{t}} x_{t-1}, \beta_{t} I\right)$</li>
<li>DDIM令$q(x_{t}|x_{t-1},…x_{0}) = q(x_{t}|x_{t-1},x_{0})$，构造了一个非马尔可夫过程，使得$q_{\sigma}(x_{1:T}|x_{0}) := q_{\sigma}(x_{T}|x_{0})\prod_{t = 2}^{T}q_{\sigma}(x_{t - 1}|x_{t}, x_{0}) $</li>
<li>具体推导如下：<script type="math/tex; mode=display">
\begin{align*}
q_{\sigma}(x_{1:T}|x_0)&:=q_{\sigma}(x_1|x_0)\cdot q_{\sigma}(x_2|x_1,x_0)\cdot q_{\sigma}(x_3|x_2,x_1,x_0)\cdots q_{\sigma}(x_T|x_{T - 1},\cdots,x_0)\\
&=q_{\sigma}(x_1|x_0)\cdot q_{\sigma}(x_2|x_1,x_0)\cdot q_{\sigma}(x_3|x_2,x_0)\cdots q_{\sigma}(x_T|x_{T - 1},x_0)\\
&=q_{\sigma}(x_1|x_0)\cdot\frac{q_{\sigma}(x_1|x_2,x_0)\cdot q_{\sigma}(x_2|x_0)}{q(x_1|x_0)}\cdot\frac{q_{\sigma}(x_2|x_3,x_0)\cdot q(x_3|x_0)}{q(x_2|x_0)}\cdots\frac{q_{\sigma}(x_{T - 1}|x_T,x_0)\cdot q(x_T,x_0)}{q(x_{T - 1}|x_0)}\\
&=q_{\sigma}(x_{T}|x_{0})\prod_{t = 2}^{T}q_{\sigma}(x_{t - 1}|x_{t}, x_{0})
\end{align*}</script></li>
</ul>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/diffusion/" rel="tag"># diffusion</a>
              <a href="/tags/%E8%AE%BA%E6%96%87/" rel="tag"># 论文</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2025/02/08/latex%E5%85%AC%E5%BC%8F%E5%A4%A7%E5%85%A8/" rel="prev" title="latex公式大全">
      <i class="fa fa-chevron-left"></i> latex公式大全
    </a></div>
      <div class="post-nav-item">
    <a href="/2025/02/18/c_cpp%E5%B8%B8%E7%94%A8%E5%A4%B4%E5%87%BD%E6%95%B0/" rel="next" title="c/c++常用头函数">
      c/c++常用头函数 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#DDIM%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB"><span class="nav-number">1.</span> <span class="nav-text">DDIM论文精读</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%A8%E6%96%87%E7%BF%BB%E8%AF%91"><span class="nav-number">1.1.</span> <span class="nav-text">全文翻译</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Abstract"><span class="nav-number">1.1.1.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Introduction"><span class="nav-number">1.1.2.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#BACKGROUND"><span class="nav-number">1.1.3.</span> <span class="nav-text">BACKGROUND</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#VARIATIONAL-INFERENCE-FOR-NON-MARKOVIAN-FORWARD-PROCESSES"><span class="nav-number">1.1.4.</span> <span class="nav-text">VARIATIONAL INFERENCE FOR NON-MARKOVIAN FORWARD PROCESSES</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#NON-MARKOVIAN-FORWARD-PROCESSES"><span class="nav-number">1.1.4.1.</span> <span class="nav-text">NON-MARKOVIAN FORWARD PROCESSES</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#GENERATIVE-PROCESS-AND-UNIFIED-VARIATIONAL-INFERENCE-OBJECTIVE"><span class="nav-number">1.1.4.2.</span> <span class="nav-text">GENERATIVE PROCESS AND UNIFIED VARIATIONAL INFERENCE OBJECTIVE</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SAMPLING-FROM-GENERALIZED-GENERATIVE-PROCESSES"><span class="nav-number">1.1.5.</span> <span class="nav-text">SAMPLING FROM GENERALIZED GENERATIVE PROCESSES</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#DENOISING-DIFFUSION-IMPLICIT-MODELS"><span class="nav-number">1.1.5.1.</span> <span class="nav-text">DENOISING DIFFUSION IMPLICIT MODELS</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ACCELERATED-GENERATION-PROCESSES"><span class="nav-number">1.1.5.2.</span> <span class="nav-text">ACCELERATED GENERATION PROCESSES</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#RELEVANCE-TO-NEURAL-ODES"><span class="nav-number">1.1.5.3.</span> <span class="nav-text">RELEVANCE TO NEURAL ODES</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#EXPERIMENTS"><span class="nav-number">1.1.6.</span> <span class="nav-text">EXPERIMENTS</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#SAMPLE-QUALITY-AND-EFFICIENCY"><span class="nav-number">1.1.6.1.</span> <span class="nav-text">SAMPLE QUALITY AND EFFICIENCY</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#SAMPLE-CONSISTENCY-IN-DDIMS"><span class="nav-number">1.1.6.2.</span> <span class="nav-text">SAMPLE CONSISTENCY IN DDIMS</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#INTERPOLATION-IN-DETERMINISTIC-GENERATIVE-PROCESSES"><span class="nav-number">1.1.6.3.</span> <span class="nav-text">INTERPOLATION IN DETERMINISTIC GENERATIVE PROCESSES</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#RECONSTRUCTION-FROM-LATENT-SPACE"><span class="nav-number">1.1.6.4.</span> <span class="nav-text">RECONSTRUCTION FROM LATENT SPACE</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RELATED-WORK"><span class="nav-number">1.1.7.</span> <span class="nav-text">RELATED WORK</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DISCUSSION"><span class="nav-number">1.1.8.</span> <span class="nav-text">DISCUSSION</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ACKNOWLEDGEMENTS"><span class="nav-number">1.1.9.</span> <span class="nav-text">ACKNOWLEDGEMENTS</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#A-NON-MARKOVIAN-FORWARD-PROCESSES-FOR-A-DISCRETE-CASE"><span class="nav-number">1.1.10.</span> <span class="nav-text">A NON-MARKOVIAN FORWARD PROCESSES FOR A DISCRETE CASE</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-PROOFS"><span class="nav-number">1.1.11.</span> <span class="nav-text">B PROOFS</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#C-ADDITIONALDERIVATIONS"><span class="nav-number">1.1.12.</span> <span class="nav-text">C ADDITIONALDERIVATIONS</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#C-1-%E5%8A%A0%E9%80%9F%E9%87%87%E6%A0%B7%E8%BF%87%E7%A8%8B"><span class="nav-number">1.1.12.1.</span> <span class="nav-text">C.1 加速采样过程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#C-2-%E5%8E%BB%E5%99%AA%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B%EF%BC%88DDPMs%EF%BC%89%E5%8E%BB%E5%99%AA%E7%9B%AE%E6%A0%87%E7%9A%84%E6%8E%A8%E5%AF%BC"><span class="nav-number">1.1.12.2.</span> <span class="nav-text">C.2 去噪扩散概率模型（DDPMs）去噪目标的推导</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#D-EXPERIMENTALDETAILS"><span class="nav-number">1.1.13.</span> <span class="nav-text">D EXPERIMENTALDETAILS</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#D-1%E6%95%B0%E6%8D%AE%E9%9B%86%E5%92%8C%E6%9E%B6%E6%9E%84"><span class="nav-number">1.1.13.1.</span> <span class="nav-text">D.1数据集和架构</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#D-2%E5%8F%8D%E5%90%91%E8%BF%87%E7%A8%8B%E5%AD%90%E5%BA%8F%E5%88%97%E9%80%89%E6%8B%A9"><span class="nav-number">1.1.13.2.</span> <span class="nav-text">D.2反向过程子序列选择</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#D-3%E6%AF%8F%E4%B8%AA%E9%87%87%E6%A0%B7%E6%AD%A5%E9%AA%A4%E7%9A%84%E5%B0%81%E9%97%AD%E5%BD%A2%E5%BC%8F%E6%96%B9%E7%A8%8B"><span class="nav-number">1.1.13.3.</span> <span class="nav-text">D.3每个采样步骤的封闭形式方程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#D-4%E6%A0%B7%E6%9C%AC%E5%92%8C%E4%B8%80%E8%87%B4%E6%80%A7"><span class="nav-number">1.1.13.4.</span> <span class="nav-text">D.4样本和一致性</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#D-5%E6%8F%92%E5%80%BC"><span class="nav-number">1.1.13.5.</span> <span class="nav-text">D.5插值</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%96%91%E9%97%AE"><span class="nav-number">1.2.</span> <span class="nav-text">疑问</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#BACKGROUND-1"><span class="nav-number">1.2.1.</span> <span class="nav-text">BACKGROUND</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#p-theta-x-0-int-p-theta-x-0-T-dx-1-T"><span class="nav-number">1.2.1.1.</span> <span class="nav-text">$p_{\theta}(x_{0}) &#x3D; \int p_{\theta}(x_{0:T})dx_{1:T}$</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#VARIATIONAL-INFERENCE-FOR-NON-MARKOVIAN-FORWARD-PROCESSES-1"><span class="nav-number">1.2.2.</span> <span class="nav-text">VARIATIONAL INFERENCE FOR NON-MARKOVIAN FORWARD PROCESSES</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#q-sigma-x-1-T-x-0-q-sigma-x-T-x-0-prod-t-2-T-q-sigma-x-t-1-x-t-x-0-%E7%9A%84%E6%8E%A8%E5%AF%BC"><span class="nav-number">1.2.2.1.</span> <span class="nav-text">$q_{\sigma}(x_{1:T}|x_{0}) :&#x3D; q_{\sigma}(x_{T}|x_{0})\prod_{t &#x3D; 2}^{T}q_{\sigma}(x_{t - 1}|x_{t}, x_{0})$的推导</span></a></li></ol></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Zongqing Li"
      src="https://github.com/hqulzq/hqulzq.github.io/blob/main/images/userimg.jpg?raw=true">
  <p class="site-author-name" itemprop="name">Zongqing Li</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">42</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">12</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">31</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/hqulzq" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;hqulzq" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:hqulzq@163.com" title="E-Mail → mailto:hqulzq@163.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

      
<script type="text/javascript" charset="utf-8" src="/js/tagcloud.js"></script>
<script type="text/javascript" charset="utf-8" src="/js/tagcanvas.js"></script>
<div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div id="myCanvasContainer" class="widget tagcloud">
        <canvas width="250" height="250" id="resCanvas" style="width=100%">
            <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Bug%E6%80%BB%E7%BB%93/" rel="tag">Bug总结</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Git/" rel="tag">Git</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Kafka/" rel="tag">Kafka</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Knife4j/" rel="tag">Knife4j</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linux/" rel="tag">Linux</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/MinIO/" rel="tag">MinIO</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Mybatis-Plus/" rel="tag">Mybatis-Plus</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Nginx/" rel="tag">Nginx</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RabbitMQ/" rel="tag">RabbitMQ</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Redis/" rel="tag">Redis</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RocketMQ/" rel="tag">RocketMQ</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/diffusion/" rel="tag">diffusion</a><span class="tag-list-count">10</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/docker/" rel="tag">docker</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/k8s/" rel="tag">k8s</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/latex/" rel="tag">latex</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/leetcode/" rel="tag">leetcode</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mongodb/" rel="tag">mongodb</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mysql/" rel="tag">mysql</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E4%BB%A3%E7%A0%81%E9%9A%8F%E6%83%B3%E5%BD%95/" rel="tag">代码随想录</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%A4%B4%E5%87%BD%E6%95%B0/" rel="tag">头函数</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%B0%9A%E5%BA%AD%E5%85%AC%E5%AF%93/" rel="tag">尚庭公寓</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86/" rel="tag">异常处理</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/" rel="tag">快速入门</a><span class="tag-list-count">13</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93/" rel="tag">技术总结</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E7%BB%84/" rel="tag">数组</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B3%A8%E8%A7%A3/" rel="tag">注解</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%8B%A5%E4%BE%9D/" rel="tag">若依</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%AE%BA%E6%96%87/" rel="tag">论文</a><span class="tag-list-count">9</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/" rel="tag">设计模式</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%93%BE%E8%A1%A8i/" rel="tag">链表i</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98/" rel="tag">项目实战</a><span class="tag-list-count">6</span></li></ul>
        </canvas>
    </div>
</div>



    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zongqing Li</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>









<script>
document.querySelectorAll('.pdfobject-container').forEach(element => {
  let url = element.dataset.target;
  let pdfOpenParams = {
    navpanes : 0,
    toolbar  : 0,
    statusbar: 0,
    pagemode : 'thumbs',
    view     : 'FitH'
  };
  let pdfOpenFragment = '#' + Object.entries(pdfOpenParams).map(([key, value]) => `${key}=${encodeURIComponent(value)}`).join('&');
  let fullURL = `/lib/pdf/web/viewer.html?file=${encodeURIComponent(url)}${pdfOpenFragment}`;

  if (NexT.utils.supportsPDFs()) {
    element.innerHTML = `<embed class="pdfobject" src="${url + pdfOpenFragment}" type="application/pdf" style="height: ${element.dataset.height};">`;
  } else {
    element.innerHTML = `<iframe src="${fullURL}" style="height: ${element.dataset.height};" frameborder="0"></iframe>`;
  }
});
</script>




  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

  
  <style>
    .copy-btn {
      display: inline-block;
      padding: 6px 12px;
      font-size: 13px;
      font-weight: 700;
      line-height: 20px;
      color: #4D4D4C;
      white-space: nowrap;
      vertical-align: middle;
      cursor: pointer;
      background-color: #F7F7F7;
      background-image: linear-gradient(#F7F7F7, #F7F7F7);
      border: 1px solid #d5d5d5;
      border-radius: 3px;
      user-select: none;
      outline: 0;
    }

    .highlight-wrap .copy-btn {
      transition: opacity .3s ease-in-out;
      opacity: 0;
      padding: 2px 6px;
      position: absolute;
      right: 4px;
      top: 8px;
    }

    .highlight-wrap:hover .copy-btn,
    .highlight-wrap .copy-btn:focus {
      opacity: 1
    }

    .highlight-wrap {
      position: relative;
    }
  </style>

  <script>
    $('.highlight').each(function (i, e) {
      var $wrap = $('<div>').addClass('highlight-wrap')
      $(e).after($wrap)
      $wrap.append($('<button>').addClass('copy-btn').append('post.copy_button').on('click', function (e) {
        var code = $(this).parent().find('.code').find('.line').map(function (i, e) {
          return $(e).text()
        }).toArray().join('\n')
        var ta = document.createElement('textarea')
        document.body.appendChild(ta)
        ta.style.position = 'absolute'
        ta.style.top = '0px'
        ta.style.left = '0px'
        ta.value = code
        ta.select()
        ta.focus()
        var result = document.execCommand('copy')
        document.body.removeChild(ta)
        
          if(result)$(this).text('post.copy_success')
          else $(this).text('post.copy_failure')
        
        $(this).blur()
      })).on('mouseleave', function (e) {
        var $b = $(this).find('.copy-btn')
        setTimeout(function () {
          $b.text('post.copy_button')
        }, 300)
      }).append(e)
    })
  </script>


<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/assets/koharu.model.json"},"display":{"position":"right","width":150,"height":300,"hOffset":-15,"vOffset":-15},"mobile":{"show":true},"react":{"opacity":0.7},"log":false});</script></body>
</html>
