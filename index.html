<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"hqulzq.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Lzq&#39;s blog">
<meta property="og:url" content="https://hqulzq.github.io/index.html">
<meta property="og:site_name" content="Lzq&#39;s blog">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Zongqing Li">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://hqulzq.github.io/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>Lzq's blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Lzq's blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">40</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">12</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">45</span></a>

  </li>
        <li class="menu-item menu-item-book">

    <a href="/book" rel="section"><i class="fas fa-book fa-fw"></i>Book</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://hqulzq.github.io/2025/03/08/GLIDE-Towards-Photorealistic-Image-Generation-and-Editing-with-Text-Guided-Diffusion-Models%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://github.com/hqulzq/hqulzq.github.io/blob/main/images/userimg.jpg?raw=true">
      <meta itemprop="name" content="Zongqing Li">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lzq's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/03/08/GLIDE-Towards-Photorealistic-Image-Generation-and-Editing-with-Text-Guided-Diffusion-Models%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/" class="post-title-link" itemprop="url">GLIDE Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models论文精读</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-03-08 11:08:14 / 修改时间：15:36:16" itemprop="dateCreated datePublished" datetime="2025-03-08T11:08:14+08:00">2025-03-08</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="全文翻译"><a href="#全文翻译" class="headerlink" title="全文翻译"></a>全文翻译</h2><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>扩散模型最近已被证明能够生成高质量的合成图像，尤其是在与引导技术相结合，以在多样性和逼真度之间进行权衡时。我们探索了用于文本条件图像合成问题的扩散模型，并比较了两种不同的引导策略：CLIP引导和无分类器引导。我们发现，在逼真度和字幕相似度方面，人类评估者更倾向于后者，并且它通常能生成逼真的样本。使用无分类器引导的35亿参数文本条件扩散模型生成的样本，即使在DALL-E使用昂贵的CLIP重排序的情况下，也更受人类评估者的青睐。此外，我们发现我们的模型可以进行微调以执行图像修复，从而实现强大的文本驱动图像编辑。我们在经过筛选的数据集上训练了一个较小的模型，并在<a target="_blank" rel="noopener" href="https://github.com/openai/glide-text2im上发布了代码和权重。">https://github.com/openai/glide-text2im上发布了代码和权重。</a></p>
<h3 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h3><p>插图、绘画和照片等图像通常可以用文本轻松描述，但创作这些图像可能需要专业技能，且要花费数小时。因此，一种能根据自然语言生成逼真图像的工具，能让人们以前所未有的轻松方式创作丰富多样的视觉内容。而使用自然语言编辑图像的能力，则进一步实现了迭代优化和精细化控制，这两点在实际应用中至关重要。</p>
<p>近期的文本条件图像模型可以根据自由形式的文本提示合成图像，还能以语义合理的方式组合不相关的对象（Xu等人，2017；Zhu等人，2019；Tao等人，2020；Ramesh等人，2021；Zhang等人，2021）。然而，它们仍无法生成能涵盖相应文本提示所有方面的逼真图像。</p>
<p>另一方面，无条件图像模型可以合成逼真的图像（Brock等人，2018；Karras等人，2019a；2019b；Razavi等人，2019），有时其逼真度高到人类无法将它们与真实图像区分开来（Zhou等人，2019）。在这一研究领域，扩散模型（Sohl-Dickstein等人，2015；Song和Ermon，2020b）已成为一类很有前景的生成模型，在许多图像生成基准测试中取得了最先进的样本质量（Ho等人，2020；Dhariwal和Nichol，2021；Ho等人，2021）。</p>
<p>为了在类别条件设定下实现逼真效果，Dhariwal和Nichol（2021）用分类器引导增强了扩散模型，这是一种让扩散模型以分类器的标签为条件的技术。首先在有噪声的图像上训练分类器，在扩散采样过程中，利用分类器的梯度将样本导向目标标签。Ho和Salimans（2021）通过使用无分类器引导，在不单独训练分类器的情况下取得了类似的结果。无分类器引导是一种在有标签和无标签的扩散模型预测之间进行插值的引导形式。</p>
<p>受引导扩散模型生成逼真样本的能力和文本到图像模型处理自由形式提示的能力启发，我们将引导扩散应用于文本条件图像合成问题。首先，我们训练了一个35亿参数的扩散模型，该模型使用文本编码器以自然语言描述为条件。接下来，我们比较了两种将扩散模型导向文本提示的技术：CLIP引导和无分类器引导。通过人工评估和自动评估，我们发现无分类器引导能产生更高质量的图像。</p>
<p>我们发现，使用无分类器引导生成的模型样本既逼真，又反映了广泛的世界知识。在由人类评判进行评估时，在逼真度方面，我们模型生成的样本87% 的情况比DALL-E（Ramesh等人，2021）的样本更受青睐；在字幕相似度方面，这一比例为69% 。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"><img src="f1.png" alt="f1"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><em>图1. 使用无分类器引导的GLIDE模型生成的部分样本。我们观察到，我们的模型能够生成带有阴影和反射的逼真图像，能以正确的方式组合多个概念，还能对新颖概念进行艺术渲染。随机样本网格，请见图17和图18。</em></td>
</tr>
</tbody>
</table>
</div>
<p>虽然我们的模型可以零样本渲染各种各样的文本提示，但对于复杂的提示，它可能难以生成逼真的图像。因此，除了零样本生成，我们还为模型提供了编辑能力，使人们能够迭代地改进模型样本，直到它们符合更复杂的提示。具体来说，我们对模型进行微调以执行图像修复，发现它能够使用自然语言提示对现有图像进行逼真的编辑。模型进行的编辑与周围环境的风格和光照相匹配，包括令人信服的阴影和反射。这些模型未来的应用可能会帮助人们以前所未有的速度和轻松程度创作引人注目的定制图像。</p>
<p>我们注意到，我们最终得到的模型可能会显著降低制作令人信服的虚假信息或深度伪造内容所需的难度。为了在防止这些有害应用的同时支持未来的研究，我们发布了一个较小的扩散模型，以及一个在经过筛选的数据集上训练的带噪声CLIP模型。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"><img src="f2.png" alt="f2"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><em>图2. GLIDE的文本条件图像修复示例。绿色区域被擦除，模型根据给定的提示进行填充。我们的模型能够匹配周围环境的风格和光照，以生成逼真的修复效果。</em></td>
</tr>
</tbody>
</table>
</div>
<p>我们将我们的系统称为GLIDE，代表用于生成和编辑的引导式语言到图像扩散模型（Guided Language to Image Diffusion for Generation and Editing）。我们将经过筛选的小模型称为GLIDE（filtered）。</p>
<h3 id="2-背景"><a href="#2-背景" class="headerlink" title="2. 背景"></a>2. 背景</h3><p>在以下部分，我们将概述最终评估模型的组成部分：扩散模型、无分类器引导和CLIP引导。</p>
<h4 id="2-1-扩散模型"><a href="#2-1-扩散模型" class="headerlink" title="2.1 扩散模型"></a>2.1 扩散模型</h4><p>我们考虑由Sohl-Dickstein等人（2015年）提出，并经Song和Ermon（2020b）以及Ho等人（2020年）改进的高斯扩散模型。给定一个来自数据分布$x_{0} \sim q(x_{0})$的样本，我们通过逐步向该样本添加高斯噪声，生成一个潜在变量的马尔可夫链$x_{1}, \ldots, x_{T}$：</p>
<script type="math/tex; mode=display">q(x_{t}|x_{t - 1}) := \mathcal{N}(x_{t};\sqrt{\alpha_{t}}x_{t - 1}, (1 - \alpha_{t})\mathcal{I})</script><p>如果每一步添加的噪声幅度$1 - \alpha_{t}$足够小，那么后验$q(x_{t - 1}|x_{t})$可以很好地用对角高斯分布近似。此外，如果整个链中添加的总噪声幅度$1 - \alpha_{1} \cdots \alpha_{T}$足够大，$x_{T}$可以很好地用$\mathcal{N}(0, \mathcal{I})$近似。这些特性表明，可以学习一个模型$p_{\theta}(x_{t - 1}|x_{t})$来近似真实的后验：</p>
<script type="math/tex; mode=display">p_{\theta}(x_{t - 1}|x_{t}) := \mathcal{N}(\mu_{\theta}(x_{t}), \sum_{\theta}(x_{t}))</script><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"><img src="f3.png" alt="f3"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><em>图3. 使用GLIDE迭代创建复杂场景。首先，我们根据“温馨的客厅”这一提示生成一幅图像，然后使用所示的图像修复蒙版和后续文本提示，在墙上添加一幅画、添加一张咖啡桌，并在咖啡桌上添加一瓶花，最后将墙移至沙发处。</em></td>
</tr>
</tbody>
</table>
</div>
<p>通过从高斯噪声$x_{T} \sim \mathcal{N}(0, \mathcal{I})$开始，并在一系列步骤$x_{T - 1}$、$x_{T - 2}, \ldots, x_{0}$中逐渐减少噪声，就可以使用这个模型生成样本$x_{0} \sim p_{\theta}(x_{0})$。虽然$log p_{\theta}(x_{0})$存在一个可处理的变分下界，但通过优化一个替代目标（对VLB中的项重新加权）可以得到更好的结果。为了计算这个替代目标，我们通过向$x_{0}$应用高斯噪声$\epsilon$生成样本$x_{t} \sim q(x_{t}|x_{0})$，然后训练一个模型$\epsilon_{\theta}$，使用标准均方误差损失来预测添加的噪声：</p>
<script type="math/tex; mode=display">L_{simple} := \mathbb{E}_{t \sim [1, T], x_{0} \sim q(x_{0}), \epsilon \sim \mathcal{N}(0, \mathcal{I})}[\|\epsilon - \epsilon_{\theta}(x_{t}, t)\|^{2}]</script><p>Ho等人（2020年）展示了如何从$\epsilon_{\theta}(x_{t}, t)$推导出$\mu_{\theta}(x_{t})$，并将$\sum_{\theta}$固定为一个常数。他们还证明了该模型与之前基于去噪得分匹配的模型（Song和Ermon，2020b；2020a）等价，得分函数$\nabla_{x}\log p(x) \propto \epsilon_{\theta}(x_{t}, t)$。在后续工作中，Nichol和Dhariwal（2021年）提出了一种学习$\sum_{\theta}$的策略，使模型能够用更少的扩散步骤生成高质量的样本。在本文中训练模型时，我们采用了这项技术。扩散模型也已成功应用于图像超分辨率（Nichol和Dhariwal，2021年；Saharia等人，2021b）。按照扩散模型的标准公式，高分辨率图像$y_{0}$在一系列步骤中逐渐被添加噪声。然而，$p_{\theta}(y_{t - 1}|y_{t}, x)$额外以降采样后的输入$x$为条件，该输入通过在通道维度上连接（双三次上采样后的）$x$提供给模型。这些模型在FID、IS和人类比较评分方面的结果优于先前的方法。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"><img src = "f4.png" width="40%" height="40%"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><em>图4. 使用GLIDE进行文本条件SDEdit（Meng等人，2021）的示例，用户通过将草图与文本说明相结合，对图像进行更可控的修改。</em></td>
</tr>
</tbody>
</table>
</div>
<h4 id="2-2-引导扩散"><a href="#2-2-引导扩散" class="headerlink" title="2.2 引导扩散"></a>2.2 引导扩散</h4><p>Dhariwal和Nichol（2021年）发现，类别条件扩散模型的样本通常可以通过分类器引导得到改进。在分类器引导中，具有均值$\mu_{\theta}(x_{t}|y)$和方差$\sum_{\theta}(x_{t}|y)$的类别条件扩散模型，会被分类器预测的目标类别$y$的对数概率$log p_{\phi}(y|x_{t})$的梯度进行加性扰动。得到的新的扰动均值$\hat{\mu}_{\theta}(x_{t}|y)$为：</p>
<script type="math/tex; mode=display">\hat{\mu}_{\theta}(x_{t}|y) = \mu_{\theta}(x_{t}|y) + s \cdot \sum_{\theta}(x_{t}|y)\nabla_{x_{t}}\log p_{\phi}(y|x_{t})</script><p>系数$s$称为引导尺度，Dhariwal和Nichol（2021年）发现，增加$s$可以提高样本质量，但会牺牲多样性。</p>
<h4 id="2-3-无分类器引导"><a href="#2-3-无分类器引导" class="headerlink" title="2.3 无分类器引导"></a>2.3 无分类器引导</h4><p>Ho和Salimans（2021年）最近提出了无分类器引导，这是一种引导扩散模型的技术，无需单独训练分类器模型。在无分类器引导中，类别条件扩散模型$\epsilon_{\theta}(x_{t}|y)$中的标签$y$在训练期间以固定概率被替换为一个空标签$\phi$。在采样期间，模型的输出会进一步向$\epsilon_{\theta}(x_{t}|y)$的方向外推，并远离$\epsilon_{\theta}(x_{t}|\emptyset)$，如下所示：</p>
<script type="math/tex; mode=display">\hat{\epsilon}_{\theta}(x_{t}|y) = \epsilon_{\theta}(x_{t}|\emptyset) + s \cdot (\epsilon_{\theta}(x_{t}|y) - \epsilon_{\theta}(x_{t}|\emptyset))</script><p>这里$s \geq 1$是引导尺度。这种函数形式的灵感来自于隐式分类器：</p>
<script type="math/tex; mode=display">p^{i}(y|x_{t}) \propto \frac{p(x_{t}|y)}{p(x_{t})}</script><p>其梯度可以用真实得分$\epsilon^{*}$表示为：</p>
<script type="math/tex; mode=display">\begin{aligned}
\nabla_{x_{t}}\log p^{i}(x_{t}|y) &\propto \nabla_{x_{t}}\log p(x_{t}|y) - \nabla_{x_{t}}\log p(x_{t}) \\
&\propto \epsilon^{*}(x_{t}|y) - \epsilon^{*}(x_{t})
\end{aligned}</script><p>为了使用通用文本提示实现无分类器引导，我们在训练期间有时会用空序列（我们也将其表示为$\emptyset$）替换文本字幕。然后，我们使用修改后的预测$\hat{\epsilon}$向字幕$c$进行引导：</p>
<script type="math/tex; mode=display">\hat{\epsilon}_{\theta}(x_{t}|c) = \epsilon_{\theta}(x_{t}|\emptyset) + s \cdot (\epsilon_{\theta}(x_{t}|c) - \epsilon_{\theta}(x_{t}|\emptyset))</script><p>无分类器引导有两个吸引人的特性。第一，它允许单个模型在引导过程中利用自身的知识，而不是依赖于单独（有时较小）的分类模型的知识。第二，当以分类器难以预测的信息（如文本）为条件时，它简化了引导过程。</p>
<h4 id="2-4-CLIP引导"><a href="#2-4-CLIP引导" class="headerlink" title="2.4 CLIP引导"></a>2.4 CLIP引导</h4><p>Radford等人（2021年）引入了CLIP，这是一种学习文本和图像之间联合表示的可扩展方法。一个CLIP模型由两个独立的部分组成：一个图像编码器$f(x)$和一个字幕编码器$g(c)$。在训练期间，从一个大型数据集中采样$(x, c)$对的批次，模型优化一个对比交叉熵损失，该损失鼓励如果图像$x$与给定的字幕$c$配对，则$f(x) \cdot g(c)$的点积较高；如果图像和字幕对应于训练数据中的不同对，则点积较低。<br>由于CLIP提供了图像与字幕的接近程度的分数，一些工作使用它来引导像GAN这样的生成模型朝着用户定义的文本字幕的方向生成（Galatolo等人，2021年；Patashnik等人，2021年；Murdock，2021年；Gal等人，2021年）。为了将相同的想法应用于扩散模型，我们可以在分类器引导中用CLIP模型替换分类器。具体来说，我们用图像和字幕编码的点积关于图像的梯度来扰动反向过程的均值：</p>
<script type="math/tex; mode=display">\hat{\mu}_{\theta}(x_{t}|c) = \mu_{\theta}(x_{t}|c) + s \cdot \sum_{\theta}(x_{t}|c)\nabla_{x_{t}}(f(x_{t}) \cdot g(c))</script><p>与分类器引导类似，我们必须在有噪声的图像$x_{t}$上训练CLIP，以在反向过程中获得正确的梯度。在我们的所有实验中，我们使用经过明确训练以感知噪声的CLIP模型，我们将其称为带噪CLIP模型。<br>先前的工作Crowson（2021a；2021b）表明，未经有噪声图像训练的公开CLIP模型仍然可以用于引导扩散模型。在附录D中，我们展示了我们的带噪CLIP引导比这种方法表现更好，且无需使用数据增强或感知损失等额外技巧。我们假设使用公开CLIP模型进行引导会对样本质量产生不利影响，因为在采样过程中遇到的有噪声的中间图像对于该模型来说是分布外的数据。</p>
<h3 id="3-相关工作"><a href="#3-相关工作" class="headerlink" title="3. 相关工作"></a>3. 相关工作</h3><p>许多研究致力于解决文本条件图像生成的问题。Xu等人（2017）、Zhu等人（2019）、Tao等人（2020）、Zhang等人（2021）和Ye等人（2021）利用公开的图像字幕数据集训练文本条件生成对抗网络（GANs）。Ramesh等人（2021）在van den Oord等人（2017）方法的基础上，通过训练基于离散潜在代码的自回归生成模型，实现了基于文本的图像合成。与我们的工作同期，Gu等人（2021）在离散潜在代码上训练文本条件离散扩散模型，并发现由此产生的系统能够生成具有竞争力的图像样本。</p>
<p>有几项研究探索了使用扩散模型进行图像修复。Meng等人（2021）发现，扩散模型不仅可以修复图像区域，还能根据图像的粗略草图（或一组颜色）进行修复。Saharia等人（2021a）发现，当直接针对图像修复任务进行训练时，扩散模型可以平滑地修复图像区域，且不会产生边缘伪影。</p>
<p>CLIP此前已被用于引导图像生成。Galatolo等人（2021）、Patashnik等人（2021）、Murdock（2021）和Gal等人（2021）使用CLIP引导GAN生成符合文本提示的图像。在线人工智能生成艺术社区使用未添加噪声的CLIP引导扩散取得了一些有前景的初步成果（Crowson，2021a；2021b）。Kim和Ye（2021）通过微调扩散模型以最小化CLIP损失，同时重构原始图像的DDIM（Song等人，2020a）潜在表示，从而使用文本提示编辑图像。Zhou等人（2021）训练基于扰动CLIP图像嵌入的GAN模型，得到了一种可以基于CLIP文本嵌入生成条件图像的模型。但这些工作都没有探索带噪声的CLIP模型，因此常常依赖数据增强和感知损失。</p>
<p>还有一些研究探索了基于文本的图像编辑。Zhang等人（2020）提出了一种双注意力机制，用于利用文本嵌入修复图像中缺失的区域。Stap等人（2020）提出了一种使用基于文本的特征向量编辑人脸图像的方法。Bau等人（2021）将CLIP与最先进的GAN模型相结合，使用文本目标修复图像。与我们的工作同期，Avrahami等人（2021）使用CLIP引导的扩散来基于文本修复图像区域。</p>
<h3 id="4-训练"><a href="#4-训练" class="headerlink" title="4. 训练"></a>4. 训练</h3><p>在主要实验中，我们训练了一个参数规模达35亿、分辨率为64×64的文本条件扩散模型，还训练了一个15亿参数的文本条件上采样扩散模型，用于将图像分辨率提升至256×256。对于CLIP引导，我们还训练了一个分辨率为64×64的带噪ViT-L CLIP模型（Dosovitskiy等人，2020）。</p>
<h4 id="4-1-文本条件扩散模型"><a href="#4-1-文本条件扩散模型" class="headerlink" title="4.1 文本条件扩散模型"></a>4.1 文本条件扩散模型</h4><p>我们采用了Dhariwal和Nichol（2021）提出的ADM模型架构，并对其进行了扩展，使其能够融入文本条件信息。对于每个带噪图像$x_t$和相应的文本描述$c$，我们的模型会预测$p(x_{t - 1}|x_t, c)$。为了以文本为条件进行预测，我们首先将文本编码成一系列$K$个标记，然后将这些标记输入到Transformer模型（Vaswani等人，2017）中。该Transformer模型的输出会以两种方式被使用：其一，最后一个标记的嵌入会替代ADM模型中的类别嵌入；其二，标记嵌入的最后一层（即一系列$K$个特征向量）会被分别投影到ADM模型中每个注意力层的维度，然后在每一层与注意力上下文进行拼接。</p>
<p>我们在与DALL-E（Ramesh等人，2021）相同的数据集上训练模型。我们采用了与Dhariwal和Nichol（2021）中ImageNet 64×64模型相同的架构，但将模型的宽度扩展到了512个通道，这使得模型视觉部分的参数约为23亿。对于文本编码Transformer，我们使用了24个宽度为2048的残差块，这使得文本编码部分的参数约为12亿。</p>
<p>此外，我们还训练了一个15亿参数的上采样扩散模型，用于将图像分辨率从64×64提升到256×256。该模型以与基础模型相同的方式融入文本条件信息，但使用了一个宽度为1024的较小文本编码器，而非2048。除此之外，该模型的架构与Dhariwal和Nichol（2021）中的ImageNet上采样器相匹配，只是我们将基础通道数增加到了384。</p>
<p>我们对基础模型进行250万次迭代训练，批次大小设为2048。对上采样模型进行160万次迭代训练，批次大小设为512。我们发现，使用16位精度和传统的损失缩放（Micikevicius等人，2017），这些模型能够稳定训练。总的训练计算量大致与训练DALL-E所需的计算量相当。</p>
<h4 id="4-2-针对无分类器引导的微调"><a href="#4-2-针对无分类器引导的微调" class="headerlink" title="4.2 针对无分类器引导的微调"></a>4.2 针对无分类器引导的微调</h4><p>在完成初始训练后，我们对基础模型进行微调，使其支持无条件图像生成。这一训练过程与预训练完全相同，只是20%的文本标记序列会被空序列替代。通过这种方式，模型在保留基于文本条件生成图像能力的同时，也具备了无条件生成图像的能力。</p>
<h4 id="4-3-图像修复"><a href="#4-3-图像修复" class="headerlink" title="4.3 图像修复"></a>4.3 图像修复</h4><p>此前，大多数使用扩散模型进行图像修复的研究并没有专门针对该任务训练扩散模型（Sohl-Dickstein等人，2015；Song等人，2020b；Meng等人，2021）。具体来说，扩散模型的图像修复通常是通过像往常一样从扩散模型中采样来实现的，但在每次采样步骤之后，会用来自$q(x_t|x_0)$的样本替换图像中的已知区域。这种方法的缺点在于，模型在采样过程中无法看到图像的完整上下文（只能看到其带噪版本），在我们早期的实验中，这偶尔会导致出现不理想的边缘伪影。</p>
<p>为了获得更好的效果，我们像Saharia等人（2021a）一样，专门对模型进行微调以执行图像修复任务。在微调过程中，我们会随机擦除训练样本中的区域，然后将剩余部分与一个掩码通道一起作为额外的条件信息输入到模型中。我们对模型架构进行了修改，增加了四个额外的输入通道：第二组RGB通道和一个掩码通道。在微调之前，我们将这些新通道对应的输入权重初始化为零。对于上采样模型，我们始终提供完整的低分辨率图像，但只提供高分辨率图像中未被掩码的区域。</p>
<h4 id="4-4-带噪CLIP模型"><a href="#4-4-带噪CLIP模型" class="headerlink" title="4.4 带噪CLIP模型"></a>4.4 带噪CLIP模型</h4><p>为了更好地与Dhariwal和Nichol（2021）提出的分类器引导技术相匹配，我们训练了带噪CLIP模型。该模型的图像编码器$f(x_t, t)$接收带噪图像$x_t$，并且在其他方面与原始CLIP模型使用相同的目标进行训练。我们在64×64分辨率下，按照与基础模型相同的噪声调度训练这些模型。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"><img src = "f5.png" width="80%" height="80%"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><em>图5. 基于MS-COCO提示的随机图像样本。对于XMC-GAN，我们采用Zhang等人（2021年）的样本。对于DALL-E，我们在温度0.85下生成样本，并使用CLIP重排序从256个样本中选择最佳样本。对于GLIDE，我们使用尺度为2.0的CLIP引导和尺度为3.0的无分类器引导。我们对GLIDE不进行任何CLIP重排序或挑选操作。</em></td>
</tr>
</tbody>
</table>
</div>
<h3 id="5-结果"><a href="#5-结果" class="headerlink" title="5. 结果"></a>5. 结果</h3><h4 id="5-1-定性结果"><a href="#5-1-定性结果" class="headerlink" title="5.1 定性结果"></a>5.1 定性结果</h4><p>在图5中对CLIP引导和无分类器引导生成的图像进行视觉比较时，我们发现，无分类器引导生成的样本往往比CLIP引导的样本看起来更逼真。本文其余的样本均使用无分类器引导生成，下一节将阐述我们这样选择的理由。</p>
<p>从图1中可以观察到，采用无分类器引导的GLIDE模型能够很好地处理各种各样的提示。该模型经常能生成逼真的阴影和反射效果，以及高质量的纹理。它还能够创作出各种风格的插画，比如模仿某位特定艺术家或画作的风格，或者像像素艺术这样的常见风格。最后，该模型能够将多个概念组合在一起（例如，一只柯基犬、领结和生日帽），同时为这些物体赋予相应的属性（例如，颜色）。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"><img src = "f6.png" width="100%" height="100%"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><em>图6. 在64×64分辨率的MS-COCO数据集上比较无分类器引导和CLIP引导在多样性与逼真度之间的权衡。</em></td>
</tr>
</tbody>
</table>
</div>
<p>在图像修复任务中，我们发现GLIDE模型能够根据文本提示对现有图像进行逼真的修改，必要时可以插入新的物体、阴影和反射（见图2）。该模型甚至能够在将物体编辑到画作中时，保持风格的一致性。我们还在图4中对SDEdit（Meng等人，2021）进行了实验，发现我们的模型能够将草图转化为逼真的图像编辑结果。在图3中，我们展示了如何通过零样本生成，再结合一系列图像修复编辑，使用GLIDE迭代生成一个复杂的场景。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"><img src = "f7.png" width="50%" height="50%"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><em>图7. 通过人类评估得出的Elo分数，用于确定无分类器引导和CLIP引导的最佳引导尺度。无分类器引导和CLIP引导的比较是分别进行的，但通过对无引导采样的Elo分数进行归一化处理，可将它们叠加在同一图表上。</em></td>
</tr>
</tbody>
</table>
</div>
<p>在图5中，我们将我们的模型与之前最先进的文本条件图像生成模型进行比较，这些模型基于MS-COCO的图像描述进行生成。结果发现，我们的模型在不进行CLIP重排序或挑选的情况下，能够生成更逼真的图像。</p>
<p>如需更多定性比较，请参阅附录C、D、E。</p>
<h4 id="5-2-定量结果"><a href="#5-2-定量结果" class="headerlink" title="5.2 定量结果"></a>5.2 定量结果</h4><p>我们首先通过观察质量 - 逼真度权衡的帕累托前沿，来评估无分类器引导和CLIP引导之间的差异。在图6中，我们在64×64分辨率下对零样本MS-COCO生成任务中的两种方法进行评估。我们考察了精度/召回率（Kynkäänniemi等人，2019）、FID（Heusel等人，2017）、Inception Score（Salimans等人，2016）和CLIP分数（Radford等人，2021）。随着两种引导尺度的增加，我们观察到在FID与IS、精度与召回率以及CLIP分数与FID之间存在明显的权衡关系。在前两条曲线中，我们发现无分类器引导几乎是帕累托最优的。而在绘制CLIP分数与FID的关系曲线时，我们看到了完全相反的趋势；特别是，CLIP引导似乎比无分类器引导更能提高CLIP分数。</p>
<p>我们假设CLIP引导在评估CLIP模型时找到了对抗样本，而不是在匹配提示方面真正优于无分类器引导。为了验证这一假设，我们聘请了人类评估人员来评判生成图像的样本质量。在这个评估设置中，向人类评估人员展示两张256×256的图像，他们必须选择哪一个样本：1）与给定的图像描述更匹配；2）看起来更逼真。如果评估人员认为两张图像没有明显差异，那么两个模型各得半分。</p>
<p>使用我们的人类评估协议，我们首先分别对两种方法的引导尺度进行扫描（图7），然后比较两种方法在最佳引导尺度下的结果（表1）。我们发现，人类评估结果与CLIP分数的评估结果不一致，他们认为无分类器引导生成的样本质量更高，与相应提示的匹配度也更高。</p>
<p>我们还将GLIDE与其他文本条件生成图像模型进行了比较。从表2中可以看出，我们的模型在从未在MS-COCO数据集上进行过明确训练的情况下，仍能在该数据集上获得有竞争力的FID分数。我们还按照Ramesh等人（2021）的方法，计算了在MS-COCO验证集的一个子集上的FID分数，该子集已去除了所有与我们训练集中图像相似的图像，这使得验证批次减少了21%。我们发现，在这种情况下，我们的FID分数从12.24略有上升至12.89，这在很大程度上可以用使用较小参考批次时FID偏差的变化来解释。最后，我们使用人类评估协议将GLIDE与DALL-E进行比较（表3）。值得注意的是，GLIDE的训练计算量与DALL-E大致相同，但模型规模要小得多（35亿参数对比120亿参数）。此外，GLIDE的采样延迟更低，并且无需进行CLIP重排序。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"><img src = "t1.png" ></th>
<th style="text-align:center"><img src = "t2.png" ></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><em>表1. 在256×256分辨率下，对MS-COCO验证集提示进行无引导扩散采样、无分类器引导和CLIP引导的人工评估所得的Elo分数。无分类器引导使用的引导尺度为3.0，CLIP引导使用的引导尺度为2.0。关于Elo分数的计算方法详见附录A.1。</em></td>
<td style="text-align:center"><em>表2. 在MS-COCO 256×256数据集上的FID对比。与之前的研究一样，我们为模型抽取3万个图像字幕，并与整个验证集进行对比。对于我们的模型，我们报告引导尺度为1.5的无分类器引导的FID数值，因为该设置下FID表现最佳。 </em></td>
</tr>
</tbody>
</table>
</div>
<p>我们对DALL-E和GLIDE进行了三组比较。第一组，比较两个模型在不进行CLIP重排序时的表现。第二组，仅对DALL-E使用CLIP重排序。第三组，对DALL-E使用CLIP重排序，同时将GLIDE的样本通过DALL-E使用的离散VAE进行处理。最后一组比较可以让我们评估DALL-E模糊的样本对人类判断的影响。我们在DALL-E模型的两个温度设置下进行了所有评估。在所有设置中，我们的模型都更受人类评估人员的青睐，即使在一些对DALL-E非常有利的配置中也是如此，这些配置允许DALL-E在测试时使用大量计算资源（通过CLIP重排序），同时降低了GLIDE样本的质量（通过VAE模糊处理）。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"><img src = "t3.png" width="80%" height="80%"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><em>表3. GLIDE与DALL-E的人类评估结果对比。我们报告了在逼真度和字幕相似度方面，我们模型的获胜概率。在最后一行，我们将DALL-E使用的离散变分自编码器（dVAE）应用于GLIDE的输出。</em></td>
</tr>
</tbody>
</table>
</div>
<p>如需查看DALL-E使用CLIP重排序以及GLIDE使用各种引导策略生成的样本网格，请参阅附录G。</p>
<h3 id="6-安全考量"><a href="#6-安全考量" class="headerlink" title="6. 安全考量"></a>6. 安全考量</h3><p>我们的模型能够生成以假乱真的图像，还能让没有专业技能的用户快速对现有图像进行令人信服的编辑。因此，如果在没有安全措施的情况下发布我们的模型，制作令人信服的虚假信息或深度伪造内容所需的技能门槛将大幅降低。此外，由于模型生成的样本反映了包括数据集中存在的各种偏见，使用该模型可能会在无意间延续有害的社会偏见。</p>
<p>为了减轻发布这些模型可能带来的有害影响，我们在训练用于发布的模型之前，对训练图像进行了筛选。首先，我们从互联网上收集了包含数亿张图像的数据集，这个数据集与训练CLIP和DALL-E所用的数据集基本不重叠，然后对这些数据应用了多项筛选。我们筛选掉了包含人物的训练图像，以降低模型在许多以人物为中心的不良应用场景中的能力。我们还担心模型被用于生成暴力图像和仇恨符号，所以也筛选掉了不少这类图像。关于我们的数据筛选过程的更多细节，请参阅附录F.1。</p>
<p>我们训练了一个参数为3亿的小型模型，称为GLIDE（filtered），使用的是经过筛选的数据集。随后，我们研究了如果开源GLIDE（filtered）的模型权重，它能在多大程度上降低被滥用的风险。在这项研究中，我们使用了一系列对抗性提示对模型进行测试，没有发现模型生成可识别的人物图像的情况，这表明我们的数据筛选器的漏报率足够低。我们还对GLIDE（filtered）的某些偏见进行了探测，发现它保留了数据集中的偏见，甚至可能会放大这些偏见。例如，当要求生成 “女孩的玩具” 时，我们的模型生成的粉色玩具和毛绒动物比 “男孩的玩具” 更多。另外，当要求生成通用的文化意象，如 “宗教场所” 时，我们的模型往往强化西方的刻板印象。我们还观察到，使用无分类器引导时，模型的偏见会被放大。最后，虽然我们限制了模型生成特定类别的图像的能力，但它仍然具备图像修复能力，这一能力被滥用的可能性是未来跨学科研究的一个重要方向。详细的示例和图像，请参阅附录F.2。</p>
<p>上述研究是单独针对GLIDE（filtered）进行的，但没有模型是孤立存在的。例如，通常可以将多个模型组合起来，获得新的能力。为了探究这个问题，我们将GLIDE（filtered）集成到一个公开的CLIP引导的扩散程序中（Crowson，2021a），研究这两个模型组合后的生成能力。我们普遍发现，虽然CLIP模型（在未筛选的数据上训练）会让我们的模型生成一些可识别的面部表情或仇恨意象，但当它与公开的ImageNet扩散模型结合使用时，生成图像的质量大致相同。更多细节，请参阅附录F.2。</p>
<p>为了推动CLIP引导扩散的进一步研究，我们还训练并发布了一个在筛选数据集上训练的带噪ViT-B CLIP模型。我们将用于训练GLIDE（filtered）的数据集与原始CLIP数据集的筛选版本结合起来。为了对这个模型进行测试，我们用它来引导GLIDE（filtered）和一个公开的64×64 ImageNet模型。在我们测试的提示中，新的CLIP模型生成的暴力图像或人物图像的质量，相比现有公开CLIP模型并没有显著提升。</p>
<p>我们还测试了GLIDE（filtered）直接重现训练图像的能力。在这个实验中，我们针对训练集中的3万个提示进行图像采样，计算每个生成图像与原始训练图像在CLIP潜在空间中的距离，然后检查距离最小的图像对。在我们检查的所有图像对中，模型都没有忠实地再现训练图像。</p>
<h3 id="7-局限性"><a href="#7-局限性" class="headerlink" title="7. 局限性"></a>7. 局限性</h3><p>虽然我们的模型通常能够以复杂的方式组合不同的概念，但它有时无法准确呈现某些描述高度不寻常的物体或场景的提示。在图8中，我们给出了一些失败案例。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"><img src = "f8.png"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><em>图8. GLIDE在处理某些不寻常物体或场景提示时的失败案例。</em></td>
</tr>
</tbody>
</table>
</div>
<p>我们未经优化的模型在单个A100 GPU上采样生成一张图像需要15秒。这比相关的GAN方法慢得多，GAN方法通过单次前向传递就能生成图像，因此更适合用于实时应用。 </p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://hqulzq.github.io/2025/03/08/Palette-Image-to-Image-Diffusion-Models%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://github.com/hqulzq/hqulzq.github.io/blob/main/images/userimg.jpg?raw=true">
      <meta itemprop="name" content="Zongqing Li">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lzq's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/03/08/Palette-Image-to-Image-Diffusion-Models%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/" class="post-title-link" itemprop="url">Palette Image-to-Image Diffusion Models论文精读</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-03-08 09:58:48 / 修改时间：11:08:13" itemprop="dateCreated datePublished" datetime="2025-03-08T09:58:48+08:00">2025-03-08</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="全文翻译"><a href="#全文翻译" class="headerlink" title="全文翻译"></a>全文翻译</h2><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>本文基于条件扩散模型开发了一个统一的图像到图像转换框架，并在四项具有挑战性的图像到图像转换任务上对该框架进行了评估，这些任务分别是彩色化、图像修复、图像扩展和JPEG图像恢复。我们对图像到图像扩散模型的简单实现，在所有任务上均优于强大的生成对抗网络（GAN）和回归基线方法，且无需针对特定任务进行超参数调整、架构定制，也无需使用任何辅助损失函数或复杂的新技术。我们揭示了去噪扩散目标中L2和L1损失对样本多样性的影响，并通过实证研究证明了自注意力机制在神经架构中的重要性。重要的是，我们倡导基于ImageNet建立统一的评估协议，采用人工评估和样本质量评分（如FID、Inception Score、预训练ResNet50的分类准确率，以及与原始图像的感知距离）。我们期望这个标准化的评估协议能够推动图像到图像转换研究的发展。最后，我们展示了一个通用的多任务扩散模型，其性能与特定任务的专业模型相当，甚至更优。有关结果和代码的概述，请查看<a target="_blank" rel="noopener" href="https://diffusionpalette.github.io/。">https://diffusionpalette.github.io/。</a><br>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2025/03/08/Palette-Image-to-Image-Diffusion-Models%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://hqulzq.github.io/2025/03/08/%E5%B8%B8%E7%94%A8%E7%A7%91%E7%A0%94%E7%BD%91%E7%AB%99%E5%90%88%E9%9B%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://github.com/hqulzq/hqulzq.github.io/blob/main/images/userimg.jpg?raw=true">
      <meta itemprop="name" content="Zongqing Li">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lzq's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/03/08/%E5%B8%B8%E7%94%A8%E7%A7%91%E7%A0%94%E7%BD%91%E7%AB%99%E5%90%88%E9%9B%86/" class="post-title-link" itemprop="url">常用科研网站合集</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-03-08 09:24:48 / 修改时间：09:45:41" itemprop="dateCreated datePublished" datetime="2025-03-08T09:24:48+08:00">2025-03-08</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Ps：本文转载于<a target="_blank" rel="noopener" href="https://hwcoder.top/Awesome-Sites">科研常用网站合集</a><br>记录科研常用网站，包括：论文检索、代码检索、学者信息、论文写作、科研论坛、截稿日期、电子书。本文持续更新。</p>
<h2 id="Paper"><a href="#Paper" class="headerlink" title="Paper"></a><a href="#paper"></a>Paper</h2><div class="table-container">
<table>
<thead>
<tr>
<th>Site</th>
<th>Describe</th>
<th>推荐指数</th>
</tr>
</thead>
<tbody>
<tr>
<td><a target="_blank" rel="noopener" href="http://www.arxivdaily.com/">ArXiv Daily</a></td>
<td><strong>每日爬取 ArXiv</strong> 各个领域论文，适合速刷</td>
<td>※※※※※</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://papers.labml.ai/papers/daily">Daliy Papers</a></td>
<td>近期<strong>热点论文</strong>追踪，每天必刷！</td>
<td>※※※※※</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/">Semantic Scholar</a></td>
<td>查看一篇论文的<strong>被引</strong>，按次数排序，<strong>更新速度很快</strong></td>
<td>※※※※</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://www.connectedpapers.com/">Connected Papers</a></td>
<td>用<strong>连通图</strong>展示同领域论文，大小论文都适用</td>
<td>※※※※</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://openreview.net/">OpenReview</a></td>
<td>检索<strong>最新在投论文</strong>，追踪顶会动向，可以看到<strong>审稿意见</strong></td>
<td>※※※※</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://www.scholar-inbox.com/">Shcolar Inbox</a></td>
<td>将最新论文按照<strong>研究兴趣匹配程度</strong>进行排序并推送</td>
<td>※※※※</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/">ArXiv</a></td>
<td>预印版论文下载，适合占坑</td>
<td>※※※</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://github.com/">GitHub</a></td>
<td>偶尔会有好心人放出领域论文集</td>
<td>※※※</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://www.paperdigest.org/">Paper Digest</a></td>
<td>快速搜索<strong>领域论文</strong>、最新会议论文索引 + <strong>highlight</strong></td>
<td>※※※</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/menu">CVF Open Access</a></td>
<td>CV 会议论文下载</td>
<td>※※</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://ai-paper-collector.vercel.app/">AI-Paper-Search</a></td>
<td>国人开发的插件，支持关键词匹配 AI 顶会</td>
<td>※</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://ac.scmor.com/">思谋学术导航</a></td>
<td>谷歌学术镜像与 Sci-Hub 导航</td>
<td>※</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://xueshu.dailyheadlines.cc/">深度学术搜索</a></td>
<td>谷歌学术镜像</td>
<td>※</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://readpaper.com/">ReadPaper</a></td>
<td>国内论文社区，可以看到别人对热点论文的<strong>笔记</strong></td>
<td>※</td>
</tr>
<tr>
<td>其他途径</td>
<td>公众号、组会分享、学术主页、顶会 Accept List</td>
<td>※※※※</td>
</tr>
</tbody>
</table>
</div>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2025/03/08/%E5%B8%B8%E7%94%A8%E7%A7%91%E7%A0%94%E7%BD%91%E7%AB%99%E5%90%88%E9%9B%86/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://hqulzq.github.io/2025/03/07/High-Resolution-Image-Synthesis-with-Latent-Diffusion-Models%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://github.com/hqulzq/hqulzq.github.io/blob/main/images/userimg.jpg?raw=true">
      <meta itemprop="name" content="Zongqing Li">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lzq's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/03/07/High-Resolution-Image-Synthesis-with-Latent-Diffusion-Models%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/" class="post-title-link" itemprop="url">High-Resolution Image Synthesis with Latent Diffusion Models论文精读</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-03-07 18:05:33" itemprop="dateCreated datePublished" datetime="2025-03-07T18:05:33+08:00">2025-03-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-03-08 10:03:12" itemprop="dateModified" datetime="2025-03-08T10:03:12+08:00">2025-03-08</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="全文翻译"><a href="#全文翻译" class="headerlink" title="全文翻译"></a>全文翻译</h2><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>通过将图像生成过程分解为去噪自编码器的顺序应用，扩散模型（DMs）在图像数据及其他领域取得了最先进的合成结果。此外，其公式允许在无需重新训练的情况下，通过一种引导机制来控制图像生成过程。然而，由于这些模型通常直接在像素空间中运行，训练强大的扩散模型往往需要消耗数百个GPU日的计算资源，并且由于顺序评估，推理成本也很高。为了在有限的计算资源上训练扩散模型，同时保持其质量和灵活性，我们将其应用于强大的预训练自编码器的潜在空间中。与以往的工作不同，在这种表示上训练扩散模型首次在降低复杂度和保留细节之间达到了接近最优的平衡，极大地提高了视觉保真度。通过在模型架构中引入交叉注意力层，我们将扩散模型转变为强大且灵活的生成器，适用于文本或边界框等一般条件输入，并且以卷积方式实现高分辨率合成也成为可能。我们的潜在扩散模型（LDMs）在图像修复和类别条件图像合成方面取得了新的最先进分数，在包括文本到图像合成、无条件图像生成和超分辨率在内的各种任务中表现出极具竞争力的性能，同时与基于像素的扩散模型相比，显著降低了计算需求。<br>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2025/03/07/High-Resolution-Image-Synthesis-with-Latent-Diffusion-Models%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://hqulzq.github.io/2025/03/02/NCSN%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://github.com/hqulzq/hqulzq.github.io/blob/main/images/userimg.jpg?raw=true">
      <meta itemprop="name" content="Zongqing Li">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lzq's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/03/02/NCSN%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/" class="post-title-link" itemprop="url">NCSN论文精读</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-03-02 22:25:20" itemprop="dateCreated datePublished" datetime="2025-03-02T22:25:20+08:00">2025-03-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-03-08 09:46:51" itemprop="dateModified" datetime="2025-03-08T09:46:51+08:00">2025-03-08</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><strong>Generative Modeling by Estimating Gradients of the Data Distribution</strong></p>
<h2 id="全文翻译"><a href="#全文翻译" class="headerlink" title="全文翻译"></a>全文翻译</h2><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>我们提出了一种新的生成模型，该模型通过朗之万动力学（Langevin dynamics）来生成样本，所使用的梯度是通过分数匹配（score matching）估计的数据分布梯度。由于当数据位于低维流形上时，梯度可能定义不明确且难以估计，因此我们用不同程度的高斯噪声对数据进行扰动，并联合估计相应的分数，即所有噪声水平下扰动数据分布的梯度向量场。在采样时，我们提出了一种退火朗之万动力学方法，在采样过程接近数据流形时，使用对应逐渐降低噪声水平的梯度。我们的框架允许灵活的模型架构，在训练过程中无需采样或使用对抗方法，并且提供了一个可用于原则性模型比较的学习目标。我们的模型在MNIST、CelebA和CIFAR - 10数据集上生成的样本可与生成对抗网络（GANs）相媲美，在CIFAR - 10数据集上达到了8.87的新最先进的初始得分（inception score）。此外，我们通过图像修复实验证明了我们的模型能够学习到有效的表示。<br>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2025/03/02/NCSN%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://hqulzq.github.io/2025/02/28/Classifier-Free-Diffusion-Guidance%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://github.com/hqulzq/hqulzq.github.io/blob/main/images/userimg.jpg?raw=true">
      <meta itemprop="name" content="Zongqing Li">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lzq's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/02/28/Classifier-Free-Diffusion-Guidance%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/" class="post-title-link" itemprop="url">Classifier-Free Diffusion Guidance论文精读</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-02-28 16:36:13" itemprop="dateCreated datePublished" datetime="2025-02-28T16:36:13+08:00">2025-02-28</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-03-08 09:57:33" itemprop="dateModified" datetime="2025-03-08T09:57:33+08:00">2025-03-08</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="全文翻译"><a href="#全文翻译" class="headerlink" title="全文翻译"></a>全文翻译</h2><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>分类器引导（Classifier guidance）是最近提出的一种方法，用于在条件扩散模型训练后，在模式覆盖和样本保真度之间进行权衡，这与其他类型生成模型中的低温采样或截断操作思路相似。分类器引导将扩散模型的得分估计与图像分类器的梯度相结合，因此需要训练一个与扩散模型分离的图像分类器。这也引发了一个问题：能否在不使用分类器的情况下进行引导。我们证明，确实可以通过一个纯生成模型在不使用分类器的情况下进行引导：在我们称为无分类器引导（classifier-free guidance）的方法中，我们联合训练一个条件扩散模型和一个无条件扩散模型，并结合得到的条件和无条件得分估计，在样本质量和多样性之间实现类似于使用分类器引导时的权衡。<br>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2025/02/28/Classifier-Free-Diffusion-Guidance%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://hqulzq.github.io/2025/02/27/PFDDIFF%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://github.com/hqulzq/hqulzq.github.io/blob/main/images/userimg.jpg?raw=true">
      <meta itemprop="name" content="Zongqing Li">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lzq's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/02/27/PFDDIFF%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/" class="post-title-link" itemprop="url">PFDDIFF论文精读</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-02-27 21:38:29" itemprop="dateCreated datePublished" datetime="2025-02-27T21:38:29+08:00">2025-02-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-03-07 19:26:56" itemprop="dateModified" datetime="2025-03-07T19:26:56+08:00">2025-03-07</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="全文翻译"><a href="#全文翻译" class="headerlink" title="全文翻译"></a>全文翻译</h2><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>扩散概率模型（Diffusion Probabilistic Models, DPMs）在图像生成领域展现出巨大潜力，但其采样效率受限于大量的去噪步骤。现有的解决方案大多通过提出快速常微分方程（ODE）求解器来加速采样过程。然而，当函数评估次数（NFE）较少时，ODE 求解器不可避免的离散化误差会被显著放大。在本研究中，我们提出了 PFDiff，这是一种全新的无需训练的正交跳步策略，它能使现有的快速 ODE 求解器在较少的 NFE 下运行。具体而言，PFDiff 首先利用过去时间步的分数替换来预测一个 “跳板” 状态。随后，它结合受 Nesterov 动量启发的前瞻更新机制，利用这个 “跳板” 状态快速更新当前的中间状态。这种方法在减少不必要的 NFE 的同时，还能校正一阶 ODE 求解器固有的离散化误差。实验结果表明，PFDiff 在各种预训练的 DPM 模型上都具有灵活的适用性，在条件 DPM 模型中表现尤为出色，超越了以往最先进的无需训练的方法。例如，以 DDIM 为基线，在 ImageNet 64x64 数据集上使用分类器引导时，我们的方法达到了 16.46 的 FID（4 次 NFE），而 DDIM 的 FID 为 138.81；在引导尺度为 7.5 的 Stable Diffusion 模型上，我们的方法以 10 次 NFE 达到了 13.06 的 FID。代码可在<a target="_blank" rel="noopener" href="https://github.com/onefly123/PFDiff获取。">https://github.com/onefly123/PFDiff获取。</a><br>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2025/02/27/PFDDIFF%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://hqulzq.github.io/2025/02/27/%E5%8A%A0%E9%80%9F%E9%87%87%E6%A0%B7%E7%BB%BC%E8%BF%B0-wgy/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://github.com/hqulzq/hqulzq.github.io/blob/main/images/userimg.jpg?raw=true">
      <meta itemprop="name" content="Zongqing Li">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lzq's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/02/27/%E5%8A%A0%E9%80%9F%E9%87%87%E6%A0%B7%E7%BB%BC%E8%BF%B0-wgy/" class="post-title-link" itemprop="url">加速采样综述-wgy</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-02-27 20:38:44" itemprop="dateCreated datePublished" datetime="2025-02-27T20:38:44+08:00">2025-02-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-03-07 19:27:08" itemprop="dateModified" datetime="2025-03-07T19:27:08+08:00">2025-03-07</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Accelerated-Sampling-for-Diffusion-Models-A-Survey"><a href="#Accelerated-Sampling-for-Diffusion-Models-A-Survey" class="headerlink" title="Accelerated Sampling for Diffusion Models: A Survey"></a>Accelerated Sampling for Diffusion Models: A Survey</h1><h2 id="全文翻译"><a href="#全文翻译" class="headerlink" title="全文翻译"></a>全文翻译</h2><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>自2020年以来，扩散概率模型（DPMs）因其简单直接的优化过程，在图像生成领域取得了革命性进展。这些模型已广泛应用于图像、文本、语音等多个领域的各种任务中。然而，与生成对抗网络（GANs）等一步生成方法相比，扩散模型的多步迭代采样过程显著降低了其采样效率。为解决扩散模型采样过程中迭代次数多、速度慢的问题，研究人员开展了大量工作，并针对这些模型的特点取得了许多突破。本文首先概述了扩散模型的基本原理和框架，随后将扩散模型的加速采样技术分为三类：采样器设计、知识蒸馏和其他加速方法。接着，我们详细讨论了每类加速方法的主要特点、适用场景，以及它们的优缺点。最后，本文总结了扩散模型加速采样的研究进展，并探讨了该领域面临的关键挑战和未来潜在的研究方向。<br>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2025/02/27/%E5%8A%A0%E9%80%9F%E9%87%87%E6%A0%B7%E7%BB%BC%E8%BF%B0-wgy/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://hqulzq.github.io/2025/02/26/ScoreSDE%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://github.com/hqulzq/hqulzq.github.io/blob/main/images/userimg.jpg?raw=true">
      <meta itemprop="name" content="Zongqing Li">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lzq's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/02/26/ScoreSDE%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/" class="post-title-link" itemprop="url">ScoreSDE论文精读</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-02-26 11:20:27" itemprop="dateCreated datePublished" datetime="2025-02-26T11:20:27+08:00">2025-02-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-03-08 09:56:17" itemprop="dateModified" datetime="2025-03-08T09:56:17+08:00">2025-03-08</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="SCORE-BASED-GENERATIVE-MODELING-THROUGH-STOCHASTIC-DIFFERENTIAL-EQUATIONS论文精读"><a href="#SCORE-BASED-GENERATIVE-MODELING-THROUGH-STOCHASTIC-DIFFERENTIAL-EQUATIONS论文精读" class="headerlink" title="SCORE-BASED GENERATIVE MODELING THROUGH STOCHASTIC DIFFERENTIAL EQUATIONS论文精读"></a>SCORE-BASED GENERATIVE MODELING THROUGH STOCHASTIC DIFFERENTIAL EQUATIONS论文精读</h1><h2 id="全文翻译"><a href="#全文翻译" class="headerlink" title="全文翻译"></a>全文翻译</h2><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>从数据中生成噪声很容易；而从噪声中生成数据则是生成式建模。我们提出一种随机微分方程（SDE），通过缓慢注入噪声，将复杂的数据分布平滑地转换为已知的先验分布；同时提出一种相应的逆向时间SDE，通过逐渐去除噪声，将先验分布转换回数据分布。关键在于，逆向时间SDE仅依赖于受扰动数据分布的时变梯度场（即分数）。利用基于分数的生成式建模的进展，我们可以用神经网络精确估计这些分数，并使用数值SDE求解器生成样本。我们表明，这个框架涵盖了先前基于分数的生成式建模和扩散概率建模方法，为新的采样过程和建模能力提供了可能。具体而言，我们引入了一个预测 - 校正框架，以纠正离散化逆向时间SDE演化过程中的误差。我们还推导了一个等效的神经常微分方程（ODE），它与SDE从相同分布中采样，但还能实现精确的似然计算，并提高采样效率。此外，我们提供了一种使用基于分数的模型解决逆问题的新方法，并通过类条件生成、图像修复和上色实验进行了验证。结合多种架构改进，我们在CIFAR - 10上的无条件图像生成任务中取得了突破性的成果，Inception分数达到9.89，FID为2.20，以2.99比特/维度的似然性创造了新纪录，并且首次从基于分数的生成模型中生成了高保真的1024×1024图像。<br>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2025/02/26/ScoreSDE%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://hqulzq.github.io/2025/02/21/Diffusion-Models-Beat-GANs-on-Image-Synthesis%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://github.com/hqulzq/hqulzq.github.io/blob/main/images/userimg.jpg?raw=true">
      <meta itemprop="name" content="Zongqing Li">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lzq's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/02/21/Diffusion-Models-Beat-GANs-on-Image-Synthesis%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/" class="post-title-link" itemprop="url">Diffusion Models Beat GANs on Image Synthesis论文精读</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-02-21 20:46:42" itemprop="dateCreated datePublished" datetime="2025-02-21T20:46:42+08:00">2025-02-21</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-03-08 10:02:36" itemprop="dateModified" datetime="2025-03-08T10:02:36+08:00">2025-03-08</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="全文翻译"><a href="#全文翻译" class="headerlink" title="全文翻译"></a>全文翻译</h2><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>我们表明，扩散模型可以实现优于当前最先进的生成模型的图像样本质量。我们通过一系列消融找到更好的架构，在无条件图像合成中实现这一点。对于条件图像合成，我们通过分类器指导进一步提高样本质量：一种简单、计算高效的方法，使用分类器的梯度来权衡多样性以获得保真度。我们在 ImageNet 128×128 上实现了 2.97 的 FID，在 ImageNet 256×256 上实现了 4.59，在 ImageNet 512×512 上实现了 7.72，即使每个样本只有 25 次前向传递，我们也可以匹配 BigGAN-deep，同时保持更好的分布覆盖率。最后，我们发现分类器指导与上采样扩散模型很好地结合，进一步将 ImageNet 256×256 的 FID 提高到 3.94，在 ImageNet 512×512 上提高到 3.85。我们在 <a target="_blank" rel="noopener" href="https://github.com/openai/guided-diffusion">https://github.com/openai/guided-diffusion</a> 发布我们的代码。<br>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2025/02/21/Diffusion-Models-Beat-GANs-on-Image-Synthesis%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Zongqing Li"
      src="https://github.com/hqulzq/hqulzq.github.io/blob/main/images/userimg.jpg?raw=true">
  <p class="site-author-name" itemprop="name">Zongqing Li</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">45</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">12</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">40</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/hqulzq" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;hqulzq" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:hqulzq@163.com" title="E-Mail → mailto:hqulzq@163.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

      
<script type="text/javascript" charset="utf-8" src="/js/tagcloud.js"></script>
<script type="text/javascript" charset="utf-8" src="/js/tagcanvas.js"></script>
<div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div id="myCanvasContainer" class="widget tagcloud">
        <canvas width="250" height="250" id="resCanvas" style="width=100%">
            <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/2019/" rel="tag">2019</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/2020/" rel="tag">2020</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/2021/" rel="tag">2021</a><span class="tag-list-count">6</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/2022/" rel="tag">2022</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Bug%E6%80%BB%E7%BB%93/" rel="tag">Bug总结</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CVPR/" rel="tag">CVPR</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CVPR-Workshop/" rel="tag">CVPR Workshop</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Git/" rel="tag">Git</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ICLR/" rel="tag">ICLR</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Kafka/" rel="tag">Kafka</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Knife4j/" rel="tag">Knife4j</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linux/" rel="tag">Linux</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/MinIO/" rel="tag">MinIO</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Mybatis-Plus/" rel="tag">Mybatis-Plus</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NeurIPS/" rel="tag">NeurIPS</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Nginx/" rel="tag">Nginx</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RabbitMQ/" rel="tag">RabbitMQ</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Redis/" rel="tag">Redis</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RocketMQ/" rel="tag">RocketMQ</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/diffusion/" rel="tag">diffusion</a><span class="tag-list-count">13</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/docker/" rel="tag">docker</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/k8s/" rel="tag">k8s</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/latex/" rel="tag">latex</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/leetcode/" rel="tag">leetcode</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mongodb/" rel="tag">mongodb</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mysql/" rel="tag">mysql</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E4%BB%A3%E7%A0%81%E9%9A%8F%E6%83%B3%E5%BD%95/" rel="tag">代码随想录</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%A4%B4%E5%87%BD%E6%95%B0/" rel="tag">头函数</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%B0%9A%E5%BA%AD%E5%85%AC%E5%AF%93/" rel="tag">尚庭公寓</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86/" rel="tag">异常处理</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/" rel="tag">快速入门</a><span class="tag-list-count">13</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93/" rel="tag">技术总结</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E7%BB%84/" rel="tag">数组</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B3%A8%E8%A7%A3/" rel="tag">注解</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%BD%91%E7%AB%99/" rel="tag">网站</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%8B%A5%E4%BE%9D/" rel="tag">若依</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%AE%BA%E6%96%87/" rel="tag">论文</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/" rel="tag">设计模式</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%93%BE%E8%A1%A8i/" rel="tag">链表i</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98/" rel="tag">项目实战</a><span class="tag-list-count">6</span></li></ul>
        </canvas>
    </div>
</div>



    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zongqing Li</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>









<script>
document.querySelectorAll('.pdfobject-container').forEach(element => {
  let url = element.dataset.target;
  let pdfOpenParams = {
    navpanes : 0,
    toolbar  : 0,
    statusbar: 0,
    pagemode : 'thumbs',
    view     : 'FitH'
  };
  let pdfOpenFragment = '#' + Object.entries(pdfOpenParams).map(([key, value]) => `${key}=${encodeURIComponent(value)}`).join('&');
  let fullURL = `/lib/pdf/web/viewer.html?file=${encodeURIComponent(url)}${pdfOpenFragment}`;

  if (NexT.utils.supportsPDFs()) {
    element.innerHTML = `<embed class="pdfobject" src="${url + pdfOpenFragment}" type="application/pdf" style="height: ${element.dataset.height};">`;
  } else {
    element.innerHTML = `<iframe src="${fullURL}" style="height: ${element.dataset.height};" frameborder="0"></iframe>`;
  }
});
</script>




  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

  
  <style>
    .copy-btn {
      display: inline-block;
      padding: 6px 12px;
      font-size: 13px;
      font-weight: 700;
      line-height: 20px;
      color: #4D4D4C;
      white-space: nowrap;
      vertical-align: middle;
      cursor: pointer;
      background-color: #F7F7F7;
      background-image: linear-gradient(#F7F7F7, #F7F7F7);
      border: 1px solid #d5d5d5;
      border-radius: 3px;
      user-select: none;
      outline: 0;
    }

    .highlight-wrap .copy-btn {
      transition: opacity .3s ease-in-out;
      opacity: 0;
      padding: 2px 6px;
      position: absolute;
      right: 4px;
      top: 8px;
    }

    .highlight-wrap:hover .copy-btn,
    .highlight-wrap .copy-btn:focus {
      opacity: 1
    }

    .highlight-wrap {
      position: relative;
    }
  </style>

  <script>
    $('.highlight').each(function (i, e) {
      var $wrap = $('<div>').addClass('highlight-wrap')
      $(e).after($wrap)
      $wrap.append($('<button>').addClass('copy-btn').append('post.copy_button').on('click', function (e) {
        var code = $(this).parent().find('.code').find('.line').map(function (i, e) {
          return $(e).text()
        }).toArray().join('\n')
        var ta = document.createElement('textarea')
        document.body.appendChild(ta)
        ta.style.position = 'absolute'
        ta.style.top = '0px'
        ta.style.left = '0px'
        ta.value = code
        ta.select()
        ta.focus()
        var result = document.execCommand('copy')
        document.body.removeChild(ta)
        
          if(result)$(this).text('post.copy_success')
          else $(this).text('post.copy_failure')
        
        $(this).blur()
      })).on('mouseleave', function (e) {
        var $b = $(this).find('.copy-btn')
        setTimeout(function () {
          $b.text('post.copy_button')
        }, 300)
      }).append(e)
    })
  </script>


<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/assets/koharu.model.json"},"display":{"position":"right","width":150,"height":300,"hOffset":-15,"vOffset":-15},"mobile":{"show":true},"react":{"opacity":0.7},"log":false});</script></body>
</html>
