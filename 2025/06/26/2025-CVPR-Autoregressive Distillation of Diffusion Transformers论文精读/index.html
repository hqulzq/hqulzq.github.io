<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"hqulzq.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="全文翻译摘要具有Transformer架构的扩散模型在生成高保真图像和实现高分辨率可扩展性方面展现出了良好的能力。然而，图像合成所需的迭代采样过程非常耗费资源。有一系列研究致力于将概率流常微分方程（ODE）的求解方案蒸馏到少步学生模型中。尽管如此，现有方法受限于依赖最新的去噪样本作为输入，这使得它们容易受到暴露偏差的影响。为了解决这一局限性，我们提出了自回归蒸馏（ARD）方法，这是一种利用ODE历">
<meta property="og:type" content="article">
<meta property="og:title" content="2025-CVPR-Autoregressive Distillation of Diffusion Transformers论文精读">
<meta property="og:url" content="https://hqulzq.github.io/2025/06/26/2025-CVPR-Autoregressive%20Distillation%20of%20Diffusion%20Transformers%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/index.html">
<meta property="og:site_name" content="Lzq&#39;s blog">
<meta property="og:description" content="全文翻译摘要具有Transformer架构的扩散模型在生成高保真图像和实现高分辨率可扩展性方面展现出了良好的能力。然而，图像合成所需的迭代采样过程非常耗费资源。有一系列研究致力于将概率流常微分方程（ODE）的求解方案蒸馏到少步学生模型中。尽管如此，现有方法受限于依赖最新的去噪样本作为输入，这使得它们容易受到暴露偏差的影响。为了解决这一局限性，我们提出了自回归蒸馏（ARD）方法，这是一种利用ODE历">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2025-06-26T14:13:24.000Z">
<meta property="article:modified_time" content="2025-07-08T08:39:48.708Z">
<meta property="article:author" content="Zongqing Li">
<meta property="article:tag" content="diffusion">
<meta property="article:tag" content="CVPR">
<meta property="article:tag" content="2025">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://hqulzq.github.io/2025/06/26/2025-CVPR-Autoregressive%20Distillation%20of%20Diffusion%20Transformers%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>2025-CVPR-Autoregressive Distillation of Diffusion Transformers论文精读 | Lzq's blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Lzq's blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">49</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">12</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">103</span></a>

  </li>
        <li class="menu-item menu-item-book">

    <a href="/book" rel="section"><i class="fas fa-book fa-fw"></i>Book</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://hqulzq.github.io/2025/06/26/2025-CVPR-Autoregressive%20Distillation%20of%20Diffusion%20Transformers%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://github.com/hqulzq/hqulzq.github.io/blob/main/images/userimg.jpg?raw=true">
      <meta itemprop="name" content="Zongqing Li">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lzq's blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          2025-CVPR-Autoregressive Distillation of Diffusion Transformers论文精读
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-06-26 22:13:24" itemprop="dateCreated datePublished" datetime="2025-06-26T22:13:24+08:00">2025-06-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-07-08 16:39:48" itemprop="dateModified" datetime="2025-07-08T16:39:48+08:00">2025-07-08</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="全文翻译"><a href="#全文翻译" class="headerlink" title="全文翻译"></a>全文翻译</h2><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>具有Transformer架构的扩散模型在生成高保真图像和实现高分辨率可扩展性方面展现出了良好的能力。然而，图像合成所需的迭代采样过程非常耗费资源。有一系列研究致力于将概率流常微分方程（ODE）的求解方案蒸馏到少步学生模型中。尽管如此，<strong>现有方法受限于依赖最新的去噪样本作为输入，这使得它们容易受到暴露偏差的影响。为了解决这一局限性，我们提出了自回归蒸馏（ARD）方法，这是一种利用ODE历史轨迹来预测未来步骤的新方法。ARD具有两个关键优势：1. 它通过利用对累积误差不太敏感的预测历史轨迹来减轻暴露偏差；2. 它将ODE轨迹的先前历史作为更有效的粗粒度信息源加以利用。ARD通过添加标记轨迹历史中每个输入的逐令牌时间嵌入来修改教师Transformer架构，并采用分块因果注意力掩码进行训练。此外，仅在较低的Transformer层中融入历史输入可提升性能和效率。</strong>我们在ImageNet的类条件生成和文本到图像（T2I）合成任务中验证了ARD的有效性。在ImageNet-256上，我们的模型与基线方法相比，FID（Fréchet Inception Distance）退化降低了5倍，而仅需增加1.1%的FLOPs（浮点运算次数）。此外，ARD在仅4步内就使ImageNet-256的FID达到1.84，并且在提示遵循度评分上优于公开的1024像素文本到图像蒸馏模型，同时与教师模型相比FID仅有微小下降。项目页面：<a target="_blank" rel="noopener" href="https://github.com/alsdudrla10/ARD">https://github.com/alsdudrla10/ARD</a>。</p>
<span id="more"></span>
<h3 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h3><p>扩散模型目前在图像合成领域占据主导地位，因其出色的泛化能力和前所未有的视觉质量[8, 12, 56, 61]。与生成对抗网络（GAN）[16]不同，扩散模型的稳定训练使其能够扩展到高分辨率图像生成任务。最近，基于扩散Transformer（DiT）[54]架构的模型因其优异的缩放特性和生成高分辨率图像的能力而广受欢迎[5, 6]。然而，从扩散模型中采样需要重复进行神经网络评估[44]，这使得高分辨率图像合成过程缓慢且资源密集。</p>
<p>扩散模型通过数值求解去噪过程来生成样本。去噪过程可用概率流常微分方程（ODE）描述[69, 71]，该方程在噪声和样本之间建立了确定性联系。<strong>为降低采样成本，一系列蒸馏模型[17, 27, 42, 45, 62, 72, 86]被提出，旨在学习用更少的步骤预测ODE的解。然而，少步学生模型会遭受暴露偏差[53, 57]的影响，因为学生模型的中间预测常因估计误差偏离教师模型的ODE轨迹。这些误差在迭代采样过程中不断累积，导致预测在接近解时变得更加不准确。</strong></p>
<p>为解决少步蒸馏模型中的暴露偏差问题，我们提出了一种适用于扩散Transformer的自回归蒸馏（ARD）方法<strong>。ARD基于当前估计值$X_{T_{x}}$和整个历史轨迹来预测下一个样本$X_{\tau_{s-1}}$，这种方式包含了更丰富的信息。该方法具有两个优势：减少累积误差，并从历史轨迹中获取更优质的粗粒度信息源。</strong>在底层网络层中融入历史轨迹，进一步引入了处理粗粒度信息的归纳偏置。我们发现，在ImageNet 256p数据集上，基于完整历史轨迹进行蒸馏时，与基线方法相比，模型的FID（弗雷歇初始距离）退化降低了五倍，而计算量仅增加1.1%。此外，我们的方法具有良好的扩展性，可用于蒸馏1024p的文本到图像扩散Transformer，其在文本-图像对齐指标上优于公开的蒸馏方法。</p>
<h3 id="2-预备知识"><a href="#2-预备知识" class="headerlink" title="2. 预备知识"></a>2. 预备知识</h3><h4 id="2-1-扩散模型"><a href="#2-1-扩散模型" class="headerlink" title="2.1 扩散模型"></a>2.1 扩散模型</h4><p>扩散模型通过随机微分方程（SDE）定义前向过程和相应的反向过程。公式（1）中的前向过程将数据$x_{0} \sim p_{\text {data }}\left(x_{0}\right)$映射为噪声$x_{T}$：</p>
<script type="math/tex; mode=display">d x_{t}=f\left(x_{t}, t\right) d t+g(t) d w_{t} \tag{1}</script><p>其中$f: \mathbb{R}^{d} \times[0, T] \rightarrow \mathbb{R}$是漂移项，$g:[0, T] \rightarrow \mathbb{R}$是扩散项，$w_{t}$是维纳过程。前向过程通常设置为方差保持[22]或方差爆炸[71]的SDE，以使$t=T$时的分布接近高斯分布。扩散模型通过反向过程从噪声$x_{T} \sim p_{\text {prior }}\left(x_{T}\right)$生成数据[1, 71]。反向过程存在一个概率流ODE（PF-ODE），它是反向SDE的确定性对应形式：</p>
<script type="math/tex; mode=display">d x_{t}=\left[f\left(x_{t}, t\right)-\frac{1}{2} g(t)^{2} \nabla_{x_{t}} \log p_{t}\left(x_{t}\right)\right] d t \tag{2}</script><p>这里$p_{t}\left(x_{t}\right)$是由公式（1）中的前向过程定义的边缘分布。PF-ODE与反向SDE具有相同的边缘分布，同时在噪声$x_{T}$和样本$x_{0}$之间提供了确定性的联系。由于得分函数$\nabla_{x_{t}} \log p_{t}\left(x_{t}\right)$难以直接求解，因此通过神经网络$\nabla_{x_{t}} \log p_{t}^{\phi}\left(x_{t}\right)$并利用得分匹配目标[70, 79]进行估计。</p>
<h4 id="2-2-步长蒸馏模型"><a href="#2-2-步长蒸馏模型" class="headerlink" title="2.2 步长蒸馏模型"></a>2.2 步长蒸馏模型</h4><p>公式（2）中ODE的解由$x_{T}+\int_{T}^{0} \frac{d x_{t}}{d t} d t$得到；然而，这需要足够多的步骤来减少离散化误差[9, 44]。为了计算每一步的$\frac{d x_{t}}{d t}$，我们需要评估学习到的神经得分函数$\nabla_{x_{t}} \log p_{t}^{\phi^{\ast}}\left(x_{t}\right)$，这会导致高昂的计算成本。为了提高推理效率，步长蒸馏[51, 62]定义了中间时间$\tau_{s}:=T \times \frac{s}{S}$（其中$S$是学生模型的总步数，$s \in\{0,1, \ldots, S\}$）。这些中间时间在教师ODE中定义了一个轨迹$\mu_{\phi^{\ast}}:=\left[x_{\tau_{S}}, x_{\tau_{S-1}}, \ldots, x_{\tau_{1}}, x_{\tau_{0}}\right]$，从初始噪声$x_{\tau_{S}}=x_{T}$开始，以干净样本$x_{\tau_{0}}=x_{0}$结束。学生模型学习一个联合概率$p\left(\mu_{\phi^{\ast}}\right)$，定义为：</p>
<script type="math/tex; mode=display">p\left(\mu_{\phi^{\ast}}\right)=p_{\text {prior }}\left(x_{\tau_{S}}\right) \times \prod_{s=1}^{S} p\left(x_{\tau_{s-1}} \mid x_{\tau_{s}}\right) \tag{3}</script><p>由于PF-ODE的确定性，每个条件概率$p\left(x_{\tau_{s-1}} \mid x_{\tau_{s}}\right)$是狄拉克δ分布，因此可以用确定性映射函数建模：$x_{\tau_{s-1}}=G\left(x_{\tau_{s}}, s\right):=x_{\tau_{s}}+\int_{\tau_{s}}^{\tau_{s-1}} \frac{d x_{t}}{d t} d t$。学生模型$G_{\theta}\left(x_{\tau_{s}}, s\right) \approx G\left(x_{\tau_{s}}, s\right)$学习模仿真实的ODE积分过程。渐进蒸馏[51, 62]提出了一种用于步长蒸馏的渐进算法，<strong>但该算法存在一个显著缺点：当学生模型再次成为教师模型时，其迭代训练阶段会累积误差。使用$L_{\text {step }}$直接从教师模型训练少步学生模型，可以减轻迭代渐进蒸馏过程带来的累积误差。</strong>我们的方法建立在步长蒸馏的基础上，直接从教师模型学习：</p>
<script type="math/tex; mode=display">\mathcal{L}_{\text {step }}:=\mathbb{E}_{\mu_{\phi^{*}}}\left[\sum_{s=1}^{S}\left\|G_{\theta}\left(x_{\tau_{s}}, s\right)-x_{\tau_{s-1}}\right\|_{2}^{2}\right] \tag{4}</script><p><strong>暴露偏差</strong>在推理过程中，生成从$x_{\tau_{S}} \sim p_{\text {prior }}\left(x_{\tau_{S}}\right)$开始。每一步，学生模型仅基于当前样本$\hat{x}_{\tau_{s}}$预测$\hat{x}_{\tau_{s-1}}=G_{\theta}\left(\hat{x}_{\tau_{s}}, s\right)$。如果$\hat{x}_{\tau_{s}}$偏离教师ODE，学生模型$G_{\theta}$将基于训练中未见过的样本进行推断。例如，图2a中展示的中间样本，一条鱼没有眼睛，而这类样本在训练数据中并未出现。这种未预见的输入会在采样过程中传播，最终导致最终样本$x_{\tau_{0}}$也缺少眼睛。除非实现完美优化，否则这种暴露偏差是迭代过程的固有局限[53, 57]。随着迭代采样过程的进行，误差会不断累积。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"><img src = "f2.png" width = "100%"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><em>图2.（a、b）基线蒸馏方法和所提出蒸馏方法的整体方案。训练轨迹由教师ODE给出。（c、d）蒸馏方法与公开生成模型在ImageNet 256p上的效率-性能权衡比较。</em></td>
</tr>
</tbody>
</table>
</div>
<h4 id="2-3-自回归模型"><a href="#2-3-自回归模型" class="headerlink" title="2.3 自回归模型"></a>2.3 自回归模型</h4><p><strong>自回归模型[32]通过将多元随机变量$x:=\left[x_{S}, x_{S-1}, \ldots, x_{0}\right]$的联合概率分布分解为条件概率的乘积来表示：$p(x)=p\left(x_{S}\right) \times \prod_{s=1}^{S} p\left(x_{s-1} \mid x_{S: s}\right)$，其中$x_{S: s}=\left[x_{S}, x_{S-1}, \ldots, x_{s}\right]$。如上文所述，这种公式不依赖于任何特定假设。分解中的每个分量$p\left(x_{s-1} \mid x_{S: s}\right)$都包含了所有先前变量的信息。</strong></p>
<h3 id="3-方法"><a href="#3-方法" class="headerlink" title="3. 方法"></a>3. 方法</h3><p>在本节中，我们将介绍扩散Transformer（DiT）的自回归蒸馏（ARD）方法。图2b概述了ARD的流程。我们将在3.1节中剖析蒸馏的概率公式，接着在3.2节探讨学生模型的Transformer架构设计，最后在3.3节阐述训练和推理过程。</p>
<h4 id="3-1-自回归蒸馏"><a href="#3-1-自回归蒸馏" class="headerlink" title="3.1 自回归蒸馏"></a>3.1 自回归蒸馏</h4><p>本节将公式（3）中的步长蒸馏公式推广至ARD。在理想蒸馏情况下，若没有完整历史轨迹信息，公式（3）的分解依然成立。然而，当每个概率$p(x_{\tau_{s-1}}|x_{\tau_s})$由$\hat{x}_{\tau_{s-1}}=G_{\theta}(x_{\tau_s},s)$近似时，由于估计误差，与真实值的偏差不可避免，从而导致2.2节中讨论的暴露偏差问题。</p>
<p>为缓解这一问题，我们受2.3节启发，以自回归方式扩展公式（3）的表达：</p>
<script type="math/tex; mode=display">p(\mu_{\phi^{\ast}})=p_{\text{prior}}(x_{\tau_S}) \times \prod_{s=1}^{S} p(x_{\tau_{s-1}}|x_{\tau_S:\tau_s}) \tag{5}</script><p>其中$x_{\tau_S:\tau_s}=[x_{\tau_S},x_{\tau_{S-1}},\dots,x_{\tau_s}]$表示历史轨迹。该公式有两个优势：（i）每一步都将真实初始噪声$x_{\tau_S}$作为输入，其与预测目标$x_{\tau_{s-1}}$存在确定性关联。此外，从$\hat{x}_{\tau_{S-1}}$到$\hat{x}_{\tau_{s+1}}$的历史轨迹预测，相比近期样本$\hat{x}_{\tau_s}$更为准确，因为在推理过程中其误差累积更少。相比之下，公式（3）的输入仅为当前样本$\hat{x}_{\tau_s}$，易受暴露偏差影响。（ii）为预测每一步的$x_{\tau_{s-1}}$，模型需同时生成粗粒度和细粒度信息。近期去噪样本$x_{\tau_s}$是细粒度信息的最佳来源，而靠近$x_{\tau_S}$的历史轨迹则是粗粒度信息的更好来源[13,60]。</p>
<p><strong>对于修改后的学生模型公式，我们旨在估计$p(x_{\tau_{s-1}}|x_{\tau_S:\tau_s})$，其仍为狄拉克δ分布。为实现这一点，我们定义新的映射函数$x_{\tau_{s-1}}=G(x_{\tau_S:\tau_s},s):=x_{\tau_s}+\int_{\tau_s}^{\tau_{s-1}}\frac{dx_t}{dt}dt$，然后用学生神经网络$G_{\theta}(x_{\tau_S:\tau_s},s)$对该函数进行近似。</strong></p>
<h4 id="3-2-Transformer设计"><a href="#3-2-Transformer设计" class="headerlink" title="3.2 Transformer设计"></a>3.2 Transformer设计</h4><p>3.1节中定义的映射函数$G_{\theta}(x_{\tau_S:\tau_s},s)$的设计并非易事，因为输入大小会随去噪步骤$s$而变化。为解决这一问题，我们对教师DiT主干进行修改，以适应多输入场景。</p>
<p><strong>架构</strong> 为处理历史轨迹，我们设计了如图3a所示的基于Transformer的自回归模型。每个输入$x_{\tau_s}$通过共享的补丁嵌入器被标记为一系列标记。由于每个输入$x_{\tau_s}$与二维网格具有相同的空间结构，位置嵌入在输入间共享。Transformer块需要识别输入序列$x_{\tau_S},\dots,x_{\tau_s}$中每个标记的顺序。为此，我们向每个标记添加额外的时间步嵌入，类似于VAR中的层级嵌入[76]。近期去噪样本$x_{\tau_s}$成为查询标记，历史序列$x_{\tau_S:\tau_s}$则成为自注意力块中的键值标记。经过L个堆叠的Transformer块后，标记经线性变换和去标记化处理，得到样本$x_{\tau_{s-1}}$。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"><img src = "f3.png" width = "90%"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><em>图3.（a）所提出的ARDTransformer架构。DiT参数在每个输入xτs之间共享。（b）训练期间使用的通用掩码选项可视化：M1代表分步蒸馏，而M4是ARD的默认设置。M2和M3是M1和M4之间的中间选项。</em></td>
</tr>
</tbody>
</table>
</div>
<p><strong>仅在底层N层使用历史轨迹</strong> 图4b、4d和4f展示了在每个L层Transformer块中，第2、3、4步时各输入的注意力分数。近期去噪样本$x_{\tau_s}$在高层中作为键标记的激活程度最高，而历史轨迹$x_{\tau_S:\tau_{s+1}}$在底层中被激活。<strong>已知DiT块的底层用于处理粗粒度信息，高层用于处理细粒度信息[19]。这一注意力分配验证了历史轨迹的有效性，其作为粗粒度信息的更好来源。然而，在图4b、4d和4f中，历史标记在高层仍有轻微波动，可能是优化不完美所致。我们在Transformer层中提出如图4a所示的额外设计选择：仅在底层N层使用历史轨迹。这种归纳偏置增强了底层对历史轨迹的利用，如图4c、4e和4g所示。</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"><img src = "f4.png" width = "90%"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><em>图4.（a）展示了我们通过仅在较低层使用历史轨迹而施加的额外归纳偏置。（b、d、f）展示了当N=L时，在第2、3、4步中每个历史输入（关键令牌）的注意力分数。（c、e、g）展示了N=6时的相同情况。输入xτs’上的注意力分数是xτs’中所有关键令牌的注意力权重之和，表示Xτs’的占比。</em></td>
</tr>
</tbody>
</table>
</div>
<h4 id="3-3-训练与推理流程"><a href="#3-3-训练与推理流程" class="headerlink" title="3.3 训练与推理流程"></a>3.3 训练与推理流程</h4><p>ARD的默认训练目标是公式（6）中的回归损失$\mathcal{L}*{ARD}$，并针对θ进行优化。图3a中的Transformer架构允许通过使用注意力掩码，同时计算所有$s\in\{1,\dots,S\}$的$\hat{x}_{\tau_{s-1}}=G_{\theta}(x_{\tau_S:\tau_s},s)$。我们可通过设计如图3b所示的注意力掩码来扩展框架。选项M4中的块级因果注意力最为灵活，它利用了整个轨迹历史。选项M1代表步长蒸馏，仅使用当前样本$x_{\tau_s}$作为输入。选项M2和M3是M1和M4之间的中间选择。M2中的窗口注意力仅使用轨迹历史中的当前样本和前一个样本。M3中的注意力掩码使用最近的去噪样本和初始噪声$x_{\tau_S}$，这有助于持续保留真实信号。我们的框架还可从最终预测$\hat{x}_{\tau_0}=G_{\theta}(x_{\tau_S:\tau_1},1)$的额外判别器损失中获益，类似于[27]。通过在该损失中使用真实数据，我们可进一步改善学生生成图像的高频细节，甚至超越教师模型。</p>
<script type="math/tex; mode=display">\mathcal{L}_{ARD}:=\mathbb{E}_{\mu_{\phi^{\ast}}}\left[\sum_{s=1}^{S}\left\|G_{\theta}(x_{\tau_S:\tau_s},s)-x_{\tau_{s-1}}\right\|_2^2\right] \tag{6}</script><p>在推理过程中，生成从$x_{\tau_S}\sim p_{\text{prior}}(x_{\tau_S})$开始。每一步，学生模型基于整个历史预测$\hat{x}_{\tau_S:\tau_s}=[x_{\tau_S},\hat{x}_{\tau_{S-1}},\dots,\hat{x}_{\tau_s}]$预测$\hat{x}_{\tau_{s-1}}=G_{\theta}(\hat{x}_{\tau_S:\tau_s},s)$。$[x_{\tau_S},\hat{x}_{\tau_{S-1}},\dots,\hat{x}_{\tau_{s+1}}]$的信息作为kv缓存存储在先前步骤中，以加速推理。推理过程中无需注意力掩码。</p>
<h3 id="4-实验"><a href="#4-实验" class="headerlink" title="4. 实验"></a>4. 实验</h3><p>本节从实证角度验证ARD的有效性。4.1节阐述ImageNet上类别条件图像合成的结果[10]，4.2节呈现文本条件图像合成的实验结果。</p>
<h4 id="4-1-类别条件图像生成"><a href="#4-1-类别条件图像生成" class="headerlink" title="4.1 类别条件图像生成"></a>4.1 类别条件图像生成</h4><p>我们采用DiT/XL-2潜在扩散Transformer架构[54]，并将其作为教师架构。该教师模型（$(\phi)$）在ImageNet 256p上进行训练。通过让教师模型以25步和1.5的无分类器引导尺度[21]运行，构建ODE轨迹$\mu_{\phi}$。总共预计算并存储256万条ODE轨迹用于蒸馏。</p>
<p><strong>评估指标</strong> 为评估样本保真度和多样性，我们遵循ADM[12]的协议，使用预训练的Inception-V3网络[75]测量FID[20]、IS[63]、精确率（Precision）和召回率（Recall）[31]。FID和IS同时量化样本质量和多样性；精确率衡量样本保真度，召回率通过量化特征空间中真实样本与生成样本的流形重叠区域来衡量样本多样性。</p>
<p><strong>ARD带来的性能提升</strong> 所提出的ARD是对先前方法[45,62]的泛化：当步数S为1时，ARD变为知识蒸馏（KD）[45]；当ARD使用图3b中的注意力掩码选项M1时，其变为步长蒸馏[62]。表1和图2c展示了ARD扩展设计带来的性能提升。对于我们的方法，将步数从2增加到4可使FID性能提升（6.29→4.32）。然而，即使步长蒸馏的步数S从2增加到4，其FID提升也微乎其微（10.92→10.25），且召回率显著下降。在图5a和5c中可以看到，步长蒸馏无法保留全局结构（如青蛙的朝向），这表明由于暴露偏差的增加，教师解的多样性未被维持（见2.2节）。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"><img src = "t1.png" width = "80%"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><em>表1：与来自同一教师的蒸馏方法的比较。教师使用在ImageNet 256p上训练的DiT-XL/2架构。损失R表示使用回归损失进行蒸馏，R+D表示使用额外的判别器损失。FLOPs和延迟在去噪过程中测量。延迟是指在H100上生成一张图像所需的时间。</em></td>
</tr>
</tbody>
</table>
</div>
<p>使用块级因果掩码M4的ARD（利用整个轨迹历史）在2步和4步的所有指标上均优于步长蒸馏。与步长蒸馏不同，当步数从2增加到4时，ARD的召回率保持稳定。图5b和5c显示，ARD保留了教师解的全局结构。因此，随着步数S的增加，ARD的性能显著提升。图6a表明，在训练过程中，ARD变体（公式（6））比原始回归损失（公式（4））收敛更有效，证明了我们自回归设计的优势。此外，对于4步ARD模型，其相对于教师的FID退化值为1.43（4.32-2.89），比步长蒸馏的7.36（10.25-2.89）低5倍。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"><img src = "f6.png" width = "90%"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><em>图6. 4步蒸馏方法的设计选择（注意力掩码选项和N）分析。</em></td>
</tr>
</tbody>
</table>
</div>
<p>当使用额外的判别器损失（R+D）时，步长蒸馏和ARD的性能均有所提升，且ARD优于步长蒸馏，达到1.84的FID。图5d显示，判别器损失使样本更清晰，同时通过教师提供的$[x_{\tau_S},x_{\tau_0}]$耦合保留了全局结构。这表明额外的判别器损失不会损害样本的多样性，甚至能改善召回率指标。判别器损失使ARD的表现优于教师模型，并在速度和质量方面超越表2和图2d中的公开少步生成模型。图2d中的速度是在NVIDIA H100上以批量大小128测量的。对于基线模型，我们使用其官方代码并在相同条件下测量速度。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"><img src = "t2.png" width = "50%"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><em>表2：与ImageNet 256p上的公开模型的比较。</em></td>
</tr>
</tbody>
</table>
</div>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"><img src = "f5.png" width = "80%"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><em>图5. 从相同的初始噪声xTS生成的ImageNet 256p样本。所有蒸馏模型都是4步模型。</em></td>
</tr>
</tbody>
</table>
</div>
<p><strong>注意力掩码消融</strong> 表1展示了当S=4且N=6时，图3b中引入的各种注意力掩码的结果。默认注意力掩码M4因其灵活性表现出最佳的FID（4.32）。M2中的窗口注意力和M3中对初始噪声的保留分别实现了4.75和4.45的FID。这些宽松选项（M2和M3）也比步长蒸馏（10.25）有显著提升。请注意，M2和M3每步都使用两个输入。M2将窗口大小设置为2，因此使用两个最近的去噪样本$[x_{\tau_{s+1}},x_{\tau_s}]$来预测$x_{\tau_{s-1}}$。M3使用$[x_{\tau_S},x_{\tau_s}]$作为输入。使用初始噪声$x_{\tau_S}$（M3）作为附加信息似乎更有益，因为它有助于每一步都保持真实输入信号。对块级因果掩码M4的注意力分数分析（图4c、4e和4g）表明，初始噪声在$x_{\tau_S:\tau_{s+1}}$中激活程度最高，证明了其中信息的重要性。</p>
<p><strong>误差累积消融</strong> 当我们从教师轨迹上的真实点开始用学生模型采样时，可以分析学生模型中哪些步骤累积了更多误差。如图6b所示，如果前三个步骤由教师求解（75%），步长蒸馏和ARD之间的性能差距较小。然而，如果早期步骤由学生预测，步长蒸馏的性能会显著下降。这表明步长蒸馏更容易受到暴露偏差的影响，而ARD更鲁棒。</p>
<p><strong>N的消融</strong> 对于4步ARD模型，如图6c所示，N=6时实现最佳性能。虽然较大的N使学生模型更灵活，但较小的N提供了有效的归纳偏置。N=6在2步情况下（见表1）和不同注意力掩码下也表现良好。对于窗口注意力M2，随着N从28减少到6，FID从5.08提高到4.75。对于保留初始噪声的M3，FID从5.01提高到4.45。此外，较小的N导致推理更快，并且需要更少的内存，因为我们需要为更少的层存储kv缓存。</p>
<p><strong>效率分析</strong> 表1和图2c显示了推理阶段去噪过程中的浮点运算（FLOPs），表明了理论计算成本。主干DiT架构的推理FLOPs为118.6 GFLOPs[54]。步长蒸馏的FLOPs与步数S成正比，并且由于无分类器引导[21]，教师需要两倍的FLOPs。由于kv缓存，ARD需要稍多的FLOPs，这增加了关注的键的数量和相应值的聚合。增加的量与N成正比，N是使用注意力缓存的层数。我们的最佳模型ARD（N=6）仅比步长蒸馏模型多使用1.1%的FLOPs。延迟表现出与FLOPs类似的趋势，除了教师模型。图3b中的注意力选项M2和M3在4步情况下需要稍少的FLOPs（477.1 GFLOPs），因为与默认注意力相比，kv缓存的量更小。当S变大时，这些宽松选项可以显著减少kv缓存的增加。</p>
<h4 id="4-2-文本条件图像生成"><a href="#4-2-文本条件图像生成" class="headerlink" title="4.2 文本条件图像生成"></a>4.2 文本条件图像生成</h4><p>我们使用具有扩散Transformer架构的17亿参数Emu[8]模型作为教师模型。该教师模型在大规模内部数据集上针对1024p分辨率进行了预训练，并在一小部分高质量美学图像上进行了微调。我们在线计算教师ODE轨迹，默认使用48个教师步骤。由于我们的蒸馏仅需要文本提示进行训练，因此我们使用大规模内部预训练数据集进行蒸馏。对于ARD学生Transformer，我们选择块级因果注意力M4。我们仅使用回归损失进行15k次训练迭代。</p>
<p><strong>评估指标</strong> 为评估样本保真度和多样性，我们在MS-COCO 2017数据集[40]上使用5k个随机真实样本和5k个随机提示生成的样本计算零样本FID。为测量提示对齐，我们使用CompBench[23]，其按照Imagine Flash[30]的评估协议包含六类提示。</p>
<p><strong>文本-图像对齐</strong> 表3显示了与公开高分辨率（≥768像素）蒸馏模型相比，在CompBench上的文本-图像对齐分数。我们基于Emu（7.5 CFG）的ARD在平均分数上优于所有其他公开高分辨率蒸馏模型。除了DMD2[82]表现相当外，ARD在所有六个类别中都超过了所有其他1024p蒸馏模型，但DMD2多使用9亿参数和更多采样步骤。在从Emu（3.0 CFG）蒸馏的ARD中，学生和教师之间的平均分数差距仅为2.3，这是所有3步蒸馏模型中最小的（每个教师的性能见补充材料）。ARD还可以成功生成遵循长而详细提示的图像（见图7）。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"><img src = "t3.png" width = "80%"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><em>表3：公开高分辨率（≥768p）蒸馏模型在CompBench上的文本-图像对齐分数。损失R表示使用回归损失，D表示使用判别器损失。最佳和次佳结果分别用粗体和下划线突出显示。</em></td>
</tr>
</tbody>
</table>
</div>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"><img src = "f7.png" width = "90%"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><em>图7. ARD与Emu教师在长（真实/非真实）提示上的样本比较。</em></td>
</tr>
</tbody>
</table>
</div>
<p><strong>样本质量</strong> 表4显示了针对1024p公开蒸馏方法的FID比较，这些方法以教师的PF-ODE为目标。虽然学生的绝对性能（FID-S）在模型中排名第二，但最佳模型LCM-LoRA（2.8B）比ARD多11亿参数。由于当蒸馏以PF-ODE为目标时，蒸馏模型的上限性能是教师，因此其性能高度依赖于教师的性能（FID-T）。表4显示了一个明显的趋势：参数更大的教师模型在FID-T上表现更好。为了量化蒸馏方法的有效性，我们测量性能下降，即教师和学生之间的差距。与基线相比，ARD表现出最小的下降。我们从同一个Emu教师训练了步长蒸馏，ARD仍然表现更好，这进一步验证了使用轨迹历史的好处。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"><img src = "t4.png" width = "60%"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><em>表4：公开1024p蒸馏模型在MS-COCO 2017上的零样本FID 5k。FID-T是教师性能，FID-S是学生性能。下降值表示学生和教师之间的性能差距。</em></td>
</tr>
</tbody>
</table>
</div>
<p>图1显示了从Emu（7.5 CFG）蒸馏的ARD生成的样本。ARD生成跨各种主题和风格的高质量图像。图7左侧的示例比较了ARD和目标教师在相同初始噪声$x_{\tau_S}$和具有现实上下文（左）及非现实上下文（右）的长详细提示下的样本。样本并不相同，但ARD生成的图像保持了高保真度并有效保留了文本信息。图像中很好地捕捉了主体和背景的详细描述。图7右侧的示例比较了教师和学生在3步中生成的样本。尽管步数相同，但ARD仅使用额外的kv缓存，而教师由于CFG需要两倍的计算量。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"><img src = "f1.png" width = "80%"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><em>图1. 我们的3步ARD模型生成的样本（1024×1024），该模型由17亿参数的Emu模型蒸馏而来。</em></td>
</tr>
</tbody>
</table>
</div>
<h3 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a>5. 总结</h3><p>本文介绍了一种新颖的扩散Transformer少步蒸馏方法，该方法通过自回归方式利用整个历史ODE轨迹，对分步蒸馏进行了泛化。我们还引入了一种改进的Transformer架构，以支持自回归蒸馏设计。ODE轨迹的使用通过在每一步都保持真实输入信号，减轻了暴露偏差。通过将历史轨迹分析为更优质的粗粒度信息来源，基于注意力权重分析，ARD引入了仅在较低层使用历史轨迹这一额外设计选择。实证结果表明，ARD优于分步蒸馏，并且超越了现有的少步生成模型。</p>
<h3 id="A-相关工作"><a href="#A-相关工作" class="headerlink" title="A. 相关工作"></a>A. 相关工作</h3><h4 id="A-1-扩散加速模型"><a href="#A-1-扩散加速模型" class="headerlink" title="A.1. 扩散加速模型"></a>A.1. 扩散加速模型</h4><p>基于ODE的加速：一系列研究提出了通过开发数值求解器来高效获取给定PF-ODE的解的方法，以在少步范围内减少离散化误差。另一类研究通过修正ODE轨迹的曲率来学习连续流ODE模型，从而减少求解ODE所需的步数。还有一种方法是利用给定ODE轨迹的确定性，将ODE的解蒸馏到少步学生模型中，这与我们的工作最为契合。一些论文建议通过利用学生预测与教师预测之间的回归损失来学习初始噪声和样本的耦合。而另一些论文则建议利用基于学生自一致性的回归损失。</p>
<p>分布匹配蒸馏：这类研究侧重于少步学生分布与教师分布之间的匹配，而不依赖于ODE轨迹。一些方法在训练过程中估计学生模型的得分函数，并确保其与教师的得分函数相似。其他方法利用判别器和对抗损失，但这通常需要大量的超参数搜索来实现稳定训练。值得注意的是，为了获得实际的性能提升，分布匹配损失通常也会与基于ODE的蒸馏方法中的回归损失相结合。这些方法需要训练辅助神经网络（例如学生得分网络、判别器）进行蒸馏，这需要额外的计算资源和内存。</p>
<h4 id="A-2-基于Transformer的视觉生成模型"><a href="#A-2-基于Transformer的视觉生成模型" class="headerlink" title="A.2. 基于Transformer的视觉生成模型"></a>A.2. 基于Transformer的视觉生成模型</h4><p>自回归模型：视觉自回归模型最初是在像素空间中应用CNN或RNN架构。随后，一系列研究提出了在带有自编码模块的向量量化嵌入空间中进行自回归建模。这些模型将图像视为一维离散值令牌序列，并采用与语言模型类似的Transformer架构。进一步的研究通过联合估计语言和视觉令牌（一次一个令牌），将自回归模型扩展到多模态生成。最近，有研究提出预测二维令牌图序列，而不是一维令牌序列。这一变化显著缩短了预测序列的长度，从而减少了推理过程中对模型的调用次数。我们的工作也基于二维令牌图展开。然而，与该研究不同的是，该研究需要一个专门的自编码器，而我们的二维令牌图序列直接来自任何预训练的扩散Transformer。</p>
<p>DART：DART是一项并行工作，它提出了一种基于扩散过程的二维令牌图序列的自回归模型。然而，它与我们的工作存在根本差异。我们提出的蒸馏方法（ARD）允许沿着预训练扩散模型的反向ODE轨迹将推理步数减少到3或4步，相比之下，DART是一种普通的扩散模型，效率非常低，需要大量的采样步骤。这导致表5中的性能存在显著差距，因为我们的模型是专门为在少步范围内工作而设计的。在二维令牌图序列的形成方式上也存在差异。在ARD中，我们直接使用教师的反向ODE轨迹。而在DART中，轨迹是通过对真实数据应用正向加噪过程形成的。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"><img src = "t5.png" width = "60%"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><em>表5：ARD与DART在ImageNet 256p上的比较。</em></td>
</tr>
</tbody>
</table>
</div>
<h3 id="B-实验细节"><a href="#B-实验细节" class="headerlink" title="B. 实验细节"></a>B. 实验细节</h3><h4 id="B-1-类别条件图像生成"><a href="#B-1-类别条件图像生成" class="headerlink" title="B.1. 类别条件图像生成"></a>B.1. 类别条件图像生成</h4><p>我们在学生模型训练中遵循教师模型的配置，仅调整了梯度裁剪和批量大小。表6展示了使用回归损失的4步学生模型的训练配置。对于2步学生模型，我们使用128的批量大小；对于1步学生模型，使用256的批量大小。学生模型从教师模型的权重初始化。默认情况下，我们将第s步的预测目标设置为$\mathbb{E}[x_0 | x_{\tau_s}]$。我们使用8块NVIDIA A100 GPU进行训练，训练过程大约需要2天。如图8a中的蓝线所示，FID在16小时（10万次迭代）内几乎收敛。我们对基线模型（分步蒸馏）也采用了相同的设置。需要注意的是，分步蒸馏并非使用原始论文中提出的渐进式算法进行训练，而是直接从教师模型学习。目标函数的相关内容请参见2.2节。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"><img src = "t6.png" width = "60%"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><em>表6：类别条件生成的详细信息。</em></td>
</tr>
</tbody>
</table>
</div>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"><img src = "f8.png" width = "80%"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><em>图8. 预测目标的消融实验。</em></td>
</tr>
</tbody>
</table>
</div>
<p>当使用额外的判别器损失时，我们类似于相关研究，将教师网络用作特征提取器，并仅训练每个Transformer块提取的特征之上的判别器头。判别器头以令牌为单位预测对数概率。我们使用铰链损失，并遵循相关研究中提出的判别器头架构。判别器在学生模型的最终预测$\hat{x}_{\tau_0}=G_{\theta}(x_{\tau_S: \tau_1}, 1)$和真实数据上进行训练。我们使用$1e^{-3}$的学习率对其进行训练，且不使用权重衰减。我们按照相关研究，在回归损失和判别器损失之间进行自适应平衡。学生模型和判别器均使用48的批量大小。通过添加判别器损失，并对经过回归损失预训练的学生模型进行进一步微调，我们在仅4万次迭代内就将FID从4.32提升至1.84。</p>
<h4 id="B-2-文本条件图像生成"><a href="#B-2-文本条件图像生成" class="headerlink" title="B.2. 文本条件图像生成"></a>B.2. 文本条件图像生成</h4><p>Emu教师模型有17亿个参数，由24个DiT层组成，并使用交叉注意力层进行文本条件控制。Emu是一个潜在扩散模型，它将1024×1024×3的图像编码到128×128×8的潜在空间中。蒸馏设置与类别条件情况下的流程相似。我们遵循与教师模型相同的训练配置，只是调整了批量大小。学生模型使用块级因果掩码（M4），且$N=1$，因为这一设置在性能和质量的权衡上表现最佳。为了快速训练，预测目标设置为$x_{\tau_s}$。我们使用32块H100 GPU训练学生模型。表7展示了用于生成图1中图像的提示词（从左到右，从上到下）。</p>
<h3 id="C-额外的实验结果"><a href="#C-额外的实验结果" class="headerlink" title="C. 额外的实验结果"></a>C. 额外的实验结果</h3><h4 id="C-1-在ImageNet-256p上的更多消融实验"><a href="#C-1-在ImageNet-256p上的更多消融实验" class="headerlink" title="C.1. 在ImageNet 256p上的更多消融实验"></a>C.1. 在ImageNet 256p上的更多消融实验</h4><p>预测目标：教师模型提供ODE路径$x_{\tau_s} \in [x_{\tau_S},…,x_{\tau_0}]$。当我们求解教师ODE时，教师还会在每个步骤$s$提供等效目标$\mathbb{E}[x_{\tau_0}|x_{\tau_s}]$，这被称为“预测的$x_{\tau_0}$”。图8a展示了类别条件实验中两个预测目标的消融结果。$x_{T_s}$预测在最初的5万次迭代中收敛更快，但使用目标$\mathbb{E}[x_{\tau_0}|x_{\tau_s}]$时，模型最终会收敛到更好的局部最优解。我们推测，$\mathbb{E}[x_{\tau_0}|x_{\tau_s}]$作为输入在提供细粒度信息方面具有优势，因为不必要的噪声已被去除。正如8b所示，在文本条件实验中，两种估计目标之间的学习速度差异更为明显。在我们的早期工作中，我们观察到在2步实验中，$x_{T_2}$预测收敛迅速且性能令人满意，因此我们在其余实验中选择了$x_{T_2}$预测。</p>
<p>历史轨迹使用层数（$N$）的消融：我们通过对$N$的不同取值进行消融实验，提供了更多的评估结果。每个指标都显示出与图6c中的FID相似的趋势。图9d展示了不同$N$值的训练曲线。我们发现$N=6$是最优值。在10万次迭代后，性能优势仍然保持，且$N=6$不仅具有更好的收敛点，学习速度也更快。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"><img src = "f9.png" width = "100%"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><em>图9. 关于N消融的更多评估结果。</em></td>
</tr>
</tbody>
</table>
</div>
<h4 id="C-2-图像操作"><a href="#C-2-图像操作" class="headerlink" title="C.2. 图像操作"></a>C.2. 图像操作</h4><p>ARD提供了类似于相关研究的图像操作能力。从初始噪声$x_{\tau_S}$开始采样，ARD通过在特定时间步$s$使用源图像$x^{src}$作为输入（而非预测$\hat{x}_{\tau_s}$），将其去噪到目标类别（即$\hat{x}_{\tau_{s-1}}=G_{\theta}([\hat{x}_{\tau_S:\tau_{s+1}},x^{src}],s)$）。后续的采样过程与之前相同。图10展示了使用4步ARD的图像转换结果。第一步的预测被替换为源图像，并输入到ARD模型中。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"><img src = "f10.png" width = "70%"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><em>图10. 使用ARD（R+D）4步模型进行的图像转换。</em></td>
</tr>
</tbody>
</table>
</div>
<h4 id="C-3-文本到图像（T2I）的图像-文本对齐的参考教师性能"><a href="#C-3-文本到图像（T2I）的图像-文本对齐的参考教师性能" class="headerlink" title="C.3. 文本到图像（T2I）的图像-文本对齐的参考教师性能"></a>C.3. 文本到图像（T2I）的图像-文本对齐的参考教师性能</h4><p>表8展示了表3中列出的3步蒸馏模型对应的教师模型性能。从Emu（3.0 CFG）蒸馏得到的ARD在平均得分上的差距为$2.3(=58.5-56.2)$，这在所有竞争者中是最小的。从Pixart-alpha蒸馏得到的Pixart-delta的差距为4.2，从SSD和LDM-XL蒸馏得到的LCM-LoRA的差距分别为8.0和3.3。所有768分辨率的学生模型均从Emu（27亿参数）蒸馏得到，表现最佳的模型Imagine Flash的差距为4.9。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"><img src = "t8.png" width = "80%"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><em>表8：每个3步T2I蒸馏模型的目标教师在CompBench上的性能。</em></td>
</tr>
</tbody>
</table>
</div>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文提出的方法是将过去的梯度信息和初始噪声的信息全部引入Transformer模型，从而减少当前时刻学生模型的误差累积，提高生成图像的保真度，并加速模型的生成。</p>
<p>可以借鉴的部分：</p>
<ul>
<li>将过去的梯度信息和初始噪声都融合进来，这样就得到了过去的轨迹变化。</li>
</ul>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/diffusion/" rel="tag"># diffusion</a>
              <a href="/tags/CVPR/" rel="tag"># CVPR</a>
              <a href="/tags/2025/" rel="tag"># 2025</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2025/06/26/2025-CVPR-Attend-to-Not-Attended-Structure-then-Detail-Token-Merging-for-Post-training-DiT-Acceleratio%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/" rel="prev" title="2025-CVPR-Attend to Not Attended Structure-then-Detail Token Merging for Post-training DiT Acceleratio论文精读">
      <i class="fa fa-chevron-left"></i> 2025-CVPR-Attend to Not Attended Structure-then-Detail Token Merging for Post-training DiT Acceleratio论文精读
    </a></div>
      <div class="post-nav-item">
    <a href="/2025/07/03/2025-ICML-Morse-Dual-Sampling-for-Lossless-Acceleration-of-Diffusion-Models%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" rel="next" title="2025-ICML-Morse Dual-Sampling for Lossless Acceleration of Diffusion Models论文阅读">
      2025-ICML-Morse Dual-Sampling for Lossless Acceleration of Diffusion Models论文阅读 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%A8%E6%96%87%E7%BF%BB%E8%AF%91"><span class="nav-number">1.</span> <span class="nav-text">全文翻译</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%91%98%E8%A6%81"><span class="nav-number">1.1.</span> <span class="nav-text">摘要</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E5%BC%95%E8%A8%80"><span class="nav-number">1.2.</span> <span class="nav-text">1. 引言</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86"><span class="nav-number">1.3.</span> <span class="nav-text">2. 预备知识</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B"><span class="nav-number">1.3.1.</span> <span class="nav-text">2.1 扩散模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-%E6%AD%A5%E9%95%BF%E8%92%B8%E9%A6%8F%E6%A8%A1%E5%9E%8B"><span class="nav-number">1.3.2.</span> <span class="nav-text">2.2 步长蒸馏模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-%E8%87%AA%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B"><span class="nav-number">1.3.3.</span> <span class="nav-text">2.3 自回归模型</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E6%96%B9%E6%B3%95"><span class="nav-number">1.4.</span> <span class="nav-text">3. 方法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-%E8%87%AA%E5%9B%9E%E5%BD%92%E8%92%B8%E9%A6%8F"><span class="nav-number">1.4.1.</span> <span class="nav-text">3.1 自回归蒸馏</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-Transformer%E8%AE%BE%E8%AE%A1"><span class="nav-number">1.4.2.</span> <span class="nav-text">3.2 Transformer设计</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-%E8%AE%AD%E7%BB%83%E4%B8%8E%E6%8E%A8%E7%90%86%E6%B5%81%E7%A8%8B"><span class="nav-number">1.4.3.</span> <span class="nav-text">3.3 训练与推理流程</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-%E5%AE%9E%E9%AA%8C"><span class="nav-number">1.5.</span> <span class="nav-text">4. 实验</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-1-%E7%B1%BB%E5%88%AB%E6%9D%A1%E4%BB%B6%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90"><span class="nav-number">1.5.1.</span> <span class="nav-text">4.1 类别条件图像生成</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-2-%E6%96%87%E6%9C%AC%E6%9D%A1%E4%BB%B6%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90"><span class="nav-number">1.5.2.</span> <span class="nav-text">4.2 文本条件图像生成</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-%E6%80%BB%E7%BB%93"><span class="nav-number">1.6.</span> <span class="nav-text">5. 总结</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#A-%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C"><span class="nav-number">1.7.</span> <span class="nav-text">A. 相关工作</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#A-1-%E6%89%A9%E6%95%A3%E5%8A%A0%E9%80%9F%E6%A8%A1%E5%9E%8B"><span class="nav-number">1.7.1.</span> <span class="nav-text">A.1. 扩散加速模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#A-2-%E5%9F%BA%E4%BA%8ETransformer%E7%9A%84%E8%A7%86%E8%A7%89%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B"><span class="nav-number">1.7.2.</span> <span class="nav-text">A.2. 基于Transformer的视觉生成模型</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-%E5%AE%9E%E9%AA%8C%E7%BB%86%E8%8A%82"><span class="nav-number">1.8.</span> <span class="nav-text">B. 实验细节</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#B-1-%E7%B1%BB%E5%88%AB%E6%9D%A1%E4%BB%B6%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90"><span class="nav-number">1.8.1.</span> <span class="nav-text">B.1. 类别条件图像生成</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-2-%E6%96%87%E6%9C%AC%E6%9D%A1%E4%BB%B6%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90"><span class="nav-number">1.8.2.</span> <span class="nav-text">B.2. 文本条件图像生成</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#C-%E9%A2%9D%E5%A4%96%E7%9A%84%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C"><span class="nav-number">1.9.</span> <span class="nav-text">C. 额外的实验结果</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#C-1-%E5%9C%A8ImageNet-256p%E4%B8%8A%E7%9A%84%E6%9B%B4%E5%A4%9A%E6%B6%88%E8%9E%8D%E5%AE%9E%E9%AA%8C"><span class="nav-number">1.9.1.</span> <span class="nav-text">C.1. 在ImageNet 256p上的更多消融实验</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#C-2-%E5%9B%BE%E5%83%8F%E6%93%8D%E4%BD%9C"><span class="nav-number">1.9.2.</span> <span class="nav-text">C.2. 图像操作</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#C-3-%E6%96%87%E6%9C%AC%E5%88%B0%E5%9B%BE%E5%83%8F%EF%BC%88T2I%EF%BC%89%E7%9A%84%E5%9B%BE%E5%83%8F-%E6%96%87%E6%9C%AC%E5%AF%B9%E9%BD%90%E7%9A%84%E5%8F%82%E8%80%83%E6%95%99%E5%B8%88%E6%80%A7%E8%83%BD"><span class="nav-number">1.9.3.</span> <span class="nav-text">C.3. 文本到图像（T2I）的图像-文本对齐的参考教师性能</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">2.</span> <span class="nav-text">总结</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Zongqing Li"
      src="https://github.com/hqulzq/hqulzq.github.io/blob/main/images/userimg.jpg?raw=true">
  <p class="site-author-name" itemprop="name">Zongqing Li</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">103</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">12</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">49</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/hqulzq" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;hqulzq" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:hqulzq@163.com" title="E-Mail → mailto:hqulzq@163.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

      
<script type="text/javascript" charset="utf-8" src="/js/tagcloud.js"></script>
<script type="text/javascript" charset="utf-8" src="/js/tagcanvas.js"></script>
<div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div id="myCanvasContainer" class="widget tagcloud">
        <canvas width="250" height="250" id="resCanvas" style="width=100%">
            <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/2019/" rel="tag">2019</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/2020/" rel="tag">2020</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/2021/" rel="tag">2021</a><span class="tag-list-count">7</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/2022/" rel="tag">2022</a><span class="tag-list-count">6</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/2023/" rel="tag">2023</a><span class="tag-list-count">9</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/2024/" rel="tag">2024</a><span class="tag-list-count">12</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/2025/" rel="tag">2025</a><span class="tag-list-count">23</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Bug%E6%80%BB%E7%BB%93/" rel="tag">Bug总结</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CVPR/" rel="tag">CVPR</a><span class="tag-list-count">25</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CVPR-Workshop/" rel="tag">CVPR Workshop</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Git/" rel="tag">Git</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ICCV/" rel="tag">ICCV</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ICLR/" rel="tag">ICLR</a><span class="tag-list-count">12</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ICML/" rel="tag">ICML</a><span class="tag-list-count">6</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/IJCAI/" rel="tag">IJCAI</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Kafka/" rel="tag">Kafka</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Knife4j/" rel="tag">Knife4j</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linux/" rel="tag">Linux</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/MinIO/" rel="tag">MinIO</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Mybatis-Plus/" rel="tag">Mybatis-Plus</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NeurIPS/" rel="tag">NeurIPS</a><span class="tag-list-count">7</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Nginx/" rel="tag">Nginx</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ODE%E6%B1%82%E8%A7%A3/" rel="tag">ODE求解</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RabbitMQ/" rel="tag">RabbitMQ</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Redis/" rel="tag">Redis</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RocketMQ/" rel="tag">RocketMQ</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/diffusion/" rel="tag">diffusion</a><span class="tag-list-count">64</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/docker/" rel="tag">docker</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/k8s/" rel="tag">k8s</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/latex/" rel="tag">latex</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/leetcode/" rel="tag">leetcode</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mongodb/" rel="tag">mongodb</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mysql/" rel="tag">mysql</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E4%BB%A3%E7%A0%81%E9%9A%8F%E6%83%B3%E5%BD%95/" rel="tag">代码随想录</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%A4%B4%E5%87%BD%E6%95%B0/" rel="tag">头函数</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%B0%9A%E5%BA%AD%E5%85%AC%E5%AF%93/" rel="tag">尚庭公寓</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86/" rel="tag">异常处理</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/" rel="tag">快速入门</a><span class="tag-list-count">13</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%80%BB%E7%BB%93/" rel="tag">总结</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93/" rel="tag">技术总结</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E7%BB%84/" rel="tag">数组</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B3%A8%E8%A7%A3/" rel="tag">注解</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%BD%91%E7%AB%99/" rel="tag">网站</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%8B%A5%E4%BE%9D/" rel="tag">若依</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%AE%BA%E6%96%87/" rel="tag">论文</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/" rel="tag">设计模式</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%93%BE%E8%A1%A8i/" rel="tag">链表i</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%98%85%E8%AF%BB/" rel="tag">阅读</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98/" rel="tag">项目实战</a><span class="tag-list-count">6</span></li></ul>
        </canvas>
    </div>
</div>



    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zongqing Li</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>









<script>
document.querySelectorAll('.pdfobject-container').forEach(element => {
  let url = element.dataset.target;
  let pdfOpenParams = {
    navpanes : 0,
    toolbar  : 0,
    statusbar: 0,
    pagemode : 'thumbs',
    view     : 'FitH'
  };
  let pdfOpenFragment = '#' + Object.entries(pdfOpenParams).map(([key, value]) => `${key}=${encodeURIComponent(value)}`).join('&');
  let fullURL = `/lib/pdf/web/viewer.html?file=${encodeURIComponent(url)}${pdfOpenFragment}`;

  if (NexT.utils.supportsPDFs()) {
    element.innerHTML = `<embed class="pdfobject" src="${url + pdfOpenFragment}" type="application/pdf" style="height: ${element.dataset.height};">`;
  } else {
    element.innerHTML = `<iframe src="${fullURL}" style="height: ${element.dataset.height};" frameborder="0"></iframe>`;
  }
});
</script>




  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

  
  <style>
    .copy-btn {
      display: inline-block;
      padding: 6px 12px;
      font-size: 13px;
      font-weight: 700;
      line-height: 20px;
      color: #4D4D4C;
      white-space: nowrap;
      vertical-align: middle;
      cursor: pointer;
      background-color: #F7F7F7;
      background-image: linear-gradient(#F7F7F7, #F7F7F7);
      border: 1px solid #d5d5d5;
      border-radius: 3px;
      user-select: none;
      outline: 0;
    }

    .highlight-wrap .copy-btn {
      transition: opacity .3s ease-in-out;
      opacity: 0;
      padding: 2px 6px;
      position: absolute;
      right: 4px;
      top: 8px;
    }

    .highlight-wrap:hover .copy-btn,
    .highlight-wrap .copy-btn:focus {
      opacity: 1
    }

    .highlight-wrap {
      position: relative;
    }
  </style>

  <script>
    $('.highlight').each(function (i, e) {
      var $wrap = $('<div>').addClass('highlight-wrap')
      $(e).after($wrap)
      $wrap.append($('<button>').addClass('copy-btn').append('post.copy_button').on('click', function (e) {
        var code = $(this).parent().find('.code').find('.line').map(function (i, e) {
          return $(e).text()
        }).toArray().join('\n')
        var ta = document.createElement('textarea')
        document.body.appendChild(ta)
        ta.style.position = 'absolute'
        ta.style.top = '0px'
        ta.style.left = '0px'
        ta.value = code
        ta.select()
        ta.focus()
        var result = document.execCommand('copy')
        document.body.removeChild(ta)
        
          if(result)$(this).text('post.copy_success')
          else $(this).text('post.copy_failure')
        
        $(this).blur()
      })).on('mouseleave', function (e) {
        var $b = $(this).find('.copy-btn')
        setTimeout(function () {
          $b.text('post.copy_button')
        }, 300)
      }).append(e)
    })
  </script>


<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/assets/koharu.model.json"},"display":{"position":"right","width":150,"height":300,"hOffset":-15,"vOffset":-15},"mobile":{"show":true},"react":{"opacity":0.7},"log":false});</script></body>
</html>
